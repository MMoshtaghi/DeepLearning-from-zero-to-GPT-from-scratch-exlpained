{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing Optimization iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" %matplotlib inline sets the backend of matplotlib to\\nthe 'inline' backend. When using the 'inline' backend,\\nyour matplotlib graphs will be included in your notebook,\\nnext to the code.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make up a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 570.25x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples = 100, noise=0.1)\n",
    "# print(y)\n",
    "# print(X)\n",
    "y = 2*y - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "sns.relplot(x=X[:,0], y=X[:,1], hue=y, style=y, palette=['r', 'b'], markers=['X', 'o'])\n",
    "X.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A NN that subscribes to the PyTorch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have our class Value\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _prev=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_prev) # the order does not matter, use set instead of list\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # After the forwrd path (when we have the data vlaue of all nodes)\n",
    "        # we start the backprop (to get the gradients for each node)\n",
    "        # the output node of each operation knows the operation and children\n",
    "        # since we dont have a backprop for leaf nodes, and \n",
    "        # each operation has different local gradient\n",
    "        # we can't define a general method in the class.\n",
    "        # so at the time of doing the operation,\n",
    "        # we can both define the local gradiant function\n",
    "        # and store the whole chain rule function in an attribute to call later.\n",
    "        # then call the local this function attribute from end node to the begining.\n",
    "        self._backprop = lambda: None\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Value({self.label} | data:{self.data} | grad:{self.grad})'\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # check if 'other' is an instance of 'Value'\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data + other.data, (self, other), _op='+')\n",
    "        \n",
    "        def _backprop():\n",
    "            # partial derivatives for each input:\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -1*other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return -1*(self + -1*other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # check if 'other' is an instance of 'Value'\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data * other.data, (self, other), _op='*')\n",
    "        \n",
    "        def _backprop():\n",
    "            # partial derivatives for each input:\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        # only support int or float powers for now!!\n",
    "        # in case of other of type 'Value', we would need another method\n",
    "        assert isinstance(other, (int, float))\n",
    "        \n",
    "        out = Value(self.data**other, (self,), _op=f'**{other}')\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += (other * self.data**(other - 1)) * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value( self.data if self.data > 0.0 else 0.0 , (self,), _op='ReLU' )\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += out.grad * (out.data > 0.0)\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), _op='exp')\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += out.data * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        tanh = (np.exp(2*self.data) - 1)/(np.exp(2*self.data) + 1)\n",
    "        out = Value( tanh, (self,), _op='tanh')\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += (1 - tanh**2) * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def backprop(self):\n",
    "        topo_sort_list = []\n",
    "        visited = set() # the order does not matter, use set instead of list\n",
    "        \n",
    "        def build_topo(root):\n",
    "            if root not in visited:\n",
    "                visited.add(root)\n",
    "                # Appending to topo_sort before its children are processed\n",
    "                # will give us out-to-left sort,\n",
    "                # but not out to leaf sort in case of b (bias leaf), try it & see it\n",
    "                for child in root._prev:\n",
    "                    build_topo(child)\n",
    "                # Appending after its children are processed\n",
    "                # will give us leaf-to-out sort\n",
    "                topo_sort_list.append(root)\n",
    "        \n",
    "        build_topo(root=self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo_sort_list):\n",
    "            node._backprop()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP\n",
    "1) make it more like PyTorch:\n",
    "\n",
    "    1.1) A parent class for zero_grad & parameters\n",
    "    \n",
    "    1.2) Adding linearity & Non-linearity to Neurons\n",
    "    \n",
    "    1.3) a repr for onjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "    # nin: number of inputs\n",
    "    def __init__(self, nin: int, act_func=None):\n",
    "        self.w = [ Value( random.uniform(-1,1), label=f'w{i}' ) for i in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1), label='b')\n",
    "        self.act_func = act_func\n",
    "    \n",
    "    def __call__(self, x: list) -> float:\n",
    "        # w * x + b -> a scalar value\n",
    "        activation = sum( ( wi*xi for wi,xi in zip(self.w, x) ) , start=self.b) # pair up w & x point wise\n",
    "        return activation if self.act_func is None else activation.tanh() # getattr(activation, self.act_func)()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{'Linear' if self.act_func is None else self.act_func} Neuron({len(self.w)})\"\n",
    "\n",
    "\n",
    "class OneMLPLayer(Module):\n",
    "\n",
    "    def __init__(self, nin: int, nout: int, **kwargs):\n",
    "        '''\n",
    "        so we need a bunch of Neurons:\n",
    "        Data structure? -> order matters -> list\n",
    "        '''\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x: list) -> list:\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        # list comprehension with double for:\n",
    "        # write the for loops in a way you write it as usual !! \n",
    "        return [parameter for neuron in self.neurons for parameter in neuron.parameters()]\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Layer of [{', '.join( str(n) for n in self.neurons)}]\"\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    '''\n",
    "    we want a bunch of layers -> order matters -> list\n",
    "    input layer: nin - hidden layer: hs - output layer: nout\n",
    "    '''\n",
    "    def __init__(self, nin: int, hs: list, nout: int, **kwargs) -> None:\n",
    "        layer_width = [nin] + hs + [nout]\n",
    "        self.layers = [OneMLPLayer(nin= layer_width[idx], nout=layer_width[idx+1], **kwargs) for idx in range(len(layer_width)-1) ]\n",
    "\n",
    "    def __call__(self, x) -> list:\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\" MLP of [{ ', '.join( str(l) for l in self.layers) }]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " MLP of [Layer of [tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2), tanh Neuron(2)], Layer of [tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16), tanh Neuron(16)], Layer of [tanh Neuron(16)]]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(nin=2, hs=[16, 16], nout=1, act_func='tanh')\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes:\n",
    "\n",
    "1- defining a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batch_size=None, l2_coef = 1e-4, margin=1.0):\n",
    "    # initialize dataloader\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        batch_idx = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[batch_idx], y[batch_idx]\n",
    "    \n",
    "    '''\n",
    "    use map() for creating batches on large data.\n",
    "    map() takes a function object and an iterable (or multiple iterables)\n",
    "    as arguments and returns an iterator that yields transformed items\n",
    "    on demand\n",
    "    \n",
    "    1. map() is written in C and is highly optimized,\n",
    "        its internal implied loop is more efficient than\n",
    "        a regular Python for loop.\n",
    "    \n",
    "    2. With a for loop, you need to store the whole list in your system’s memory.\n",
    "        With map(), you get items on demand,\n",
    "        and only one item is in your system’s memory at a given time.\n",
    "    '''\n",
    "    # X is a Matrix, but we want Value objects of each data point's dimension:\n",
    "    # xrow is a vector or list. map is an iterator, wrap it with list\n",
    "    inputs = [ list(map(Value, xrow)) for xrow in Xb]\n",
    "\n",
    "    # forward the model to get y_preds or scores -> model(inputs), use map() instead of for loop\n",
    "    scores = list(map(model, inputs)) # y_preds\n",
    "\n",
    "    # Hinge Loss or svm \"max-margin\" loss\n",
    "    ## $$$$ very important: loss is always + , use ReLU !!!\n",
    "    losses = [ (margin - yi*si).relu() for yi, si in zip(yb, scores)]\n",
    "    ## in SVM we have αi (importance of data points for finding SVs)\n",
    "    ## here we assume all data points are equally important\n",
    "    data_loss = sum(losses)/ len(losses)\n",
    "\n",
    "    ## in SVM we also have an L2-norm of weights as weight regularization term\n",
    "    reg_loss = sum(p*p for p in model.parameters())\n",
    "\n",
    "    ## total loss \n",
    "    total_loss = data_loss + l2_coef * reg_loss\n",
    "\n",
    "    # also get accuracy\n",
    "    ''' accuracy is about which side of the hyperplanes is the prediction\n",
    "     so saying yi==si is wrong,\n",
    "     we must check the sign of yi * si or as in SVM ti * w*xi'''\n",
    "    # below is computationally lighter :)\n",
    "    accuracy = [ (yi > 0.0) == (si.data > 0.0) for yi, si in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value( | data:1.0594960069738981 | grad:0.0) 0.5\n"
     ]
    }
   ],
   "source": [
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The power of SGD & low batch size\n",
    "1. give the whole batch to the model (batch_size = None , niter=20):\n",
    "    you will see that the model would not improve and even might get worse (gradients increase)\n",
    "\n",
    "2. then give it (batch_size = 1 , niter=1000), surprise!! model does a really good jo now\n",
    "\n",
    "3. why? it's clear, bc:\n",
    "    \n",
    "    In the former, we `average the loss` for all data points and compute the gradients. It's `hard to find a gradient that satisfies all data points!` i.e. gradients from different data points cancel each other, and `make the gradients low for all weights`!! so we don't move a lot from our previous position!\n",
    "    \n",
    "    In contrast, when we use batch_size=1, for one data point, some weights will have a high gradient and some others will have a low one. we only change those with a high one. For another sample this will happen for different set of weights. The point we change weights, we move to another position\n",
    "\n",
    "in my experience:\n",
    "\n",
    "    1 iteration (niter=2) with batch_size=1 and wrong classification, you will encounter more of the following:\n",
    "    \n",
    "        total_grad : 14.251573775690757\n",
    "        \n",
    "        w0's grad : 8.248696785199401e-05\n",
    "    \n",
    "    1 iteration with whole data:\n",
    "    \n",
    "        total_grad : 0.6998460422710074\n",
    "        \n",
    "        w0's grad : -0.014186355940754706\n",
    "\n",
    "\n",
    "**don't get to excited**, we compute the `forward path in parallel` for all data in batch and average the loss.\n",
    "\n",
    "so if we just compute it for one data point every time, `it's gonna take much longer to finish the whole dataset`!! especially when data is high dimensional.\n",
    "\n",
    "It's a trade-off :\n",
    "\n",
    "`energy+time+money (GPUs!!) VS usually better model`\n",
    "\n",
    "\n",
    "## The importance of Tuning L2_coef & margin !!!\n",
    "The larger the l2_coef, the lower the weights, the less complex model\n",
    "\n",
    "if you want a wider margin, you need larger weights (lower l2_coef) too.\n",
    "\n",
    "Caveat: the model might no be able to find any decision boundary for either high margin or with high weights:\n",
    "\n",
    "    - with too high margin: it's simply impossible given the data.\n",
    "\n",
    "    - with too high weights: as in SVM, to have the highest margin (distance) you need lower ||w|| remember!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 | loss: 1.052544328810015 | accuracy: 100.0%\n",
      "iteration: 2 | loss: 1.991212494587796 | accuracy: 50.0%\n",
      "iteration: 3 | loss: 2.971337159162061 | accuracy: 0.0%\n",
      "iteration: 4 | loss: 1.028884868142833 | accuracy: 100.0%\n",
      "iteration: 5 | loss: 2.9866534941828164 | accuracy: 0.0%\n",
      "iteration: 6 | loss: 2.967778127880282 | accuracy: 0.0%\n",
      "iteration: 7 | loss: 2.989880716773222 | accuracy: 0.0%\n",
      "iteration: 8 | loss: 1.982462925298357 | accuracy: 50.0%\n",
      "iteration: 9 | loss: 2.9867966703552855 | accuracy: 0.0%\n",
      "iteration: 10 | loss: 2.9153941173621667 | accuracy: 0.0%\n",
      "iteration: 11 | loss: 2.025609859152322 | accuracy: 50.0%\n",
      "iteration: 12 | loss: 2.0330885072136655 | accuracy: 50.0%\n",
      "iteration: 13 | loss: 2.03897536309612 | accuracy: 50.0%\n",
      "iteration: 14 | loss: 2.955804629241808 | accuracy: 0.0%\n",
      "iteration: 15 | loss: 1.9603532667834318 | accuracy: 50.0%\n",
      "iteration: 16 | loss: 2.8739854728499172 | accuracy: 0.0%\n",
      "iteration: 17 | loss: 2.0601472686598608 | accuracy: 50.0%\n",
      "iteration: 18 | loss: 1.8863241609831691 | accuracy: 50.0%\n",
      "iteration: 19 | loss: 2.9337947555799957 | accuracy: 0.0%\n",
      "iteration: 20 | loss: 1.1396069173423276 | accuracy: 100.0%\n",
      "iteration: 21 | loss: 1.9159435405678038 | accuracy: 50.0%\n",
      "iteration: 22 | loss: 2.0030134540078124 | accuracy: 50.0%\n",
      "iteration: 23 | loss: 2.9292745560902116 | accuracy: 0.0%\n",
      "iteration: 24 | loss: 1.6679774574452273 | accuracy: 50.0%\n",
      "iteration: 25 | loss: 1.332692397259563 | accuracy: 100.0%\n",
      "iteration: 26 | loss: 1.2749341926441153 | accuracy: 100.0%\n",
      "iteration: 27 | loss: 1.9927837386880105 | accuracy: 50.0%\n",
      "iteration: 28 | loss: 1.9820914255985633 | accuracy: 50.0%\n",
      "iteration: 29 | loss: 1.360143657973763 | accuracy: 100.0%\n",
      "iteration: 30 | loss: 1.5200152261172972 | accuracy: 100.0%\n",
      "iteration: 31 | loss: 1.0935575289865564 | accuracy: 100.0%\n",
      "iteration: 32 | loss: 1.1211815717856535 | accuracy: 100.0%\n",
      "iteration: 33 | loss: 2.8521775049596956 | accuracy: 0.0%\n",
      "iteration: 34 | loss: 1.0202051451591942 | accuracy: 100.0%\n",
      "iteration: 35 | loss: 1.053301531638638 | accuracy: 100.0%\n",
      "iteration: 36 | loss: 1.0692224251951872 | accuracy: 100.0%\n",
      "iteration: 37 | loss: 1.0297569197892615 | accuracy: 100.0%\n",
      "iteration: 38 | loss: 1.6926392182845387 | accuracy: 50.0%\n",
      "iteration: 39 | loss: 1.3028323608195476 | accuracy: 100.0%\n",
      "iteration: 40 | loss: 2.141346662196803 | accuracy: 50.0%\n",
      "iteration: 41 | loss: 1.0478990467903828 | accuracy: 100.0%\n",
      "iteration: 42 | loss: 1.1085225494248663 | accuracy: 100.0%\n",
      "iteration: 43 | loss: 2.0005662835121254 | accuracy: 50.0%\n",
      "iteration: 44 | loss: 1.0484823624014903 | accuracy: 100.0%\n",
      "iteration: 45 | loss: 1.038505440885386 | accuracy: 100.0%\n",
      "iteration: 46 | loss: 1.0723708918076422 | accuracy: 100.0%\n",
      "iteration: 47 | loss: 1.9700846097721811 | accuracy: 50.0%\n",
      "iteration: 48 | loss: 1.093076500810733 | accuracy: 100.0%\n",
      "iteration: 49 | loss: 1.399633705493823 | accuracy: 100.0%\n",
      "iteration: 50 | loss: 1.02108841535594 | accuracy: 100.0%\n",
      "iteration: 51 | loss: 1.9986435217616707 | accuracy: 50.0%\n",
      "iteration: 52 | loss: 1.9986874455404127 | accuracy: 50.0%\n",
      "iteration: 53 | loss: 1.7917534127525698 | accuracy: 50.0%\n",
      "iteration: 54 | loss: 1.0333368203548938 | accuracy: 100.0%\n",
      "iteration: 55 | loss: 1.157139235976937 | accuracy: 100.0%\n",
      "iteration: 56 | loss: 1.0034601301530766 | accuracy: 100.0%\n",
      "iteration: 57 | loss: 1.0052545613299382 | accuracy: 100.0%\n",
      "iteration: 58 | loss: 1.0739946325050267 | accuracy: 100.0%\n",
      "iteration: 59 | loss: 2.0034291362907917 | accuracy: 50.0%\n",
      "iteration: 60 | loss: 1.0051381317437063 | accuracy: 100.0%\n",
      "iteration: 61 | loss: 1.0214562690970093 | accuracy: 100.0%\n",
      "iteration: 62 | loss: 1.9581524380305835 | accuracy: 50.0%\n",
      "iteration: 63 | loss: 1.259464021280564 | accuracy: 100.0%\n",
      "iteration: 64 | loss: 1.1573350510017957 | accuracy: 100.0%\n",
      "iteration: 65 | loss: 2.5064716619130434 | accuracy: 0.0%\n",
      "iteration: 66 | loss: 1.972675528700418 | accuracy: 50.0%\n",
      "iteration: 67 | loss: 2.135896459328281 | accuracy: 50.0%\n",
      "iteration: 68 | loss: 2.0758462609902693 | accuracy: 50.0%\n",
      "iteration: 69 | loss: 1.0260552043314441 | accuracy: 100.0%\n",
      "iteration: 70 | loss: 1.0019122354236634 | accuracy: 100.0%\n",
      "iteration: 71 | loss: 1.00458383694278 | accuracy: 100.0%\n",
      "iteration: 72 | loss: 1.1000125925921336 | accuracy: 100.0%\n",
      "iteration: 73 | loss: 1.1061728193514209 | accuracy: 100.0%\n",
      "iteration: 74 | loss: 1.357517524760444 | accuracy: 100.0%\n",
      "iteration: 75 | loss: 2.0062134209516276 | accuracy: 50.0%\n",
      "iteration: 76 | loss: 1.996967017699562 | accuracy: 50.0%\n",
      "iteration: 77 | loss: 2.021633027166174 | accuracy: 50.0%\n",
      "iteration: 78 | loss: 1.88259018223647 | accuracy: 50.0%\n",
      "iteration: 79 | loss: 1.110117471070679 | accuracy: 100.0%\n",
      "iteration: 80 | loss: 1.0568839043464227 | accuracy: 100.0%\n",
      "iteration: 81 | loss: 1.0022958943788867 | accuracy: 100.0%\n",
      "iteration: 82 | loss: 1.053299855646237 | accuracy: 100.0%\n",
      "iteration: 83 | loss: 1.008642528833039 | accuracy: 100.0%\n",
      "iteration: 84 | loss: 1.7553878867730055 | accuracy: 50.0%\n",
      "iteration: 85 | loss: 1.006753322509117 | accuracy: 100.0%\n",
      "iteration: 86 | loss: 1.1889164153514133 | accuracy: 100.0%\n",
      "iteration: 87 | loss: 1.208460276403552 | accuracy: 100.0%\n",
      "iteration: 88 | loss: 2.013578295603922 | accuracy: 50.0%\n",
      "iteration: 89 | loss: 1.9695042253974322 | accuracy: 50.0%\n",
      "iteration: 90 | loss: 1.003906230395511 | accuracy: 100.0%\n",
      "iteration: 91 | loss: 1.0058459972596603 | accuracy: 100.0%\n",
      "iteration: 92 | loss: 1.0222553350350023 | accuracy: 100.0%\n",
      "iteration: 93 | loss: 1.0080383167793099 | accuracy: 100.0%\n",
      "iteration: 94 | loss: 1.003460571496515 | accuracy: 100.0%\n",
      "iteration: 95 | loss: 1.0048204440169248 | accuracy: 100.0%\n",
      "iteration: 96 | loss: 2.000705188245933 | accuracy: 50.0%\n",
      "iteration: 97 | loss: 1.0030050703170166 | accuracy: 100.0%\n",
      "iteration: 98 | loss: 1.9829394765052548 | accuracy: 50.0%\n",
      "iteration: 99 | loss: 1.179023497165549 | accuracy: 100.0%\n",
      "iteration: 100 | loss: 1.8020490992081035 | accuracy: 50.0%\n",
      "iteration: 101 | loss: 1.8173640037309844 | accuracy: 50.0%\n",
      "iteration: 102 | loss: 1.009795323650363 | accuracy: 100.0%\n",
      "iteration: 103 | loss: 1.008789863386215 | accuracy: 100.0%\n",
      "iteration: 104 | loss: 2.0058556618496124 | accuracy: 50.0%\n",
      "iteration: 105 | loss: 1.0058591806251662 | accuracy: 100.0%\n",
      "iteration: 106 | loss: 2.0058211173595826 | accuracy: 50.0%\n",
      "iteration: 107 | loss: 2.0244297251281846 | accuracy: 50.0%\n",
      "iteration: 108 | loss: 1.9465800883378608 | accuracy: 50.0%\n",
      "iteration: 109 | loss: 1.0041594634933153 | accuracy: 100.0%\n",
      "iteration: 110 | loss: 1.9559139750078822 | accuracy: 50.0%\n",
      "iteration: 111 | loss: 1.0055546940924311 | accuracy: 100.0%\n",
      "iteration: 112 | loss: 1.020687337263367 | accuracy: 100.0%\n",
      "iteration: 113 | loss: 1.0023764551024639 | accuracy: 100.0%\n",
      "iteration: 114 | loss: 1.0037561629004037 | accuracy: 100.0%\n",
      "iteration: 115 | loss: 1.003213545398185 | accuracy: 100.0%\n",
      "iteration: 116 | loss: 1.0072112744970139 | accuracy: 100.0%\n",
      "iteration: 117 | loss: 1.999377973806409 | accuracy: 50.0%\n",
      "iteration: 118 | loss: 1.9391550757068425 | accuracy: 50.0%\n",
      "iteration: 119 | loss: 1.1659046797779868 | accuracy: 100.0%\n",
      "iteration: 120 | loss: 1.003962801269584 | accuracy: 100.0%\n",
      "iteration: 121 | loss: 1.0028919042895226 | accuracy: 100.0%\n",
      "iteration: 122 | loss: 1.9919940332445991 | accuracy: 50.0%\n",
      "iteration: 123 | loss: 1.0031479417242741 | accuracy: 100.0%\n",
      "iteration: 124 | loss: 1.6625751384179097 | accuracy: 50.0%\n",
      "iteration: 125 | loss: 1.28561545838343 | accuracy: 100.0%\n",
      "iteration: 126 | loss: 1.0021284946715545 | accuracy: 100.0%\n",
      "iteration: 127 | loss: 1.0035927767377886 | accuracy: 100.0%\n",
      "iteration: 128 | loss: 1.117265809198241 | accuracy: 100.0%\n",
      "iteration: 129 | loss: 1.0031935187808019 | accuracy: 100.0%\n",
      "iteration: 130 | loss: 1.0108630770793474 | accuracy: 100.0%\n",
      "iteration: 131 | loss: 2.0008744900690414 | accuracy: 50.0%\n",
      "iteration: 132 | loss: 1.99972523036702 | accuracy: 50.0%\n",
      "iteration: 133 | loss: 1.340427784085789 | accuracy: 100.0%\n",
      "iteration: 134 | loss: 1.8151808435069983 | accuracy: 50.0%\n",
      "iteration: 135 | loss: 1.2572774376931335 | accuracy: 100.0%\n",
      "iteration: 136 | loss: 1.6045834505570251 | accuracy: 50.0%\n",
      "iteration: 137 | loss: 1.0091729072688245 | accuracy: 100.0%\n",
      "iteration: 138 | loss: 1.9907358245384792 | accuracy: 50.0%\n",
      "iteration: 139 | loss: 1.9800795833267888 | accuracy: 50.0%\n",
      "iteration: 140 | loss: 1.5991790956107466 | accuracy: 100.0%\n",
      "iteration: 141 | loss: 1.0135171636711533 | accuracy: 100.0%\n",
      "iteration: 142 | loss: 1.0070051670781917 | accuracy: 100.0%\n",
      "iteration: 143 | loss: 1.0047914323299993 | accuracy: 100.0%\n",
      "iteration: 144 | loss: 1.057977791722161 | accuracy: 100.0%\n",
      "iteration: 145 | loss: 1.4080291986990778 | accuracy: 100.0%\n",
      "iteration: 146 | loss: 1.871841349516208 | accuracy: 50.0%\n",
      "iteration: 147 | loss: 1.9932609035488469 | accuracy: 50.0%\n",
      "iteration: 148 | loss: 1.0052165657604404 | accuracy: 100.0%\n",
      "iteration: 149 | loss: 1.5425494800821369 | accuracy: 100.0%\n",
      "iteration: 150 | loss: 1.0150630821538138 | accuracy: 100.0%\n",
      "iteration: 151 | loss: 1.0776096312089585 | accuracy: 100.0%\n",
      "iteration: 152 | loss: 1.0031153536693709 | accuracy: 100.0%\n",
      "iteration: 153 | loss: 1.105126358799524 | accuracy: 100.0%\n",
      "iteration: 154 | loss: 1.0026227960514922 | accuracy: 100.0%\n",
      "iteration: 155 | loss: 1.003024186882625 | accuracy: 100.0%\n",
      "iteration: 156 | loss: 1.0056594448366827 | accuracy: 100.0%\n",
      "iteration: 157 | loss: 1.0037169399524952 | accuracy: 100.0%\n",
      "iteration: 158 | loss: 1.0056814872663757 | accuracy: 100.0%\n",
      "iteration: 159 | loss: 1.008408679212422 | accuracy: 100.0%\n",
      "iteration: 160 | loss: 1.0058110843598531 | accuracy: 100.0%\n",
      "iteration: 161 | loss: 1.5587010683481894 | accuracy: 50.0%\n",
      "iteration: 162 | loss: 1.995099563173647 | accuracy: 50.0%\n",
      "iteration: 163 | loss: 1.0035406243991196 | accuracy: 100.0%\n",
      "iteration: 164 | loss: 2.0008696562258024 | accuracy: 50.0%\n",
      "iteration: 165 | loss: 1.998218059648536 | accuracy: 50.0%\n",
      "iteration: 166 | loss: 1.1932988594119052 | accuracy: 100.0%\n",
      "iteration: 167 | loss: 1.004689431983619 | accuracy: 100.0%\n",
      "iteration: 168 | loss: 1.002494229920526 | accuracy: 100.0%\n",
      "iteration: 169 | loss: 1.8333105262054135 | accuracy: 50.0%\n",
      "iteration: 170 | loss: 1.817960312169462 | accuracy: 50.0%\n",
      "iteration: 171 | loss: 2.00310100287551 | accuracy: 50.0%\n",
      "iteration: 172 | loss: 1.029553519635593 | accuracy: 100.0%\n",
      "iteration: 173 | loss: 1.0024889461620843 | accuracy: 100.0%\n",
      "iteration: 174 | loss: 1.028412208765869 | accuracy: 100.0%\n",
      "iteration: 175 | loss: 1.0132638489389452 | accuracy: 100.0%\n",
      "iteration: 176 | loss: 1.0369861458764196 | accuracy: 100.0%\n",
      "iteration: 177 | loss: 1.991904453052638 | accuracy: 50.0%\n",
      "iteration: 178 | loss: 2.0024228810988878 | accuracy: 50.0%\n",
      "iteration: 179 | loss: 1.0031625593367461 | accuracy: 100.0%\n",
      "iteration: 180 | loss: 1.2439643334419033 | accuracy: 100.0%\n",
      "iteration: 181 | loss: 1.0032204084014031 | accuracy: 100.0%\n",
      "iteration: 182 | loss: 1.0042207372597993 | accuracy: 100.0%\n",
      "iteration: 183 | loss: 1.00754128101353 | accuracy: 100.0%\n",
      "iteration: 184 | loss: 1.004119400730504 | accuracy: 100.0%\n",
      "iteration: 185 | loss: 1.0041921689330724 | accuracy: 100.0%\n",
      "iteration: 186 | loss: 1.1928448204134094 | accuracy: 100.0%\n",
      "iteration: 187 | loss: 1.0062647326641603 | accuracy: 100.0%\n",
      "iteration: 188 | loss: 1.0159357436632366 | accuracy: 100.0%\n",
      "iteration: 189 | loss: 1.0036368998337868 | accuracy: 100.0%\n",
      "iteration: 190 | loss: 1.0405170655207552 | accuracy: 100.0%\n",
      "iteration: 191 | loss: 1.0033463176082358 | accuracy: 100.0%\n",
      "iteration: 192 | loss: 1.6938563676387306 | accuracy: 50.0%\n",
      "iteration: 193 | loss: 1.0492064900140894 | accuracy: 100.0%\n",
      "iteration: 194 | loss: 1.2586253172347124 | accuracy: 100.0%\n",
      "iteration: 195 | loss: 1.0036487309216446 | accuracy: 100.0%\n",
      "iteration: 196 | loss: 1.4954875060572004 | accuracy: 100.0%\n",
      "iteration: 197 | loss: 1.0045904967932442 | accuracy: 100.0%\n",
      "iteration: 198 | loss: 1.0089332869959247 | accuracy: 100.0%\n",
      "iteration: 199 | loss: 1.0025927679041244 | accuracy: 100.0%\n",
      "iteration: 200 | loss: 1.7501995552285328 | accuracy: 50.0%\n",
      "iteration: 201 | loss: 1.002847781944765 | accuracy: 100.0%\n",
      "iteration: 202 | loss: 1.0559671331350955 | accuracy: 100.0%\n",
      "iteration: 203 | loss: 1.0491810527078507 | accuracy: 100.0%\n",
      "iteration: 204 | loss: 1.9992006936453228 | accuracy: 50.0%\n",
      "iteration: 205 | loss: 1.007297825866641 | accuracy: 100.0%\n",
      "iteration: 206 | loss: 1.0067476473921924 | accuracy: 100.0%\n",
      "iteration: 207 | loss: 1.7423629997767545 | accuracy: 50.0%\n",
      "iteration: 208 | loss: 1.5333825042190858 | accuracy: 50.0%\n",
      "iteration: 209 | loss: 1.2964410245007765 | accuracy: 100.0%\n",
      "iteration: 210 | loss: 1.0058053844672892 | accuracy: 100.0%\n",
      "iteration: 211 | loss: 1.017502970178259 | accuracy: 100.0%\n",
      "iteration: 212 | loss: 1.0115304695170588 | accuracy: 100.0%\n",
      "iteration: 213 | loss: 1.0063055866770139 | accuracy: 100.0%\n",
      "iteration: 214 | loss: 1.8515003920928979 | accuracy: 50.0%\n",
      "iteration: 215 | loss: 1.0123735492913832 | accuracy: 100.0%\n",
      "iteration: 216 | loss: 1.018561965821755 | accuracy: 100.0%\n",
      "iteration: 217 | loss: 1.0088104925910009 | accuracy: 100.0%\n",
      "iteration: 218 | loss: 1.0036917797663705 | accuracy: 100.0%\n",
      "iteration: 219 | loss: 1.0040973072703296 | accuracy: 100.0%\n",
      "iteration: 220 | loss: 1.9952425463196577 | accuracy: 50.0%\n",
      "iteration: 221 | loss: 1.0093636167599673 | accuracy: 100.0%\n",
      "iteration: 222 | loss: 1.0052129261063105 | accuracy: 100.0%\n",
      "iteration: 223 | loss: 1.9901784581676951 | accuracy: 50.0%\n",
      "iteration: 224 | loss: 1.9795124576274343 | accuracy: 50.0%\n",
      "iteration: 225 | loss: 1.1164764390660429 | accuracy: 100.0%\n",
      "iteration: 226 | loss: 1.0040151558319026 | accuracy: 100.0%\n",
      "iteration: 227 | loss: 1.9983205721514958 | accuracy: 50.0%\n",
      "iteration: 228 | loss: 1.0401493982243535 | accuracy: 100.0%\n",
      "iteration: 229 | loss: 1.0323347420408293 | accuracy: 100.0%\n",
      "iteration: 230 | loss: 1.6508636105523808 | accuracy: 50.0%\n",
      "iteration: 231 | loss: 2.0005452295810358 | accuracy: 50.0%\n",
      "iteration: 232 | loss: 1.0909310018592246 | accuracy: 100.0%\n",
      "iteration: 233 | loss: 1.0033789333553351 | accuracy: 100.0%\n",
      "iteration: 234 | loss: 1.0988299788646736 | accuracy: 100.0%\n",
      "iteration: 235 | loss: 2.001923702458907 | accuracy: 50.0%\n",
      "iteration: 236 | loss: 1.6147509887561518 | accuracy: 50.0%\n",
      "iteration: 237 | loss: 1.0067067826787768 | accuracy: 100.0%\n",
      "iteration: 238 | loss: 1.003268764652063 | accuracy: 100.0%\n",
      "iteration: 239 | loss: 1.002441400414202 | accuracy: 100.0%\n",
      "iteration: 240 | loss: 1.006318517744263 | accuracy: 100.0%\n",
      "iteration: 241 | loss: 1.0016796869565643 | accuracy: 100.0%\n",
      "iteration: 242 | loss: 1.03198725468714 | accuracy: 100.0%\n",
      "iteration: 243 | loss: 1.0028496603853514 | accuracy: 100.0%\n",
      "iteration: 244 | loss: 1.024641670838205 | accuracy: 100.0%\n",
      "iteration: 245 | loss: 1.0061199745608818 | accuracy: 100.0%\n",
      "iteration: 246 | loss: 1.6514003131639337 | accuracy: 50.0%\n",
      "iteration: 247 | loss: 1.0026660865626362 | accuracy: 100.0%\n",
      "iteration: 248 | loss: 1.0030507823351469 | accuracy: 100.0%\n",
      "iteration: 249 | loss: 1.0042277975997778 | accuracy: 100.0%\n",
      "iteration: 250 | loss: 1.1959586458470617 | accuracy: 100.0%\n",
      "iteration: 251 | loss: 1.0136925572423598 | accuracy: 100.0%\n",
      "iteration: 252 | loss: 1.001330353757855 | accuracy: 100.0%\n",
      "iteration: 253 | loss: 1.0314282309871772 | accuracy: 100.0%\n",
      "iteration: 254 | loss: 2.0004915024774217 | accuracy: 50.0%\n",
      "iteration: 255 | loss: 1.9882904845092502 | accuracy: 50.0%\n",
      "iteration: 256 | loss: 1.0132799122998075 | accuracy: 100.0%\n",
      "iteration: 257 | loss: 1.0092305319406323 | accuracy: 100.0%\n",
      "iteration: 258 | loss: 1.0046856965044335 | accuracy: 100.0%\n",
      "iteration: 259 | loss: 1.0035534466922789 | accuracy: 100.0%\n",
      "iteration: 260 | loss: 1.0030834504654842 | accuracy: 100.0%\n",
      "iteration: 261 | loss: 1.0096889801034337 | accuracy: 100.0%\n",
      "iteration: 262 | loss: 1.0118654319113012 | accuracy: 100.0%\n",
      "iteration: 263 | loss: 1.9887098273053605 | accuracy: 50.0%\n",
      "iteration: 264 | loss: 1.0995366148990617 | accuracy: 100.0%\n",
      "iteration: 265 | loss: 1.016666530424229 | accuracy: 100.0%\n",
      "iteration: 266 | loss: 1.0057710063510832 | accuracy: 100.0%\n",
      "iteration: 267 | loss: 1.9994753835649084 | accuracy: 50.0%\n",
      "iteration: 268 | loss: 1.00359878542662 | accuracy: 100.0%\n",
      "iteration: 269 | loss: 1.0013883218755966 | accuracy: 100.0%\n",
      "iteration: 270 | loss: 1.9892728594506237 | accuracy: 50.0%\n",
      "iteration: 271 | loss: 1.2064464017547982 | accuracy: 100.0%\n",
      "iteration: 272 | loss: 1.0093672478419222 | accuracy: 100.0%\n",
      "iteration: 273 | loss: 1.006961512387113 | accuracy: 100.0%\n",
      "iteration: 274 | loss: 1.3520382788473888 | accuracy: 100.0%\n",
      "iteration: 275 | loss: 2.000721591045801 | accuracy: 50.0%\n",
      "iteration: 276 | loss: 2.000815043303475 | accuracy: 50.0%\n",
      "iteration: 277 | loss: 1.0013643112912163 | accuracy: 100.0%\n",
      "iteration: 278 | loss: 1.0018669624822159 | accuracy: 100.0%\n",
      "iteration: 279 | loss: 1.0036974933679994 | accuracy: 100.0%\n",
      "iteration: 280 | loss: 1.0013233350521742 | accuracy: 100.0%\n",
      "iteration: 281 | loss: 2.2740260632317377 | accuracy: 50.0%\n",
      "iteration: 282 | loss: 1.035262639161253 | accuracy: 100.0%\n",
      "iteration: 283 | loss: 1.5915864344277542 | accuracy: 50.0%\n",
      "iteration: 284 | loss: 1.0013433900997792 | accuracy: 100.0%\n",
      "iteration: 285 | loss: 1.5942360656393186 | accuracy: 50.0%\n",
      "iteration: 286 | loss: 1.0194218648582285 | accuracy: 100.0%\n",
      "iteration: 287 | loss: 1.0027727430074194 | accuracy: 100.0%\n",
      "iteration: 288 | loss: 1.009537091481423 | accuracy: 100.0%\n",
      "iteration: 289 | loss: 1.6914005081406132 | accuracy: 50.0%\n",
      "iteration: 290 | loss: 1.0345994440511286 | accuracy: 100.0%\n",
      "iteration: 291 | loss: 1.2864687096010299 | accuracy: 100.0%\n",
      "iteration: 292 | loss: 1.9808156824906893 | accuracy: 50.0%\n",
      "iteration: 293 | loss: 1.005376109490345 | accuracy: 100.0%\n",
      "iteration: 294 | loss: 1.0708433070217906 | accuracy: 100.0%\n",
      "iteration: 295 | loss: 1.0069052332822153 | accuracy: 100.0%\n",
      "iteration: 296 | loss: 1.9985306410696029 | accuracy: 50.0%\n",
      "iteration: 297 | loss: 1.001310639680191 | accuracy: 100.0%\n",
      "iteration: 298 | loss: 1.005896347264544 | accuracy: 100.0%\n",
      "iteration: 299 | loss: 1.4142523140216081 | accuracy: 100.0%\n",
      "iteration: 300 | loss: 1.0019820007725646 | accuracy: 100.0%\n",
      "iteration: 301 | loss: 1.0052194294583894 | accuracy: 100.0%\n",
      "iteration: 302 | loss: 2.319704843881723 | accuracy: 50.0%\n",
      "iteration: 303 | loss: 1.0015784651615876 | accuracy: 100.0%\n",
      "iteration: 304 | loss: 1.9927094642165375 | accuracy: 50.0%\n",
      "iteration: 305 | loss: 1.0547376127245307 | accuracy: 100.0%\n",
      "iteration: 306 | loss: 1.001286774006377 | accuracy: 100.0%\n",
      "iteration: 307 | loss: 1.0062536999412883 | accuracy: 100.0%\n",
      "iteration: 308 | loss: 1.0497568778620006 | accuracy: 100.0%\n",
      "iteration: 309 | loss: 1.0018445633664828 | accuracy: 100.0%\n",
      "iteration: 310 | loss: 1.0019615955015986 | accuracy: 100.0%\n",
      "iteration: 311 | loss: 1.0111136141553967 | accuracy: 100.0%\n",
      "iteration: 312 | loss: 1.003739984720499 | accuracy: 100.0%\n",
      "iteration: 313 | loss: 1.0079028304371265 | accuracy: 100.0%\n",
      "iteration: 314 | loss: 1.0027422826531973 | accuracy: 100.0%\n",
      "iteration: 315 | loss: 1.00827775274619 | accuracy: 100.0%\n",
      "iteration: 316 | loss: 1.9840086947665148 | accuracy: 50.0%\n",
      "iteration: 317 | loss: 1.0053748915274061 | accuracy: 100.0%\n",
      "iteration: 318 | loss: 1.002863091510684 | accuracy: 100.0%\n",
      "iteration: 319 | loss: 1.1810029885601598 | accuracy: 100.0%\n",
      "iteration: 320 | loss: 1.0017547062002417 | accuracy: 100.0%\n",
      "iteration: 321 | loss: 1.1005221683109112 | accuracy: 100.0%\n",
      "iteration: 322 | loss: 1.001707733650213 | accuracy: 100.0%\n",
      "iteration: 323 | loss: 1.9043343470590657 | accuracy: 50.0%\n",
      "iteration: 324 | loss: 1.9915028804449335 | accuracy: 50.0%\n",
      "iteration: 325 | loss: 1.0039729195549354 | accuracy: 100.0%\n",
      "iteration: 326 | loss: 1.032081654407187 | accuracy: 100.0%\n",
      "iteration: 327 | loss: 1.008402257408498 | accuracy: 100.0%\n",
      "iteration: 328 | loss: 1.774752308251665 | accuracy: 50.0%\n",
      "iteration: 329 | loss: 2.1705135776400546 | accuracy: 50.0%\n",
      "iteration: 330 | loss: 1.0082896425695431 | accuracy: 100.0%\n",
      "iteration: 331 | loss: 1.11056566397816 | accuracy: 100.0%\n",
      "iteration: 332 | loss: 1.4929462590306561 | accuracy: 100.0%\n",
      "iteration: 333 | loss: 1.0037791335412631 | accuracy: 100.0%\n",
      "iteration: 334 | loss: 1.0171293284608216 | accuracy: 100.0%\n",
      "iteration: 335 | loss: 1.1298561890968333 | accuracy: 100.0%\n",
      "iteration: 336 | loss: 1.0013042178066591 | accuracy: 100.0%\n",
      "iteration: 337 | loss: 2.012536896037185 | accuracy: 50.0%\n",
      "iteration: 338 | loss: 1.7743109205835983 | accuracy: 50.0%\n",
      "iteration: 339 | loss: 1.0039503767860207 | accuracy: 100.0%\n",
      "iteration: 340 | loss: 1.5940863808632497 | accuracy: 50.0%\n",
      "iteration: 341 | loss: 1.0426788580215363 | accuracy: 100.0%\n",
      "iteration: 342 | loss: 1.5627063752216008 | accuracy: 50.0%\n",
      "iteration: 343 | loss: 1.002944534186305 | accuracy: 100.0%\n",
      "iteration: 344 | loss: 1.0013671964146478 | accuracy: 100.0%\n",
      "iteration: 345 | loss: 1.0996891888058005 | accuracy: 100.0%\n",
      "iteration: 346 | loss: 1.0035702336222745 | accuracy: 100.0%\n",
      "iteration: 347 | loss: 1.875453897570246 | accuracy: 50.0%\n",
      "iteration: 348 | loss: 1.0069064905012752 | accuracy: 100.0%\n",
      "iteration: 349 | loss: 1.0371086698266256 | accuracy: 100.0%\n",
      "iteration: 350 | loss: 1.1106655277467632 | accuracy: 100.0%\n",
      "iteration: 351 | loss: 1.1049611817568443 | accuracy: 100.0%\n",
      "iteration: 352 | loss: 1.0644329990995085 | accuracy: 100.0%\n",
      "iteration: 353 | loss: 1.0052982235800614 | accuracy: 100.0%\n",
      "iteration: 354 | loss: 2.8994487902770167 | accuracy: 0.0%\n",
      "iteration: 355 | loss: 1.0105191345258109 | accuracy: 100.0%\n",
      "iteration: 356 | loss: 1.001359305127953 | accuracy: 100.0%\n",
      "iteration: 357 | loss: 1.0037168536061816 | accuracy: 100.0%\n",
      "iteration: 358 | loss: 1.0043281318541815 | accuracy: 100.0%\n",
      "iteration: 359 | loss: 1.0023989365881898 | accuracy: 100.0%\n",
      "iteration: 360 | loss: 1.0393270044533205 | accuracy: 100.0%\n",
      "iteration: 361 | loss: 1.0187102148106337 | accuracy: 100.0%\n",
      "iteration: 362 | loss: 1.2768338256760923 | accuracy: 100.0%\n",
      "iteration: 363 | loss: 1.1991157072668057 | accuracy: 100.0%\n",
      "iteration: 364 | loss: 2.0029428608645845 | accuracy: 50.0%\n",
      "iteration: 365 | loss: 1.3637788678761167 | accuracy: 100.0%\n",
      "iteration: 366 | loss: 1.0575440555479259 | accuracy: 100.0%\n",
      "iteration: 367 | loss: 1.0096622561261444 | accuracy: 100.0%\n",
      "iteration: 368 | loss: 1.1099659005988722 | accuracy: 100.0%\n",
      "iteration: 369 | loss: 1.0223097621000836 | accuracy: 100.0%\n",
      "iteration: 370 | loss: 1.091583485565647 | accuracy: 100.0%\n",
      "iteration: 371 | loss: 1.0041331729995677 | accuracy: 100.0%\n",
      "iteration: 372 | loss: 1.0757939416755593 | accuracy: 100.0%\n",
      "iteration: 373 | loss: 2.0538555938259084 | accuracy: 50.0%\n",
      "iteration: 374 | loss: 1.0016994885637425 | accuracy: 100.0%\n",
      "iteration: 375 | loss: 1.0130949438290546 | accuracy: 100.0%\n",
      "iteration: 376 | loss: 1.002374830824062 | accuracy: 100.0%\n",
      "iteration: 377 | loss: 1.9992097918242653 | accuracy: 50.0%\n",
      "iteration: 378 | loss: 1.001602722610326 | accuracy: 100.0%\n",
      "iteration: 379 | loss: 1.0034310584097927 | accuracy: 100.0%\n",
      "iteration: 380 | loss: 1.0031563062384903 | accuracy: 100.0%\n",
      "iteration: 381 | loss: 2.020907129632466 | accuracy: 50.0%\n",
      "iteration: 382 | loss: 1.0350625783498317 | accuracy: 100.0%\n",
      "iteration: 383 | loss: 1.0036998358718394 | accuracy: 100.0%\n",
      "iteration: 384 | loss: 1.5804188561305563 | accuracy: 100.0%\n",
      "iteration: 385 | loss: 1.004822656750172 | accuracy: 100.0%\n",
      "iteration: 386 | loss: 1.0076333754095284 | accuracy: 100.0%\n",
      "iteration: 387 | loss: 1.0370132243869092 | accuracy: 100.0%\n",
      "iteration: 388 | loss: 2.247094416951133 | accuracy: 50.0%\n",
      "iteration: 389 | loss: 1.0012152355060853 | accuracy: 100.0%\n",
      "iteration: 390 | loss: 1.0749345127035503 | accuracy: 100.0%\n",
      "iteration: 391 | loss: 1.1604876885608948 | accuracy: 100.0%\n",
      "iteration: 392 | loss: 2.001131547923478 | accuracy: 50.0%\n",
      "iteration: 393 | loss: 1.0162050512094283 | accuracy: 100.0%\n",
      "iteration: 394 | loss: 1.8116292451655525 | accuracy: 50.0%\n",
      "iteration: 395 | loss: 1.0012417812166614 | accuracy: 100.0%\n",
      "iteration: 396 | loss: 1.097802593999863 | accuracy: 100.0%\n",
      "iteration: 397 | loss: 2.003188727299348 | accuracy: 50.0%\n",
      "iteration: 398 | loss: 1.3477023948761175 | accuracy: 100.0%\n",
      "iteration: 399 | loss: 1.0061841587733213 | accuracy: 100.0%\n",
      "iteration: 400 | loss: 2.0005319093820533 | accuracy: 50.0%\n",
      "iteration: 401 | loss: 1.0049298223756291 | accuracy: 100.0%\n",
      "iteration: 402 | loss: 1.004919243316671 | accuracy: 100.0%\n",
      "iteration: 403 | loss: 1.7208196733332293 | accuracy: 50.0%\n",
      "iteration: 404 | loss: 1.0015669063662807 | accuracy: 100.0%\n",
      "iteration: 405 | loss: 1.0561328467420077 | accuracy: 100.0%\n",
      "iteration: 406 | loss: 1.64312309367972 | accuracy: 50.0%\n",
      "iteration: 407 | loss: 1.006667566046375 | accuracy: 100.0%\n",
      "iteration: 408 | loss: 1.0418757441334798 | accuracy: 100.0%\n",
      "iteration: 409 | loss: 1.0027139502273816 | accuracy: 100.0%\n",
      "iteration: 410 | loss: 1.9895743812692748 | accuracy: 50.0%\n",
      "iteration: 411 | loss: 1.0167763358868303 | accuracy: 100.0%\n",
      "iteration: 412 | loss: 1.0018663888561732 | accuracy: 100.0%\n",
      "iteration: 413 | loss: 1.0112921075830676 | accuracy: 100.0%\n",
      "iteration: 414 | loss: 2.019729782679748 | accuracy: 50.0%\n",
      "iteration: 415 | loss: 1.0057160526978812 | accuracy: 100.0%\n",
      "iteration: 416 | loss: 1.0973076622537565 | accuracy: 100.0%\n",
      "iteration: 417 | loss: 1.003381869575492 | accuracy: 100.0%\n",
      "iteration: 418 | loss: 1.0169247104431522 | accuracy: 100.0%\n",
      "iteration: 419 | loss: 1.0018974924955568 | accuracy: 100.0%\n",
      "iteration: 420 | loss: 1.001539107232031 | accuracy: 100.0%\n",
      "iteration: 421 | loss: 1.0039583678244737 | accuracy: 100.0%\n",
      "iteration: 422 | loss: 1.0024300516954776 | accuracy: 100.0%\n",
      "iteration: 423 | loss: 1.0025452662553582 | accuracy: 100.0%\n",
      "iteration: 424 | loss: 1.0013190629733097 | accuracy: 100.0%\n",
      "iteration: 425 | loss: 1.0227747449823092 | accuracy: 100.0%\n",
      "iteration: 426 | loss: 1.0025902014002506 | accuracy: 100.0%\n",
      "iteration: 427 | loss: 1.0137224999671945 | accuracy: 100.0%\n",
      "iteration: 428 | loss: 1.0038057106771907 | accuracy: 100.0%\n",
      "iteration: 429 | loss: 1.002826370268574 | accuracy: 100.0%\n",
      "iteration: 430 | loss: 1.0029599454250384 | accuracy: 100.0%\n",
      "iteration: 431 | loss: 1.0031157408667983 | accuracy: 100.0%\n",
      "iteration: 432 | loss: 1.0888708943010323 | accuracy: 100.0%\n",
      "iteration: 433 | loss: 1.99278923205263 | accuracy: 50.0%\n",
      "iteration: 434 | loss: 1.00282666476238 | accuracy: 100.0%\n",
      "iteration: 435 | loss: 1.9275768768890056 | accuracy: 50.0%\n",
      "iteration: 436 | loss: 1.5930036797177802 | accuracy: 50.0%\n",
      "iteration: 437 | loss: 1.0049621957489725 | accuracy: 100.0%\n",
      "iteration: 438 | loss: 1.0035856184517629 | accuracy: 100.0%\n",
      "iteration: 439 | loss: 1.0014605294191947 | accuracy: 100.0%\n",
      "iteration: 440 | loss: 1.0020332331963968 | accuracy: 100.0%\n",
      "iteration: 441 | loss: 1.0022218858029017 | accuracy: 100.0%\n",
      "iteration: 442 | loss: 1.0175308532725509 | accuracy: 100.0%\n",
      "iteration: 443 | loss: 1.0029488261833353 | accuracy: 100.0%\n",
      "iteration: 444 | loss: 1.0016791345376916 | accuracy: 100.0%\n",
      "iteration: 445 | loss: 1.0048328874031827 | accuracy: 100.0%\n",
      "iteration: 446 | loss: 1.0024532986433425 | accuracy: 100.0%\n",
      "iteration: 447 | loss: 1.0014911375372928 | accuracy: 100.0%\n",
      "iteration: 448 | loss: 1.1178781387002248 | accuracy: 100.0%\n",
      "iteration: 449 | loss: 1.0263792387886188 | accuracy: 100.0%\n",
      "iteration: 450 | loss: 1.001924374544239 | accuracy: 100.0%\n",
      "iteration: 451 | loss: 1.0012909667324121 | accuracy: 100.0%\n",
      "iteration: 452 | loss: 1.001520703972185 | accuracy: 100.0%\n",
      "iteration: 453 | loss: 1.429358395323364 | accuracy: 100.0%\n",
      "iteration: 454 | loss: 1.9640042606553882 | accuracy: 50.0%\n",
      "iteration: 455 | loss: 1.0025645890933912 | accuracy: 100.0%\n",
      "iteration: 456 | loss: 1.8726718608470403 | accuracy: 50.0%\n",
      "iteration: 457 | loss: 1.1849610738142902 | accuracy: 100.0%\n",
      "iteration: 458 | loss: 1.0587171348981712 | accuracy: 100.0%\n",
      "iteration: 459 | loss: 1.0873921400507511 | accuracy: 100.0%\n",
      "iteration: 460 | loss: 1.8571533639457387 | accuracy: 50.0%\n",
      "iteration: 461 | loss: 1.0050168157175234 | accuracy: 100.0%\n",
      "iteration: 462 | loss: 1.0488745557470953 | accuracy: 100.0%\n",
      "iteration: 463 | loss: 1.7278677226967134 | accuracy: 50.0%\n",
      "iteration: 464 | loss: 1.0053749947969104 | accuracy: 100.0%\n",
      "iteration: 465 | loss: 1.027350775542637 | accuracy: 100.0%\n",
      "iteration: 466 | loss: 1.062992139726456 | accuracy: 100.0%\n",
      "iteration: 467 | loss: 1.0038384621853362 | accuracy: 100.0%\n",
      "iteration: 468 | loss: 2.459273408167197 | accuracy: 50.0%\n",
      "iteration: 469 | loss: 1.0033710756037857 | accuracy: 100.0%\n",
      "iteration: 470 | loss: 1.0198229658983438 | accuracy: 100.0%\n",
      "iteration: 471 | loss: 1.002156706438549 | accuracy: 100.0%\n",
      "iteration: 472 | loss: 1.0013290663403418 | accuracy: 100.0%\n",
      "iteration: 473 | loss: 1.001298073892059 | accuracy: 100.0%\n",
      "iteration: 474 | loss: 1.0092464718762844 | accuracy: 100.0%\n",
      "iteration: 475 | loss: 1.1580592356400032 | accuracy: 100.0%\n",
      "iteration: 476 | loss: 1.001280438520456 | accuracy: 100.0%\n",
      "iteration: 477 | loss: 1.0015370663883238 | accuracy: 100.0%\n",
      "iteration: 478 | loss: 2.0008053434439703 | accuracy: 50.0%\n",
      "iteration: 479 | loss: 1.0047678262515545 | accuracy: 100.0%\n",
      "iteration: 480 | loss: 1.003570425190097 | accuracy: 100.0%\n",
      "iteration: 481 | loss: 1.0034378487995534 | accuracy: 100.0%\n",
      "iteration: 482 | loss: 1.8879772783391267 | accuracy: 50.0%\n",
      "iteration: 483 | loss: 1.0062418924323522 | accuracy: 100.0%\n",
      "iteration: 484 | loss: 1.0013642448766884 | accuracy: 100.0%\n",
      "iteration: 485 | loss: 1.0041057217595188 | accuracy: 100.0%\n",
      "iteration: 486 | loss: 1.0611635205850585 | accuracy: 100.0%\n",
      "iteration: 487 | loss: 1.0012472884529098 | accuracy: 100.0%\n",
      "iteration: 488 | loss: 1.9862801340186809 | accuracy: 50.0%\n",
      "iteration: 489 | loss: 1.007883182838081 | accuracy: 100.0%\n",
      "iteration: 490 | loss: 1.7545549962394282 | accuracy: 50.0%\n",
      "iteration: 491 | loss: 1.0014106120652326 | accuracy: 100.0%\n",
      "iteration: 492 | loss: 1.013626275530196 | accuracy: 100.0%\n",
      "iteration: 493 | loss: 1.0017173977901777 | accuracy: 100.0%\n",
      "iteration: 494 | loss: 1.8502724733218403 | accuracy: 50.0%\n",
      "iteration: 495 | loss: 1.001865995504256 | accuracy: 100.0%\n",
      "iteration: 496 | loss: 1.0022663795097169 | accuracy: 100.0%\n",
      "iteration: 497 | loss: 1.0014618664714194 | accuracy: 100.0%\n",
      "iteration: 498 | loss: 1.0049206407355262 | accuracy: 100.0%\n",
      "iteration: 499 | loss: 1.0026379428003507 | accuracy: 100.0%\n",
      "iteration: 500 | loss: 1.015274945358726 | accuracy: 100.0%\n",
      "iteration: 501 | loss: 1.001275793541887 | accuracy: 100.0%\n",
      "iteration: 502 | loss: 1.1117387196302553 | accuracy: 100.0%\n",
      "iteration: 503 | loss: 2.0086419695007502 | accuracy: 50.0%\n",
      "iteration: 504 | loss: 1.793132842686199 | accuracy: 50.0%\n",
      "iteration: 505 | loss: 1.8563940744960208 | accuracy: 50.0%\n",
      "iteration: 506 | loss: 1.4124923668844327 | accuracy: 100.0%\n",
      "iteration: 507 | loss: 1.9608054750736108 | accuracy: 50.0%\n",
      "iteration: 508 | loss: 1.2052377841078954 | accuracy: 100.0%\n",
      "iteration: 509 | loss: 1.449838601948483 | accuracy: 100.0%\n",
      "iteration: 510 | loss: 1.0192352849419235 | accuracy: 100.0%\n",
      "iteration: 511 | loss: 1.0015503554505758 | accuracy: 100.0%\n",
      "iteration: 512 | loss: 1.0056081498408511 | accuracy: 100.0%\n",
      "iteration: 513 | loss: 1.1457167014377274 | accuracy: 100.0%\n",
      "iteration: 514 | loss: 1.0025392046281214 | accuracy: 100.0%\n",
      "iteration: 515 | loss: 1.0116710835398308 | accuracy: 100.0%\n",
      "iteration: 516 | loss: 1.0253220224190995 | accuracy: 100.0%\n",
      "iteration: 517 | loss: 1.0065492180960562 | accuracy: 100.0%\n",
      "iteration: 518 | loss: 1.0038692982090869 | accuracy: 100.0%\n",
      "iteration: 519 | loss: 1.0217023694480762 | accuracy: 100.0%\n",
      "iteration: 520 | loss: 1.0067416990371778 | accuracy: 100.0%\n",
      "iteration: 521 | loss: 1.0037097804545712 | accuracy: 100.0%\n",
      "iteration: 522 | loss: 1.021487631990866 | accuracy: 100.0%\n",
      "iteration: 523 | loss: 1.003763795123177 | accuracy: 100.0%\n",
      "iteration: 524 | loss: 1.030303473167447 | accuracy: 100.0%\n",
      "iteration: 525 | loss: 1.0052294342892574 | accuracy: 100.0%\n",
      "iteration: 526 | loss: 1.0013414548363695 | accuracy: 100.0%\n",
      "iteration: 527 | loss: 1.019335105570833 | accuracy: 100.0%\n",
      "iteration: 528 | loss: 1.0015504733092844 | accuracy: 100.0%\n",
      "iteration: 529 | loss: 1.1804323440708087 | accuracy: 100.0%\n",
      "iteration: 530 | loss: 1.0033172488071698 | accuracy: 100.0%\n",
      "iteration: 531 | loss: 1.0687215082691925 | accuracy: 100.0%\n",
      "iteration: 532 | loss: 1.00245538745788 | accuracy: 100.0%\n",
      "iteration: 533 | loss: 1.02421982001172 | accuracy: 100.0%\n",
      "iteration: 534 | loss: 1.0063224962783117 | accuracy: 100.0%\n",
      "iteration: 535 | loss: 1.9661269009134825 | accuracy: 50.0%\n",
      "iteration: 536 | loss: 1.003352574423374 | accuracy: 100.0%\n",
      "iteration: 537 | loss: 1.0217932056359693 | accuracy: 100.0%\n",
      "iteration: 538 | loss: 1.0017894050689025 | accuracy: 100.0%\n",
      "iteration: 539 | loss: 1.0027963695627125 | accuracy: 100.0%\n",
      "iteration: 540 | loss: 1.0019902392595597 | accuracy: 100.0%\n",
      "iteration: 541 | loss: 1.9288978876734797 | accuracy: 50.0%\n",
      "iteration: 542 | loss: 1.0020878410083198 | accuracy: 100.0%\n",
      "iteration: 543 | loss: 1.8806703818814472 | accuracy: 50.0%\n",
      "iteration: 544 | loss: 1.0721254700737397 | accuracy: 100.0%\n",
      "iteration: 545 | loss: 1.0301380166111376 | accuracy: 100.0%\n",
      "iteration: 546 | loss: 1.002798586908153 | accuracy: 100.0%\n",
      "iteration: 547 | loss: 1.00168960781028 | accuracy: 100.0%\n",
      "iteration: 548 | loss: 1.0047806619896573 | accuracy: 100.0%\n",
      "iteration: 549 | loss: 1.1795164724438538 | accuracy: 100.0%\n",
      "iteration: 550 | loss: 1.0026164932333983 | accuracy: 100.0%\n",
      "iteration: 551 | loss: 1.0276167597656651 | accuracy: 100.0%\n",
      "iteration: 552 | loss: 1.0312846899600587 | accuracy: 100.0%\n",
      "iteration: 553 | loss: 1.0030907699884573 | accuracy: 100.0%\n",
      "iteration: 554 | loss: 1.7834229460805266 | accuracy: 50.0%\n",
      "iteration: 555 | loss: 1.0067199636213475 | accuracy: 100.0%\n",
      "iteration: 556 | loss: 1.0042755687402478 | accuracy: 100.0%\n",
      "iteration: 557 | loss: 1.0250095732587061 | accuracy: 100.0%\n",
      "iteration: 558 | loss: 1.0033047846164684 | accuracy: 100.0%\n",
      "iteration: 559 | loss: 1.004598852714454 | accuracy: 100.0%\n",
      "iteration: 560 | loss: 1.00794927781348 | accuracy: 100.0%\n",
      "iteration: 561 | loss: 1.0031051725164823 | accuracy: 100.0%\n",
      "iteration: 562 | loss: 1.0846818525279784 | accuracy: 100.0%\n",
      "iteration: 563 | loss: 1.0078875711308295 | accuracy: 100.0%\n",
      "iteration: 564 | loss: 1.0038068790144168 | accuracy: 100.0%\n",
      "iteration: 565 | loss: 1.9699450758218622 | accuracy: 50.0%\n",
      "iteration: 566 | loss: 1.1136869325163046 | accuracy: 100.0%\n",
      "iteration: 567 | loss: 1.0052244053258739 | accuracy: 100.0%\n",
      "iteration: 568 | loss: 1.0013310449160835 | accuracy: 100.0%\n",
      "iteration: 569 | loss: 1.023713741249835 | accuracy: 100.0%\n",
      "iteration: 570 | loss: 2.0020385184841096 | accuracy: 50.0%\n",
      "iteration: 571 | loss: 1.0024746526984114 | accuracy: 100.0%\n",
      "iteration: 572 | loss: 1.9092381537672993 | accuracy: 50.0%\n",
      "iteration: 573 | loss: 1.0164852267197755 | accuracy: 100.0%\n",
      "iteration: 574 | loss: 1.0020535097560426 | accuracy: 100.0%\n",
      "iteration: 575 | loss: 1.081838669621359 | accuracy: 100.0%\n",
      "iteration: 576 | loss: 1.0026563668794553 | accuracy: 100.0%\n",
      "iteration: 577 | loss: 1.0030965740740245 | accuracy: 100.0%\n",
      "iteration: 578 | loss: 1.1900258497158724 | accuracy: 100.0%\n",
      "iteration: 579 | loss: 1.0021197612708959 | accuracy: 100.0%\n",
      "iteration: 580 | loss: 1.0383185299294753 | accuracy: 100.0%\n",
      "iteration: 581 | loss: 1.0299252798186633 | accuracy: 100.0%\n",
      "iteration: 582 | loss: 1.001402665653206 | accuracy: 100.0%\n",
      "iteration: 583 | loss: 1.0082442634481226 | accuracy: 100.0%\n",
      "iteration: 584 | loss: 1.0121379497539762 | accuracy: 100.0%\n",
      "iteration: 585 | loss: 1.001595945477666 | accuracy: 100.0%\n",
      "iteration: 586 | loss: 1.0037422282488897 | accuracy: 100.0%\n",
      "iteration: 587 | loss: 1.0015826316614467 | accuracy: 100.0%\n",
      "iteration: 588 | loss: 1.9996025598247085 | accuracy: 50.0%\n",
      "iteration: 589 | loss: 1.9381306801044293 | accuracy: 50.0%\n",
      "iteration: 590 | loss: 1.006386787123425 | accuracy: 100.0%\n",
      "iteration: 591 | loss: 1.0042834222637331 | accuracy: 100.0%\n",
      "iteration: 592 | loss: 1.9684509602270095 | accuracy: 50.0%\n",
      "iteration: 593 | loss: 1.0049236062770357 | accuracy: 100.0%\n",
      "iteration: 594 | loss: 1.0725953559296628 | accuracy: 100.0%\n",
      "iteration: 595 | loss: 2.0005899049681104 | accuracy: 50.0%\n",
      "iteration: 596 | loss: 1.002881794036685 | accuracy: 100.0%\n",
      "iteration: 597 | loss: 1.001331909504265 | accuracy: 100.0%\n",
      "iteration: 598 | loss: 1.0152506516386945 | accuracy: 100.0%\n",
      "iteration: 599 | loss: 1.1178356720118725 | accuracy: 100.0%\n",
      "iteration: 600 | loss: 1.0027440567358075 | accuracy: 100.0%\n",
      "iteration: 601 | loss: 1.1737068839694744 | accuracy: 100.0%\n",
      "iteration: 602 | loss: 1.0024154858611805 | accuracy: 100.0%\n",
      "iteration: 603 | loss: 1.001333181725347 | accuracy: 100.0%\n",
      "iteration: 604 | loss: 1.852116354242609 | accuracy: 50.0%\n",
      "iteration: 605 | loss: 1.0026617255084667 | accuracy: 100.0%\n",
      "iteration: 606 | loss: 1.0024439718877565 | accuracy: 100.0%\n",
      "iteration: 607 | loss: 1.0061549748965464 | accuracy: 100.0%\n",
      "iteration: 608 | loss: 1.0208689330684209 | accuracy: 100.0%\n",
      "iteration: 609 | loss: 1.0032416571202385 | accuracy: 100.0%\n",
      "iteration: 610 | loss: 1.001863462636509 | accuracy: 100.0%\n",
      "iteration: 611 | loss: 1.001753164602404 | accuracy: 100.0%\n",
      "iteration: 612 | loss: 1.0109511961082345 | accuracy: 100.0%\n",
      "iteration: 613 | loss: 2.065236339603902 | accuracy: 50.0%\n",
      "iteration: 614 | loss: 1.8205471163465403 | accuracy: 50.0%\n",
      "iteration: 615 | loss: 1.0110243899856852 | accuracy: 100.0%\n",
      "iteration: 616 | loss: 1.3262600854498725 | accuracy: 100.0%\n",
      "iteration: 617 | loss: 1.0143508065424347 | accuracy: 100.0%\n",
      "iteration: 618 | loss: 1.0013542148395995 | accuracy: 100.0%\n",
      "iteration: 619 | loss: 1.929659273397191 | accuracy: 50.0%\n",
      "iteration: 620 | loss: 1.0172651057966682 | accuracy: 100.0%\n",
      "iteration: 621 | loss: 1.9306977135574201 | accuracy: 50.0%\n",
      "iteration: 622 | loss: 1.0088889134849561 | accuracy: 100.0%\n",
      "iteration: 623 | loss: 1.0058081277598594 | accuracy: 100.0%\n",
      "iteration: 624 | loss: 2.858808127342997 | accuracy: 0.0%\n",
      "iteration: 625 | loss: 1.0024156336683372 | accuracy: 100.0%\n",
      "iteration: 626 | loss: 1.0474409010810573 | accuracy: 100.0%\n",
      "iteration: 627 | loss: 1.0017126667719105 | accuracy: 100.0%\n",
      "iteration: 628 | loss: 1.9592763252221301 | accuracy: 50.0%\n",
      "iteration: 629 | loss: 1.002687114301549 | accuracy: 100.0%\n",
      "iteration: 630 | loss: 1.016248650045825 | accuracy: 100.0%\n",
      "iteration: 631 | loss: 1.0028066200549441 | accuracy: 100.0%\n",
      "iteration: 632 | loss: 1.9381954545867528 | accuracy: 50.0%\n",
      "iteration: 633 | loss: 1.0203181551532514 | accuracy: 100.0%\n",
      "iteration: 634 | loss: 1.936698204724421 | accuracy: 50.0%\n",
      "iteration: 635 | loss: 1.003524160952065 | accuracy: 100.0%\n",
      "iteration: 636 | loss: 1.0083791359997913 | accuracy: 100.0%\n",
      "iteration: 637 | loss: 1.238525040983158 | accuracy: 100.0%\n",
      "iteration: 638 | loss: 1.9179615978297753 | accuracy: 50.0%\n",
      "iteration: 639 | loss: 1.3879646723656338 | accuracy: 100.0%\n",
      "iteration: 640 | loss: 2.1251858146994618 | accuracy: 50.0%\n",
      "iteration: 641 | loss: 1.7478117036597518 | accuracy: 50.0%\n",
      "iteration: 642 | loss: 1.0533990632983208 | accuracy: 100.0%\n",
      "iteration: 643 | loss: 1.0019779160957487 | accuracy: 100.0%\n",
      "iteration: 644 | loss: 1.0196711425958924 | accuracy: 100.0%\n",
      "iteration: 645 | loss: 1.1121186250596977 | accuracy: 100.0%\n",
      "iteration: 646 | loss: 1.837825379980226 | accuracy: 50.0%\n",
      "iteration: 647 | loss: 1.019105454580688 | accuracy: 100.0%\n",
      "iteration: 648 | loss: 1.0170859517551472 | accuracy: 100.0%\n",
      "iteration: 649 | loss: 1.0252257343079656 | accuracy: 100.0%\n",
      "iteration: 650 | loss: 1.164501348442374 | accuracy: 100.0%\n",
      "iteration: 651 | loss: 1.0203622382910351 | accuracy: 100.0%\n",
      "iteration: 652 | loss: 1.0203497615084716 | accuracy: 100.0%\n",
      "iteration: 653 | loss: 1.0015044195745317 | accuracy: 100.0%\n",
      "iteration: 654 | loss: 1.6812586569978283 | accuracy: 50.0%\n",
      "iteration: 655 | loss: 1.1129296704969962 | accuracy: 100.0%\n",
      "iteration: 656 | loss: 1.0114765523772695 | accuracy: 100.0%\n",
      "iteration: 657 | loss: 1.0325319805138762 | accuracy: 100.0%\n",
      "iteration: 658 | loss: 1.040736797989427 | accuracy: 100.0%\n",
      "iteration: 659 | loss: 1.0856379197632886 | accuracy: 100.0%\n",
      "iteration: 660 | loss: 1.005112821768357 | accuracy: 100.0%\n",
      "iteration: 661 | loss: 1.9535315880786774 | accuracy: 50.0%\n",
      "iteration: 662 | loss: 1.9830488310835572 | accuracy: 50.0%\n",
      "iteration: 663 | loss: 1.0197061833990269 | accuracy: 100.0%\n",
      "iteration: 664 | loss: 1.0102499460382586 | accuracy: 100.0%\n",
      "iteration: 665 | loss: 2.5876391183139322 | accuracy: 0.0%\n",
      "iteration: 666 | loss: 1.07980150019648 | accuracy: 100.0%\n",
      "iteration: 667 | loss: 1.0187533644688362 | accuracy: 100.0%\n",
      "iteration: 668 | loss: 1.0999997761630562 | accuracy: 100.0%\n",
      "iteration: 669 | loss: 1.2771748626425625 | accuracy: 100.0%\n",
      "iteration: 670 | loss: 1.0226236129869681 | accuracy: 100.0%\n",
      "iteration: 671 | loss: 1.002010131087896 | accuracy: 100.0%\n",
      "iteration: 672 | loss: 1.0033538334596503 | accuracy: 100.0%\n",
      "iteration: 673 | loss: 2.7002424452342297 | accuracy: 0.0%\n",
      "iteration: 674 | loss: 1.2236294797496385 | accuracy: 100.0%\n",
      "iteration: 675 | loss: 1.0154431922341574 | accuracy: 100.0%\n",
      "iteration: 676 | loss: 1.0023523249867825 | accuracy: 100.0%\n",
      "iteration: 677 | loss: 1.068298786424063 | accuracy: 100.0%\n",
      "iteration: 678 | loss: 1.5630244817168757 | accuracy: 50.0%\n",
      "iteration: 679 | loss: 1.235571142982085 | accuracy: 100.0%\n",
      "iteration: 680 | loss: 1.0014083791179615 | accuracy: 100.0%\n",
      "iteration: 681 | loss: 1.0050902180155628 | accuracy: 100.0%\n",
      "iteration: 682 | loss: 1.947901955420313 | accuracy: 50.0%\n",
      "iteration: 683 | loss: 1.0035465064189688 | accuracy: 100.0%\n",
      "iteration: 684 | loss: 1.0063578818445804 | accuracy: 100.0%\n",
      "iteration: 685 | loss: 1.0078955748448173 | accuracy: 100.0%\n",
      "iteration: 686 | loss: 1.0039046943880747 | accuracy: 100.0%\n",
      "iteration: 687 | loss: 1.0017167273925802 | accuracy: 100.0%\n",
      "iteration: 688 | loss: 1.0026445279236638 | accuracy: 100.0%\n",
      "iteration: 689 | loss: 1.0018074749519914 | accuracy: 100.0%\n",
      "iteration: 690 | loss: 1.0137589335058805 | accuracy: 100.0%\n",
      "iteration: 691 | loss: 1.0265723726931117 | accuracy: 100.0%\n",
      "iteration: 692 | loss: 1.0934800064203773 | accuracy: 100.0%\n",
      "iteration: 693 | loss: 1.8687738507715541 | accuracy: 50.0%\n",
      "iteration: 694 | loss: 1.94444540389546 | accuracy: 50.0%\n",
      "iteration: 695 | loss: 1.0822986429749022 | accuracy: 100.0%\n",
      "iteration: 696 | loss: 1.0519115885115367 | accuracy: 100.0%\n",
      "iteration: 697 | loss: 1.0104907798258325 | accuracy: 100.0%\n",
      "iteration: 698 | loss: 1.0834081938606654 | accuracy: 100.0%\n",
      "iteration: 699 | loss: 1.0052064705158978 | accuracy: 100.0%\n",
      "iteration: 700 | loss: 1.0457242124445523 | accuracy: 100.0%\n",
      "iteration: 701 | loss: 1.0033834006452662 | accuracy: 100.0%\n",
      "iteration: 702 | loss: 1.0135120349561486 | accuracy: 100.0%\n",
      "iteration: 703 | loss: 1.0137338131839335 | accuracy: 100.0%\n",
      "iteration: 704 | loss: 1.185502000525591 | accuracy: 100.0%\n",
      "iteration: 705 | loss: 1.9964252127507878 | accuracy: 50.0%\n",
      "iteration: 706 | loss: 1.0075493842653203 | accuracy: 100.0%\n",
      "iteration: 707 | loss: 1.536437602248512 | accuracy: 50.0%\n",
      "iteration: 708 | loss: 2.0102778249323405 | accuracy: 50.0%\n",
      "iteration: 709 | loss: 1.0129136692517933 | accuracy: 100.0%\n",
      "iteration: 710 | loss: 1.0132587596131166 | accuracy: 100.0%\n",
      "iteration: 711 | loss: 1.1952093915115063 | accuracy: 100.0%\n",
      "iteration: 712 | loss: 1.004176692754996 | accuracy: 100.0%\n",
      "iteration: 713 | loss: 1.010528966812133 | accuracy: 100.0%\n",
      "iteration: 714 | loss: 1.0217943352788377 | accuracy: 100.0%\n",
      "iteration: 715 | loss: 1.035246859422757 | accuracy: 100.0%\n",
      "iteration: 716 | loss: 1.0015231066695627 | accuracy: 100.0%\n",
      "iteration: 717 | loss: 1.0706921902492854 | accuracy: 100.0%\n",
      "iteration: 718 | loss: 1.9961275862150119 | accuracy: 50.0%\n",
      "iteration: 719 | loss: 2.007759356695783 | accuracy: 50.0%\n",
      "iteration: 720 | loss: 1.002383484092114 | accuracy: 100.0%\n",
      "iteration: 721 | loss: 1.0076504561833723 | accuracy: 100.0%\n",
      "iteration: 722 | loss: 1.0087945791946398 | accuracy: 100.0%\n",
      "iteration: 723 | loss: 1.0032258565674785 | accuracy: 100.0%\n",
      "iteration: 724 | loss: 1.4765017585181974 | accuracy: 100.0%\n",
      "iteration: 725 | loss: 1.0041066907638048 | accuracy: 100.0%\n",
      "iteration: 726 | loss: 1.122537838844996 | accuracy: 100.0%\n",
      "iteration: 727 | loss: 1.0262255255288666 | accuracy: 100.0%\n",
      "iteration: 728 | loss: 1.8897424473153788 | accuracy: 50.0%\n",
      "iteration: 729 | loss: 1.020814938698388 | accuracy: 100.0%\n",
      "iteration: 730 | loss: 1.0124895696283436 | accuracy: 100.0%\n",
      "iteration: 731 | loss: 1.0545644964651852 | accuracy: 100.0%\n",
      "iteration: 732 | loss: 1.0617235594909489 | accuracy: 100.0%\n",
      "iteration: 733 | loss: 1.0021664853282652 | accuracy: 100.0%\n",
      "iteration: 734 | loss: 1.0437195771690781 | accuracy: 100.0%\n",
      "iteration: 735 | loss: 1.5918536859732106 | accuracy: 50.0%\n",
      "iteration: 736 | loss: 1.1177726736462665 | accuracy: 100.0%\n",
      "iteration: 737 | loss: 1.1114357604643677 | accuracy: 100.0%\n",
      "iteration: 738 | loss: 1.0472775173276392 | accuracy: 100.0%\n",
      "iteration: 739 | loss: 1.7459812727138926 | accuracy: 50.0%\n",
      "iteration: 740 | loss: 1.0393468497045033 | accuracy: 100.0%\n",
      "iteration: 741 | loss: 1.0385838619459418 | accuracy: 100.0%\n",
      "iteration: 742 | loss: 1.023291143115235 | accuracy: 100.0%\n",
      "iteration: 743 | loss: 2.0130155452460565 | accuracy: 50.0%\n",
      "iteration: 744 | loss: 1.0130825012788658 | accuracy: 100.0%\n",
      "iteration: 745 | loss: 1.0157544309046802 | accuracy: 100.0%\n",
      "iteration: 746 | loss: 1.036755938195143 | accuracy: 100.0%\n",
      "iteration: 747 | loss: 1.1291785186882106 | accuracy: 100.0%\n",
      "iteration: 748 | loss: 1.017062099099402 | accuracy: 100.0%\n",
      "iteration: 749 | loss: 1.0088632268609148 | accuracy: 100.0%\n",
      "iteration: 750 | loss: 1.0045375690348433 | accuracy: 100.0%\n",
      "iteration: 751 | loss: 1.0196896412640248 | accuracy: 100.0%\n",
      "iteration: 752 | loss: 1.1210063404072346 | accuracy: 100.0%\n",
      "iteration: 753 | loss: 1.0331994522879202 | accuracy: 100.0%\n",
      "iteration: 754 | loss: 1.1062725204600454 | accuracy: 100.0%\n",
      "iteration: 755 | loss: 1.002735972142209 | accuracy: 100.0%\n",
      "iteration: 756 | loss: 1.9215858357617444 | accuracy: 50.0%\n",
      "iteration: 757 | loss: 1.0150425048679657 | accuracy: 100.0%\n",
      "iteration: 758 | loss: 1.0045354625049594 | accuracy: 100.0%\n",
      "iteration: 759 | loss: 1.007354822581236 | accuracy: 100.0%\n",
      "iteration: 760 | loss: 1.8204497207470935 | accuracy: 50.0%\n",
      "iteration: 761 | loss: 1.0464376730938938 | accuracy: 100.0%\n",
      "iteration: 762 | loss: 1.006747054741794 | accuracy: 100.0%\n",
      "iteration: 763 | loss: 1.0163092096213155 | accuracy: 100.0%\n",
      "iteration: 764 | loss: 1.010474874439401 | accuracy: 100.0%\n",
      "iteration: 765 | loss: 1.0049935158614456 | accuracy: 100.0%\n",
      "iteration: 766 | loss: 1.0025878066993708 | accuracy: 100.0%\n",
      "iteration: 767 | loss: 1.0024032714681084 | accuracy: 100.0%\n",
      "iteration: 768 | loss: 1.0231849987924695 | accuracy: 100.0%\n",
      "iteration: 769 | loss: 1.3375026700066084 | accuracy: 100.0%\n",
      "iteration: 770 | loss: 1.0054650736202637 | accuracy: 100.0%\n",
      "iteration: 771 | loss: 1.1962888920124934 | accuracy: 100.0%\n",
      "iteration: 772 | loss: 1.003951706080763 | accuracy: 100.0%\n",
      "iteration: 773 | loss: 1.0093038050813044 | accuracy: 100.0%\n",
      "iteration: 774 | loss: 1.0025336365796897 | accuracy: 100.0%\n",
      "iteration: 775 | loss: 1.0040143780602 | accuracy: 100.0%\n",
      "iteration: 776 | loss: 1.0059544656110264 | accuracy: 100.0%\n",
      "iteration: 777 | loss: 1.0056729702525533 | accuracy: 100.0%\n",
      "iteration: 778 | loss: 1.0029058438927478 | accuracy: 100.0%\n",
      "iteration: 779 | loss: 1.0118169789158509 | accuracy: 100.0%\n",
      "iteration: 780 | loss: 1.981093392918071 | accuracy: 50.0%\n",
      "iteration: 781 | loss: 1.282115463447062 | accuracy: 100.0%\n",
      "iteration: 782 | loss: 1.0034034749472445 | accuracy: 100.0%\n",
      "iteration: 783 | loss: 1.002836584400369 | accuracy: 100.0%\n",
      "iteration: 784 | loss: 1.4268157354047197 | accuracy: 100.0%\n",
      "iteration: 785 | loss: 1.1094898638698152 | accuracy: 100.0%\n",
      "iteration: 786 | loss: 1.7753584975070047 | accuracy: 50.0%\n",
      "iteration: 787 | loss: 1.0195427831830208 | accuracy: 100.0%\n",
      "iteration: 788 | loss: 1.007476449420548 | accuracy: 100.0%\n",
      "iteration: 789 | loss: 1.4365044169566226 | accuracy: 100.0%\n",
      "iteration: 790 | loss: 1.006236821532467 | accuracy: 100.0%\n",
      "iteration: 791 | loss: 1.0046392835761586 | accuracy: 100.0%\n",
      "iteration: 792 | loss: 1.0558687837987237 | accuracy: 100.0%\n",
      "iteration: 793 | loss: 1.061514832184855 | accuracy: 100.0%\n",
      "iteration: 794 | loss: 1.9822027597554324 | accuracy: 50.0%\n",
      "iteration: 795 | loss: 1.0075604371447802 | accuracy: 100.0%\n",
      "iteration: 796 | loss: 1.0047882561522004 | accuracy: 100.0%\n",
      "iteration: 797 | loss: 1.0035317005477136 | accuracy: 100.0%\n",
      "iteration: 798 | loss: 1.0106704044599377 | accuracy: 100.0%\n",
      "iteration: 799 | loss: 1.008806862275052 | accuracy: 100.0%\n",
      "iteration: 800 | loss: 1.9437951412925367 | accuracy: 50.0%\n",
      "iteration: 801 | loss: 1.0662809949065073 | accuracy: 100.0%\n",
      "iteration: 802 | loss: 1.0167085915634462 | accuracy: 100.0%\n",
      "iteration: 803 | loss: 1.0092742173919351 | accuracy: 100.0%\n",
      "iteration: 804 | loss: 1.0043146683869268 | accuracy: 100.0%\n",
      "iteration: 805 | loss: 1.0059913507763565 | accuracy: 100.0%\n",
      "iteration: 806 | loss: 1.006405003177016 | accuracy: 100.0%\n",
      "iteration: 807 | loss: 1.2085422933829142 | accuracy: 100.0%\n",
      "iteration: 808 | loss: 1.0102770303483828 | accuracy: 100.0%\n",
      "iteration: 809 | loss: 1.002215015634478 | accuracy: 100.0%\n",
      "iteration: 810 | loss: 1.9372497174252106 | accuracy: 50.0%\n",
      "iteration: 811 | loss: 1.0119319610204447 | accuracy: 100.0%\n",
      "iteration: 812 | loss: 1.0042380524581889 | accuracy: 100.0%\n",
      "iteration: 813 | loss: 1.2765036606904803 | accuracy: 100.0%\n",
      "iteration: 814 | loss: 1.9954947621309056 | accuracy: 50.0%\n",
      "iteration: 815 | loss: 1.516905460218454 | accuracy: 100.0%\n",
      "iteration: 816 | loss: 1.029648943791257 | accuracy: 100.0%\n",
      "iteration: 817 | loss: 1.3942611281360737 | accuracy: 100.0%\n",
      "iteration: 818 | loss: 1.0425705693910223 | accuracy: 100.0%\n",
      "iteration: 819 | loss: 1.0490426104056263 | accuracy: 100.0%\n",
      "iteration: 820 | loss: 1.0523503350770627 | accuracy: 100.0%\n",
      "iteration: 821 | loss: 1.513368322570128 | accuracy: 100.0%\n",
      "iteration: 822 | loss: 1.6945987881100106 | accuracy: 50.0%\n",
      "iteration: 823 | loss: 1.02126104844716 | accuracy: 100.0%\n",
      "iteration: 824 | loss: 1.0050384593747497 | accuracy: 100.0%\n",
      "iteration: 825 | loss: 1.0748582919883904 | accuracy: 100.0%\n",
      "iteration: 826 | loss: 1.8697082369128162 | accuracy: 50.0%\n",
      "iteration: 827 | loss: 1.099305312206106 | accuracy: 100.0%\n",
      "iteration: 828 | loss: 1.0028543888790307 | accuracy: 100.0%\n",
      "iteration: 829 | loss: 1.1403549122847694 | accuracy: 100.0%\n",
      "iteration: 830 | loss: 1.0041168204968323 | accuracy: 100.0%\n",
      "iteration: 831 | loss: 1.3662735143308409 | accuracy: 100.0%\n",
      "iteration: 832 | loss: 1.0385036627635784 | accuracy: 100.0%\n",
      "iteration: 833 | loss: 1.0364783054059068 | accuracy: 100.0%\n",
      "iteration: 834 | loss: 1.6847146521338754 | accuracy: 50.0%\n",
      "iteration: 835 | loss: 1.0997274156476435 | accuracy: 100.0%\n",
      "iteration: 836 | loss: 1.1070939930163335 | accuracy: 100.0%\n",
      "iteration: 837 | loss: 1.1621227345871168 | accuracy: 100.0%\n",
      "iteration: 838 | loss: 1.0148407765773582 | accuracy: 100.0%\n",
      "iteration: 839 | loss: 1.166964696375295 | accuracy: 100.0%\n",
      "iteration: 840 | loss: 1.0188172646777456 | accuracy: 100.0%\n",
      "iteration: 841 | loss: 1.006711484156668 | accuracy: 100.0%\n",
      "iteration: 842 | loss: 1.0042017960876928 | accuracy: 100.0%\n",
      "iteration: 843 | loss: 1.0037746895242456 | accuracy: 100.0%\n",
      "iteration: 844 | loss: 1.0224368679325897 | accuracy: 100.0%\n",
      "iteration: 845 | loss: 1.00433750905749 | accuracy: 100.0%\n",
      "iteration: 846 | loss: 1.0037799179169504 | accuracy: 100.0%\n",
      "iteration: 847 | loss: 1.0563959939809142 | accuracy: 100.0%\n",
      "iteration: 848 | loss: 1.0505437483375275 | accuracy: 100.0%\n",
      "iteration: 849 | loss: 1.2188041551269562 | accuracy: 100.0%\n",
      "iteration: 850 | loss: 1.0095937253919862 | accuracy: 100.0%\n",
      "iteration: 851 | loss: 1.026527133155668 | accuracy: 100.0%\n",
      "iteration: 852 | loss: 1.0069138757042613 | accuracy: 100.0%\n",
      "iteration: 853 | loss: 1.0447120823543983 | accuracy: 100.0%\n",
      "iteration: 854 | loss: 1.016696246649806 | accuracy: 100.0%\n",
      "iteration: 855 | loss: 1.7597676798563169 | accuracy: 50.0%\n",
      "iteration: 856 | loss: 1.0123944280320007 | accuracy: 100.0%\n",
      "iteration: 857 | loss: 1.964556154936003 | accuracy: 50.0%\n",
      "iteration: 858 | loss: 1.6231681250874834 | accuracy: 50.0%\n",
      "iteration: 859 | loss: 1.9495142805853105 | accuracy: 50.0%\n",
      "iteration: 860 | loss: 1.2620300608761 | accuracy: 100.0%\n",
      "iteration: 861 | loss: 1.0784633706212134 | accuracy: 100.0%\n",
      "iteration: 862 | loss: 1.011645645236561 | accuracy: 100.0%\n",
      "iteration: 863 | loss: 1.1026760938515061 | accuracy: 100.0%\n",
      "iteration: 864 | loss: 2.6669938798898754 | accuracy: 0.0%\n",
      "iteration: 865 | loss: 1.5747705888504158 | accuracy: 50.0%\n",
      "iteration: 866 | loss: 1.6166040196749287 | accuracy: 50.0%\n",
      "iteration: 867 | loss: 1.3833177816461963 | accuracy: 100.0%\n",
      "iteration: 868 | loss: 1.0137139719457062 | accuracy: 100.0%\n",
      "iteration: 869 | loss: 1.8259958654801476 | accuracy: 50.0%\n",
      "iteration: 870 | loss: 1.0299367314705388 | accuracy: 100.0%\n",
      "iteration: 871 | loss: 1.023998208535543 | accuracy: 100.0%\n",
      "iteration: 872 | loss: 1.0097615590452742 | accuracy: 100.0%\n",
      "iteration: 873 | loss: 1.0152153739427479 | accuracy: 100.0%\n",
      "iteration: 874 | loss: 1.0593042632115945 | accuracy: 100.0%\n",
      "iteration: 875 | loss: 1.0498723608583367 | accuracy: 100.0%\n",
      "iteration: 876 | loss: 1.0136377758600954 | accuracy: 100.0%\n",
      "iteration: 877 | loss: 1.7260110525429373 | accuracy: 50.0%\n",
      "iteration: 878 | loss: 1.5037308168152352 | accuracy: 100.0%\n",
      "iteration: 879 | loss: 1.5275455601524577 | accuracy: 50.0%\n",
      "iteration: 880 | loss: 1.1502337890737286 | accuracy: 100.0%\n",
      "iteration: 881 | loss: 1.464715987983334 | accuracy: 100.0%\n",
      "iteration: 882 | loss: 1.6164950885943008 | accuracy: 100.0%\n",
      "iteration: 883 | loss: 1.0791282601383232 | accuracy: 100.0%\n",
      "iteration: 884 | loss: 1.092548302691433 | accuracy: 100.0%\n",
      "iteration: 885 | loss: 1.6143879439394473 | accuracy: 50.0%\n",
      "iteration: 886 | loss: 1.0423890693687075 | accuracy: 100.0%\n",
      "iteration: 887 | loss: 1.017082050791919 | accuracy: 100.0%\n",
      "iteration: 888 | loss: 1.2081405620300838 | accuracy: 100.0%\n",
      "iteration: 889 | loss: 1.0681341644591917 | accuracy: 100.0%\n",
      "iteration: 890 | loss: 1.687714078217447 | accuracy: 50.0%\n",
      "iteration: 891 | loss: 1.045773479366697 | accuracy: 100.0%\n",
      "iteration: 892 | loss: 1.4826681143357576 | accuracy: 100.0%\n",
      "iteration: 893 | loss: 1.0199564533034107 | accuracy: 100.0%\n",
      "iteration: 894 | loss: 1.0627512039915672 | accuracy: 100.0%\n",
      "iteration: 895 | loss: 1.049956377434687 | accuracy: 100.0%\n",
      "iteration: 896 | loss: 1.2197535295137814 | accuracy: 100.0%\n",
      "iteration: 897 | loss: 1.04151561381605 | accuracy: 100.0%\n",
      "iteration: 898 | loss: 1.1344343883842156 | accuracy: 100.0%\n",
      "iteration: 899 | loss: 1.0206297895935785 | accuracy: 100.0%\n",
      "iteration: 900 | loss: 1.6674354653500036 | accuracy: 50.0%\n",
      "iteration: 901 | loss: 1.0767308707927357 | accuracy: 100.0%\n",
      "iteration: 902 | loss: 1.2502587157003187 | accuracy: 100.0%\n",
      "iteration: 903 | loss: 1.1283543926682371 | accuracy: 100.0%\n",
      "iteration: 904 | loss: 1.1738105121853297 | accuracy: 100.0%\n",
      "iteration: 905 | loss: 1.0789114804486373 | accuracy: 100.0%\n",
      "iteration: 906 | loss: 1.2125624509433315 | accuracy: 100.0%\n",
      "iteration: 907 | loss: 1.0173576646440228 | accuracy: 100.0%\n",
      "iteration: 908 | loss: 1.0215076082604417 | accuracy: 100.0%\n",
      "iteration: 909 | loss: 1.434782017566563 | accuracy: 100.0%\n",
      "iteration: 910 | loss: 1.0177170971354927 | accuracy: 100.0%\n",
      "iteration: 911 | loss: 1.0348399557233323 | accuracy: 100.0%\n",
      "iteration: 912 | loss: 1.8837627959863843 | accuracy: 50.0%\n",
      "iteration: 913 | loss: 1.011192692409048 | accuracy: 100.0%\n",
      "iteration: 914 | loss: 1.0226885479451293 | accuracy: 100.0%\n",
      "iteration: 915 | loss: 1.0166668691886447 | accuracy: 100.0%\n",
      "iteration: 916 | loss: 1.0460931674360368 | accuracy: 100.0%\n",
      "iteration: 917 | loss: 1.0101417102368777 | accuracy: 100.0%\n",
      "iteration: 918 | loss: 1.1014704160400874 | accuracy: 100.0%\n",
      "iteration: 919 | loss: 1.0151905181260679 | accuracy: 100.0%\n",
      "iteration: 920 | loss: 1.04853143850203 | accuracy: 100.0%\n",
      "iteration: 921 | loss: 1.0141883910652112 | accuracy: 100.0%\n",
      "iteration: 922 | loss: 1.0552960539295873 | accuracy: 100.0%\n",
      "iteration: 923 | loss: 1.0079414587311195 | accuracy: 100.0%\n",
      "iteration: 924 | loss: 1.0165683157698713 | accuracy: 100.0%\n",
      "iteration: 925 | loss: 1.0356296829035916 | accuracy: 100.0%\n",
      "iteration: 926 | loss: 1.0185084127689157 | accuracy: 100.0%\n",
      "iteration: 927 | loss: 1.1345389853433925 | accuracy: 100.0%\n",
      "iteration: 928 | loss: 1.0104145790814623 | accuracy: 100.0%\n",
      "iteration: 929 | loss: 1.0425011981499681 | accuracy: 100.0%\n",
      "iteration: 930 | loss: 1.119886387796642 | accuracy: 100.0%\n",
      "iteration: 931 | loss: 1.0096325990937682 | accuracy: 100.0%\n",
      "iteration: 932 | loss: 1.0961192972320153 | accuracy: 100.0%\n",
      "iteration: 933 | loss: 1.1116432478193574 | accuracy: 100.0%\n",
      "iteration: 934 | loss: 1.0139862464742697 | accuracy: 100.0%\n",
      "iteration: 935 | loss: 1.03227212289493 | accuracy: 100.0%\n",
      "iteration: 936 | loss: 1.0186313787232966 | accuracy: 100.0%\n",
      "iteration: 937 | loss: 1.0330737544257174 | accuracy: 100.0%\n",
      "iteration: 938 | loss: 1.0178529566690069 | accuracy: 100.0%\n",
      "iteration: 939 | loss: 1.0137725962311686 | accuracy: 100.0%\n",
      "iteration: 940 | loss: 1.01411645129232 | accuracy: 100.0%\n",
      "iteration: 941 | loss: 1.5429934915156847 | accuracy: 50.0%\n",
      "iteration: 942 | loss: 1.5593902682569436 | accuracy: 50.0%\n",
      "iteration: 943 | loss: 1.0381518658408446 | accuracy: 100.0%\n",
      "iteration: 944 | loss: 1.147267147985922 | accuracy: 100.0%\n",
      "iteration: 945 | loss: 1.0369773008657466 | accuracy: 100.0%\n",
      "iteration: 946 | loss: 1.1346327415894604 | accuracy: 100.0%\n",
      "iteration: 947 | loss: 1.1983720368952828 | accuracy: 100.0%\n",
      "iteration: 948 | loss: 1.205964391080115 | accuracy: 100.0%\n",
      "iteration: 949 | loss: 1.0408736534701306 | accuracy: 100.0%\n",
      "iteration: 950 | loss: 1.0959685434995377 | accuracy: 100.0%\n",
      "iteration: 951 | loss: 1.0161958761124426 | accuracy: 100.0%\n",
      "iteration: 952 | loss: 1.0110684501265412 | accuracy: 100.0%\n",
      "iteration: 953 | loss: 1.1541383028532206 | accuracy: 100.0%\n",
      "iteration: 954 | loss: 1.0080616943239937 | accuracy: 100.0%\n",
      "iteration: 955 | loss: 1.6445391283492554 | accuracy: 50.0%\n",
      "iteration: 956 | loss: 1.015616062221323 | accuracy: 100.0%\n",
      "iteration: 957 | loss: 1.0781585474898219 | accuracy: 100.0%\n",
      "iteration: 958 | loss: 1.0107688857157437 | accuracy: 100.0%\n",
      "iteration: 959 | loss: 1.201529758922189 | accuracy: 100.0%\n",
      "iteration: 960 | loss: 1.0225384620019589 | accuracy: 100.0%\n",
      "iteration: 961 | loss: 1.0565853903523879 | accuracy: 100.0%\n",
      "iteration: 962 | loss: 1.0102838626935238 | accuracy: 100.0%\n",
      "iteration: 963 | loss: 1.0124234699625148 | accuracy: 100.0%\n",
      "iteration: 964 | loss: 1.0550836071166372 | accuracy: 100.0%\n",
      "iteration: 965 | loss: 1.0266395838541922 | accuracy: 100.0%\n",
      "iteration: 966 | loss: 1.0294881589569453 | accuracy: 100.0%\n",
      "iteration: 967 | loss: 1.0153485195400647 | accuracy: 100.0%\n",
      "iteration: 968 | loss: 1.0440705550871925 | accuracy: 100.0%\n",
      "iteration: 969 | loss: 1.017068679595677 | accuracy: 100.0%\n",
      "iteration: 970 | loss: 1.0091225384010432 | accuracy: 100.0%\n",
      "iteration: 971 | loss: 1.2059536841035614 | accuracy: 100.0%\n",
      "iteration: 972 | loss: 1.0235035749172803 | accuracy: 100.0%\n",
      "iteration: 973 | loss: 1.013442286192004 | accuracy: 100.0%\n",
      "iteration: 974 | loss: 1.0440186178722919 | accuracy: 100.0%\n",
      "iteration: 975 | loss: 1.0325028526685656 | accuracy: 100.0%\n",
      "iteration: 976 | loss: 1.008168602122466 | accuracy: 100.0%\n",
      "iteration: 977 | loss: 1.007691128941449 | accuracy: 100.0%\n",
      "iteration: 978 | loss: 1.0474952272240485 | accuracy: 100.0%\n",
      "iteration: 979 | loss: 1.077260139720685 | accuracy: 100.0%\n",
      "iteration: 980 | loss: 1.0109971811352778 | accuracy: 100.0%\n",
      "iteration: 981 | loss: 1.0072275724940196 | accuracy: 100.0%\n",
      "iteration: 982 | loss: 1.1040521819588576 | accuracy: 100.0%\n",
      "iteration: 983 | loss: 1.0089284761732304 | accuracy: 100.0%\n",
      "iteration: 984 | loss: 1.0222728162771828 | accuracy: 100.0%\n",
      "iteration: 985 | loss: 1.0074184748824402 | accuracy: 100.0%\n",
      "iteration: 986 | loss: 1.0238634654127146 | accuracy: 100.0%\n",
      "iteration: 987 | loss: 1.0080848277731194 | accuracy: 100.0%\n",
      "iteration: 988 | loss: 1.008727944905632 | accuracy: 100.0%\n",
      "iteration: 989 | loss: 1.7119807868952224 | accuracy: 50.0%\n",
      "iteration: 990 | loss: 1.034153400260173 | accuracy: 100.0%\n",
      "iteration: 991 | loss: 1.234890123108911 | accuracy: 100.0%\n",
      "iteration: 992 | loss: 1.009780376044778 | accuracy: 100.0%\n",
      "iteration: 993 | loss: 1.006115858830511 | accuracy: 100.0%\n",
      "iteration: 994 | loss: 1.0254055446335797 | accuracy: 100.0%\n",
      "iteration: 995 | loss: 1.0083795020372412 | accuracy: 100.0%\n",
      "iteration: 996 | loss: 1.009177965499121 | accuracy: 100.0%\n",
      "iteration: 997 | loss: 1.0137687712523065 | accuracy: 100.0%\n",
      "iteration: 998 | loss: 1.0052811737061018 | accuracy: 100.0%\n",
      "iteration: 999 | loss: 1.012034543094668 | accuracy: 100.0%\n",
      "iteration: 1000 | loss: 1.007248296108521 | accuracy: 100.0%\n",
      "iteration: 1001 | loss: 1.0167285550270955 | accuracy: 100.0%\n",
      "iteration: 1002 | loss: 1.0094946283951782 | accuracy: 100.0%\n",
      "iteration: 1003 | loss: 1.889710598711298 | accuracy: 50.0%\n",
      "iteration: 1004 | loss: 1.016237146658428 | accuracy: 100.0%\n",
      "iteration: 1005 | loss: 1.0092613674863717 | accuracy: 100.0%\n",
      "iteration: 1006 | loss: 1.0054350700559676 | accuracy: 100.0%\n",
      "iteration: 1007 | loss: 1.0062354983805426 | accuracy: 100.0%\n",
      "iteration: 1008 | loss: 1.0173092046355094 | accuracy: 100.0%\n",
      "iteration: 1009 | loss: 1.0800689518766118 | accuracy: 100.0%\n",
      "iteration: 1010 | loss: 1.0107024869908057 | accuracy: 100.0%\n",
      "iteration: 1011 | loss: 1.0068969585265342 | accuracy: 100.0%\n",
      "iteration: 1012 | loss: 1.004919310891791 | accuracy: 100.0%\n",
      "iteration: 1013 | loss: 1.0143093614338632 | accuracy: 100.0%\n",
      "iteration: 1014 | loss: 1.0224749290798079 | accuracy: 100.0%\n",
      "iteration: 1015 | loss: 1.0045128500757472 | accuracy: 100.0%\n",
      "iteration: 1016 | loss: 1.0378450575699156 | accuracy: 100.0%\n",
      "iteration: 1017 | loss: 1.0074822402815833 | accuracy: 100.0%\n",
      "iteration: 1018 | loss: 1.004126800907029 | accuracy: 100.0%\n",
      "iteration: 1019 | loss: 1.0568469605249298 | accuracy: 100.0%\n",
      "iteration: 1020 | loss: 1.0433454852799455 | accuracy: 100.0%\n",
      "iteration: 1021 | loss: 1.7086630361678028 | accuracy: 50.0%\n",
      "iteration: 1022 | loss: 1.007725376177791 | accuracy: 100.0%\n",
      "iteration: 1023 | loss: 1.0142615564041058 | accuracy: 100.0%\n",
      "iteration: 1024 | loss: 1.0192366153447179 | accuracy: 100.0%\n",
      "iteration: 1025 | loss: 1.0408349446117295 | accuracy: 100.0%\n",
      "iteration: 1026 | loss: 1.0467745877188532 | accuracy: 100.0%\n",
      "iteration: 1027 | loss: 1.0117567761520712 | accuracy: 100.0%\n",
      "iteration: 1028 | loss: 1.0082136038312186 | accuracy: 100.0%\n",
      "iteration: 1029 | loss: 1.01059524118696 | accuracy: 100.0%\n",
      "iteration: 1030 | loss: 1.0673477358929273 | accuracy: 100.0%\n",
      "iteration: 1031 | loss: 1.0173425416110184 | accuracy: 100.0%\n",
      "iteration: 1032 | loss: 1.0188434512527926 | accuracy: 100.0%\n",
      "iteration: 1033 | loss: 1.035209450970766 | accuracy: 100.0%\n",
      "iteration: 1034 | loss: 1.0302965084099385 | accuracy: 100.0%\n",
      "iteration: 1035 | loss: 1.0349543173337892 | accuracy: 100.0%\n",
      "iteration: 1036 | loss: 1.0161583508245362 | accuracy: 100.0%\n",
      "iteration: 1037 | loss: 1.0146200726895078 | accuracy: 100.0%\n",
      "iteration: 1038 | loss: 1.119118057683549 | accuracy: 100.0%\n",
      "iteration: 1039 | loss: 1.0120974739839927 | accuracy: 100.0%\n",
      "iteration: 1040 | loss: 1.017694634686768 | accuracy: 100.0%\n",
      "iteration: 1041 | loss: 1.015421910718911 | accuracy: 100.0%\n",
      "iteration: 1042 | loss: 1.0650990053948044 | accuracy: 100.0%\n",
      "iteration: 1043 | loss: 1.0072380687924256 | accuracy: 100.0%\n",
      "iteration: 1044 | loss: 1.0169256162165907 | accuracy: 100.0%\n",
      "iteration: 1045 | loss: 1.0032460920055308 | accuracy: 100.0%\n",
      "iteration: 1046 | loss: 1.006453433066102 | accuracy: 100.0%\n",
      "iteration: 1047 | loss: 1.0284702671065074 | accuracy: 100.0%\n",
      "iteration: 1048 | loss: 1.0180257529189163 | accuracy: 100.0%\n",
      "iteration: 1049 | loss: 1.0050114261435872 | accuracy: 100.0%\n",
      "iteration: 1050 | loss: 1.032599817843188 | accuracy: 100.0%\n",
      "iteration: 1051 | loss: 1.0328909078438264 | accuracy: 100.0%\n",
      "iteration: 1052 | loss: 1.0053869170459548 | accuracy: 100.0%\n",
      "iteration: 1053 | loss: 1.004976633999927 | accuracy: 100.0%\n",
      "iteration: 1054 | loss: 1.012561377533171 | accuracy: 100.0%\n",
      "iteration: 1055 | loss: 1.0056394809276095 | accuracy: 100.0%\n",
      "iteration: 1056 | loss: 1.0087306779353329 | accuracy: 100.0%\n",
      "iteration: 1057 | loss: 1.7851576565232754 | accuracy: 50.0%\n",
      "iteration: 1058 | loss: 1.011799689567356 | accuracy: 100.0%\n",
      "iteration: 1059 | loss: 1.005742774840535 | accuracy: 100.0%\n",
      "iteration: 1060 | loss: 1.0179359564297303 | accuracy: 100.0%\n",
      "iteration: 1061 | loss: 1.3793091510171016 | accuracy: 100.0%\n",
      "iteration: 1062 | loss: 1.4113205854158462 | accuracy: 100.0%\n",
      "iteration: 1063 | loss: 1.0536323524966174 | accuracy: 100.0%\n",
      "iteration: 1064 | loss: 1.0070954755894868 | accuracy: 100.0%\n",
      "iteration: 1065 | loss: 1.0059095979984602 | accuracy: 100.0%\n",
      "iteration: 1066 | loss: 1.0030098961964349 | accuracy: 100.0%\n",
      "iteration: 1067 | loss: 1.4039559236855657 | accuracy: 100.0%\n",
      "iteration: 1068 | loss: 1.0141206054052334 | accuracy: 100.0%\n",
      "iteration: 1069 | loss: 1.1766633480281605 | accuracy: 100.0%\n",
      "iteration: 1070 | loss: 1.0124482961776602 | accuracy: 100.0%\n",
      "iteration: 1071 | loss: 1.1814426056276723 | accuracy: 100.0%\n",
      "iteration: 1072 | loss: 1.0111597754176809 | accuracy: 100.0%\n",
      "iteration: 1073 | loss: 1.0052194713079585 | accuracy: 100.0%\n",
      "iteration: 1074 | loss: 1.0108235137274386 | accuracy: 100.0%\n",
      "iteration: 1075 | loss: 1.0129410218617252 | accuracy: 100.0%\n",
      "iteration: 1076 | loss: 1.776592063658794 | accuracy: 50.0%\n",
      "iteration: 1077 | loss: 1.0116624080809111 | accuracy: 100.0%\n",
      "iteration: 1078 | loss: 1.0060409435999165 | accuracy: 100.0%\n",
      "iteration: 1079 | loss: 1.05498890567116 | accuracy: 100.0%\n",
      "iteration: 1080 | loss: 1.061103544754385 | accuracy: 100.0%\n",
      "iteration: 1081 | loss: 1.0212704329011244 | accuracy: 100.0%\n",
      "iteration: 1082 | loss: 1.0181795871467305 | accuracy: 100.0%\n",
      "iteration: 1083 | loss: 1.3722956952862995 | accuracy: 100.0%\n",
      "iteration: 1084 | loss: 1.1433101772710013 | accuracy: 100.0%\n",
      "iteration: 1085 | loss: 1.0086654325308544 | accuracy: 100.0%\n",
      "iteration: 1086 | loss: 1.009226020136535 | accuracy: 100.0%\n",
      "iteration: 1087 | loss: 1.6837084442659878 | accuracy: 50.0%\n",
      "iteration: 1088 | loss: 1.1646442188599426 | accuracy: 100.0%\n",
      "iteration: 1089 | loss: 1.0447955717995943 | accuracy: 100.0%\n",
      "iteration: 1090 | loss: 1.0215142980724254 | accuracy: 100.0%\n",
      "iteration: 1091 | loss: 1.0054569798855 | accuracy: 100.0%\n",
      "iteration: 1092 | loss: 1.018523924270875 | accuracy: 100.0%\n",
      "iteration: 1093 | loss: 1.007453692703376 | accuracy: 100.0%\n",
      "iteration: 1094 | loss: 1.0394394083481195 | accuracy: 100.0%\n",
      "iteration: 1095 | loss: 1.0098965511523281 | accuracy: 100.0%\n",
      "iteration: 1096 | loss: 1.0146652335966933 | accuracy: 100.0%\n",
      "iteration: 1097 | loss: 1.005713510714206 | accuracy: 100.0%\n",
      "iteration: 1098 | loss: 1.0454579632145897 | accuracy: 100.0%\n",
      "iteration: 1099 | loss: 1.0218990207121152 | accuracy: 100.0%\n",
      "iteration: 1100 | loss: 1.0178174155586537 | accuracy: 100.0%\n",
      "iteration: 1101 | loss: 1.0197442699428885 | accuracy: 100.0%\n",
      "iteration: 1102 | loss: 1.1035969182873582 | accuracy: 100.0%\n",
      "iteration: 1103 | loss: 1.0183427284566504 | accuracy: 100.0%\n",
      "iteration: 1104 | loss: 1.0178945671337385 | accuracy: 100.0%\n",
      "iteration: 1105 | loss: 1.0322952254453812 | accuracy: 100.0%\n",
      "iteration: 1106 | loss: 1.0210781276720453 | accuracy: 100.0%\n",
      "iteration: 1107 | loss: 1.0079402577399905 | accuracy: 100.0%\n",
      "iteration: 1108 | loss: 1.1138643948642009 | accuracy: 100.0%\n",
      "iteration: 1109 | loss: 1.046299058535111 | accuracy: 100.0%\n",
      "iteration: 1110 | loss: 1.0216820694458415 | accuracy: 100.0%\n",
      "iteration: 1111 | loss: 1.0113789544601093 | accuracy: 100.0%\n",
      "iteration: 1112 | loss: 1.0054079299515915 | accuracy: 100.0%\n",
      "iteration: 1113 | loss: 1.00749485031017 | accuracy: 100.0%\n",
      "iteration: 1114 | loss: 1.0413869922670542 | accuracy: 100.0%\n",
      "iteration: 1115 | loss: 1.005912224645157 | accuracy: 100.0%\n",
      "iteration: 1116 | loss: 1.560572445476836 | accuracy: 50.0%\n",
      "iteration: 1117 | loss: 1.0341836319430797 | accuracy: 100.0%\n",
      "iteration: 1118 | loss: 1.0163220034679181 | accuracy: 100.0%\n",
      "iteration: 1119 | loss: 1.0942468698509473 | accuracy: 100.0%\n",
      "iteration: 1120 | loss: 1.020525250416254 | accuracy: 100.0%\n",
      "iteration: 1121 | loss: 1.0084902460098732 | accuracy: 100.0%\n",
      "iteration: 1122 | loss: 1.0116435659896954 | accuracy: 100.0%\n",
      "iteration: 1123 | loss: 1.01698279137721 | accuracy: 100.0%\n",
      "iteration: 1124 | loss: 1.025458961844085 | accuracy: 100.0%\n",
      "iteration: 1125 | loss: 1.0228961787873132 | accuracy: 100.0%\n",
      "iteration: 1126 | loss: 1.0226028070051212 | accuracy: 100.0%\n",
      "iteration: 1127 | loss: 1.0271044759820736 | accuracy: 100.0%\n",
      "iteration: 1128 | loss: 1.3033702872417827 | accuracy: 100.0%\n",
      "iteration: 1129 | loss: 1.0944927154199864 | accuracy: 100.0%\n",
      "iteration: 1130 | loss: 1.0050969059485142 | accuracy: 100.0%\n",
      "iteration: 1131 | loss: 1.010174290392955 | accuracy: 100.0%\n",
      "iteration: 1132 | loss: 1.0273716034599405 | accuracy: 100.0%\n",
      "iteration: 1133 | loss: 1.0230629041890311 | accuracy: 100.0%\n",
      "iteration: 1134 | loss: 1.1042283201682366 | accuracy: 100.0%\n",
      "iteration: 1135 | loss: 1.0080521060986032 | accuracy: 100.0%\n",
      "iteration: 1136 | loss: 1.007791782368811 | accuracy: 100.0%\n",
      "iteration: 1137 | loss: 1.020748345254239 | accuracy: 100.0%\n",
      "iteration: 1138 | loss: 1.7343533750544347 | accuracy: 50.0%\n",
      "iteration: 1139 | loss: 1.0162574103605606 | accuracy: 100.0%\n",
      "iteration: 1140 | loss: 1.0216911499408912 | accuracy: 100.0%\n",
      "iteration: 1141 | loss: 1.0168808701441348 | accuracy: 100.0%\n",
      "iteration: 1142 | loss: 1.2703295217361494 | accuracy: 100.0%\n",
      "iteration: 1143 | loss: 1.0160431600670108 | accuracy: 100.0%\n",
      "iteration: 1144 | loss: 1.0544744598375408 | accuracy: 100.0%\n",
      "iteration: 1145 | loss: 1.0066603833465837 | accuracy: 100.0%\n",
      "iteration: 1146 | loss: 1.005668664889874 | accuracy: 100.0%\n",
      "iteration: 1147 | loss: 1.0240099294855445 | accuracy: 100.0%\n",
      "iteration: 1148 | loss: 1.0063819327288543 | accuracy: 100.0%\n",
      "iteration: 1149 | loss: 1.037966670487874 | accuracy: 100.0%\n",
      "iteration: 1150 | loss: 1.4716253397331345 | accuracy: 100.0%\n",
      "iteration: 1151 | loss: 1.0033285523177318 | accuracy: 100.0%\n",
      "iteration: 1152 | loss: 1.4118661051722987 | accuracy: 100.0%\n",
      "iteration: 1153 | loss: 1.0086305401022366 | accuracy: 100.0%\n",
      "iteration: 1154 | loss: 1.036766281439664 | accuracy: 100.0%\n",
      "iteration: 1155 | loss: 1.2111396042341596 | accuracy: 100.0%\n",
      "iteration: 1156 | loss: 1.011310302379355 | accuracy: 100.0%\n",
      "iteration: 1157 | loss: 1.3388263939997571 | accuracy: 100.0%\n",
      "iteration: 1158 | loss: 1.0214916346502494 | accuracy: 100.0%\n",
      "iteration: 1159 | loss: 1.0363963919968868 | accuracy: 100.0%\n",
      "iteration: 1160 | loss: 1.0072317131858297 | accuracy: 100.0%\n",
      "iteration: 1161 | loss: 1.105243312885071 | accuracy: 100.0%\n",
      "iteration: 1162 | loss: 1.0114588343425788 | accuracy: 100.0%\n",
      "iteration: 1163 | loss: 1.6036090576590776 | accuracy: 50.0%\n",
      "iteration: 1164 | loss: 1.1457221212986137 | accuracy: 100.0%\n",
      "iteration: 1165 | loss: 1.119075028597118 | accuracy: 100.0%\n",
      "iteration: 1166 | loss: 1.0133373318649626 | accuracy: 100.0%\n",
      "iteration: 1167 | loss: 1.4728206957772267 | accuracy: 100.0%\n",
      "iteration: 1168 | loss: 1.0082514163364025 | accuracy: 100.0%\n",
      "iteration: 1169 | loss: 1.0314934325665717 | accuracy: 100.0%\n",
      "iteration: 1170 | loss: 1.0339814215716754 | accuracy: 100.0%\n",
      "iteration: 1171 | loss: 1.03929487996685 | accuracy: 100.0%\n",
      "iteration: 1172 | loss: 1.0763413178023926 | accuracy: 100.0%\n",
      "iteration: 1173 | loss: 1.03847769568975 | accuracy: 100.0%\n",
      "iteration: 1174 | loss: 1.2084822852678676 | accuracy: 100.0%\n",
      "iteration: 1175 | loss: 1.0606775878906491 | accuracy: 100.0%\n",
      "iteration: 1176 | loss: 1.0116132751910347 | accuracy: 100.0%\n",
      "iteration: 1177 | loss: 1.0201521455014915 | accuracy: 100.0%\n",
      "iteration: 1178 | loss: 1.0504624624615295 | accuracy: 100.0%\n",
      "iteration: 1179 | loss: 1.0101515060577393 | accuracy: 100.0%\n",
      "iteration: 1180 | loss: 1.814361453606374 | accuracy: 50.0%\n",
      "iteration: 1181 | loss: 1.005159010718799 | accuracy: 100.0%\n",
      "iteration: 1182 | loss: 1.0257190968480188 | accuracy: 100.0%\n",
      "iteration: 1183 | loss: 1.2170784183075478 | accuracy: 100.0%\n",
      "iteration: 1184 | loss: 1.0131363329942702 | accuracy: 100.0%\n",
      "iteration: 1185 | loss: 1.0112438393168472 | accuracy: 100.0%\n",
      "iteration: 1186 | loss: 1.0663584059944684 | accuracy: 100.0%\n",
      "iteration: 1187 | loss: 1.0425410906769754 | accuracy: 100.0%\n",
      "iteration: 1188 | loss: 1.152144331957466 | accuracy: 100.0%\n",
      "iteration: 1189 | loss: 1.0121919077013366 | accuracy: 100.0%\n",
      "iteration: 1190 | loss: 1.0157866012449597 | accuracy: 100.0%\n",
      "iteration: 1191 | loss: 1.4816659717776444 | accuracy: 100.0%\n",
      "iteration: 1192 | loss: 1.0582391882110553 | accuracy: 100.0%\n",
      "iteration: 1193 | loss: 1.0206200358972908 | accuracy: 100.0%\n",
      "iteration: 1194 | loss: 1.0203933343163873 | accuracy: 100.0%\n",
      "iteration: 1195 | loss: 1.0365049759005547 | accuracy: 100.0%\n",
      "iteration: 1196 | loss: 1.0082993888015699 | accuracy: 100.0%\n",
      "iteration: 1197 | loss: 1.010605367211577 | accuracy: 100.0%\n",
      "iteration: 1198 | loss: 1.0157242540015667 | accuracy: 100.0%\n",
      "iteration: 1199 | loss: 1.0567842602438682 | accuracy: 100.0%\n",
      "iteration: 1200 | loss: 1.0130100468787158 | accuracy: 100.0%\n",
      "iteration: 1201 | loss: 1.0117047532197265 | accuracy: 100.0%\n",
      "iteration: 1202 | loss: 1.012343929770344 | accuracy: 100.0%\n",
      "iteration: 1203 | loss: 1.0223488417313404 | accuracy: 100.0%\n",
      "iteration: 1204 | loss: 1.0065222756399328 | accuracy: 100.0%\n",
      "iteration: 1205 | loss: 1.010167737872564 | accuracy: 100.0%\n",
      "iteration: 1206 | loss: 1.003544378685934 | accuracy: 100.0%\n",
      "iteration: 1207 | loss: 1.3703550057586606 | accuracy: 100.0%\n",
      "iteration: 1208 | loss: 1.3455277176699418 | accuracy: 100.0%\n",
      "iteration: 1209 | loss: 1.1456261733640476 | accuracy: 100.0%\n",
      "iteration: 1210 | loss: 1.0026224924067495 | accuracy: 100.0%\n",
      "iteration: 1211 | loss: 1.0188408717609143 | accuracy: 100.0%\n",
      "iteration: 1212 | loss: 1.0031412177854495 | accuracy: 100.0%\n",
      "iteration: 1213 | loss: 1.028813206409747 | accuracy: 100.0%\n",
      "iteration: 1214 | loss: 1.0229612234954923 | accuracy: 100.0%\n",
      "iteration: 1215 | loss: 1.0079847967108884 | accuracy: 100.0%\n",
      "iteration: 1216 | loss: 1.009406191771203 | accuracy: 100.0%\n",
      "iteration: 1217 | loss: 1.0088811212369053 | accuracy: 100.0%\n",
      "iteration: 1218 | loss: 1.0035280296715785 | accuracy: 100.0%\n",
      "iteration: 1219 | loss: 1.0047512680518895 | accuracy: 100.0%\n",
      "iteration: 1220 | loss: 1.0037243295964042 | accuracy: 100.0%\n",
      "iteration: 1221 | loss: 1.0042164217595542 | accuracy: 100.0%\n",
      "iteration: 1222 | loss: 1.0034036985322172 | accuracy: 100.0%\n",
      "iteration: 1223 | loss: 1.0185268921301256 | accuracy: 100.0%\n",
      "iteration: 1224 | loss: 1.0089887867098486 | accuracy: 100.0%\n",
      "iteration: 1225 | loss: 1.0096998498575656 | accuracy: 100.0%\n",
      "iteration: 1226 | loss: 1.017304077891842 | accuracy: 100.0%\n",
      "iteration: 1227 | loss: 1.0051964589647324 | accuracy: 100.0%\n",
      "iteration: 1228 | loss: 1.0251203623210772 | accuracy: 100.0%\n",
      "iteration: 1229 | loss: 1.008724069310573 | accuracy: 100.0%\n",
      "iteration: 1230 | loss: 1.3960915643229048 | accuracy: 100.0%\n",
      "iteration: 1231 | loss: 1.0040740713778906 | accuracy: 100.0%\n",
      "iteration: 1232 | loss: 1.013345850480704 | accuracy: 100.0%\n",
      "iteration: 1233 | loss: 1.042486260727847 | accuracy: 100.0%\n",
      "iteration: 1234 | loss: 1.0360676503418533 | accuracy: 100.0%\n",
      "iteration: 1235 | loss: 1.0120941510036616 | accuracy: 100.0%\n",
      "iteration: 1236 | loss: 1.0133986369471906 | accuracy: 100.0%\n",
      "iteration: 1237 | loss: 1.6816990135119745 | accuracy: 50.0%\n",
      "iteration: 1238 | loss: 1.0357926052147746 | accuracy: 100.0%\n",
      "iteration: 1239 | loss: 1.0578819857386814 | accuracy: 100.0%\n",
      "iteration: 1240 | loss: 1.021209485334618 | accuracy: 100.0%\n",
      "iteration: 1241 | loss: 1.015331653145906 | accuracy: 100.0%\n",
      "iteration: 1242 | loss: 1.0402032621666168 | accuracy: 100.0%\n",
      "iteration: 1243 | loss: 1.0072542628893604 | accuracy: 100.0%\n",
      "iteration: 1244 | loss: 1.0810598455857543 | accuracy: 100.0%\n",
      "iteration: 1245 | loss: 1.0227880778733722 | accuracy: 100.0%\n",
      "iteration: 1246 | loss: 1.0336700997664074 | accuracy: 100.0%\n",
      "iteration: 1247 | loss: 1.0084446040081032 | accuracy: 100.0%\n",
      "iteration: 1248 | loss: 1.0072274213249353 | accuracy: 100.0%\n",
      "iteration: 1249 | loss: 1.0044349962096468 | accuracy: 100.0%\n",
      "iteration: 1250 | loss: 1.0301147033982527 | accuracy: 100.0%\n",
      "iteration: 1251 | loss: 1.033515599477313 | accuracy: 100.0%\n",
      "iteration: 1252 | loss: 1.0842426495168052 | accuracy: 100.0%\n",
      "iteration: 1253 | loss: 1.005414209727031 | accuracy: 100.0%\n",
      "iteration: 1254 | loss: 1.0236585605524267 | accuracy: 100.0%\n",
      "iteration: 1255 | loss: 1.0154766380926545 | accuracy: 100.0%\n",
      "iteration: 1256 | loss: 1.0059384975060222 | accuracy: 100.0%\n",
      "iteration: 1257 | loss: 1.0346555156405683 | accuracy: 100.0%\n",
      "iteration: 1258 | loss: 1.0064726691236758 | accuracy: 100.0%\n",
      "iteration: 1259 | loss: 1.020194456969936 | accuracy: 100.0%\n",
      "iteration: 1260 | loss: 1.0213749281356514 | accuracy: 100.0%\n",
      "iteration: 1261 | loss: 1.0160302017043765 | accuracy: 100.0%\n",
      "iteration: 1262 | loss: 1.0100971512889625 | accuracy: 100.0%\n",
      "iteration: 1263 | loss: 1.4240067473951836 | accuracy: 100.0%\n",
      "iteration: 1264 | loss: 1.0760016091572546 | accuracy: 100.0%\n",
      "iteration: 1265 | loss: 1.0102209404606262 | accuracy: 100.0%\n",
      "iteration: 1266 | loss: 1.0033089470935612 | accuracy: 100.0%\n",
      "iteration: 1267 | loss: 1.1869088802807164 | accuracy: 100.0%\n",
      "iteration: 1268 | loss: 1.0186976062328414 | accuracy: 100.0%\n",
      "iteration: 1269 | loss: 1.0063035631317852 | accuracy: 100.0%\n",
      "iteration: 1270 | loss: 1.0029043698648061 | accuracy: 100.0%\n",
      "iteration: 1271 | loss: 1.0024731412082846 | accuracy: 100.0%\n",
      "iteration: 1272 | loss: 1.0050760168614508 | accuracy: 100.0%\n",
      "iteration: 1273 | loss: 1.015746208839171 | accuracy: 100.0%\n",
      "iteration: 1274 | loss: 1.0061915329234248 | accuracy: 100.0%\n",
      "iteration: 1275 | loss: 1.0192257478209463 | accuracy: 100.0%\n",
      "iteration: 1276 | loss: 1.0338199391204193 | accuracy: 100.0%\n",
      "iteration: 1277 | loss: 1.014903484409557 | accuracy: 100.0%\n",
      "iteration: 1278 | loss: 1.3794578574333303 | accuracy: 100.0%\n",
      "iteration: 1279 | loss: 1.0073258182038025 | accuracy: 100.0%\n",
      "iteration: 1280 | loss: 1.0027363146284156 | accuracy: 100.0%\n",
      "iteration: 1281 | loss: 1.0055656765007792 | accuracy: 100.0%\n",
      "iteration: 1282 | loss: 1.0396462106875777 | accuracy: 100.0%\n",
      "iteration: 1283 | loss: 1.1232032805959375 | accuracy: 100.0%\n",
      "iteration: 1284 | loss: 1.02803126064542 | accuracy: 100.0%\n",
      "iteration: 1285 | loss: 1.0059200211032933 | accuracy: 100.0%\n",
      "iteration: 1286 | loss: 1.0022336307092634 | accuracy: 100.0%\n",
      "iteration: 1287 | loss: 1.0187077635367943 | accuracy: 100.0%\n",
      "iteration: 1288 | loss: 1.0046506923772447 | accuracy: 100.0%\n",
      "iteration: 1289 | loss: 1.0137058324293209 | accuracy: 100.0%\n",
      "iteration: 1290 | loss: 1.0281822247867132 | accuracy: 100.0%\n",
      "iteration: 1291 | loss: 1.0400513126565394 | accuracy: 100.0%\n",
      "iteration: 1292 | loss: 1.0091349256968594 | accuracy: 100.0%\n",
      "iteration: 1293 | loss: 1.0423692562693678 | accuracy: 100.0%\n",
      "iteration: 1294 | loss: 1.0043759175417157 | accuracy: 100.0%\n",
      "iteration: 1295 | loss: 1.0030178433925456 | accuracy: 100.0%\n",
      "iteration: 1296 | loss: 1.2065589447258351 | accuracy: 100.0%\n",
      "iteration: 1297 | loss: 1.096650254565761 | accuracy: 100.0%\n",
      "iteration: 1298 | loss: 1.0421740380941111 | accuracy: 100.0%\n",
      "iteration: 1299 | loss: 1.4024729185061213 | accuracy: 100.0%\n",
      "iteration: 1300 | loss: 1.005035223919133 | accuracy: 100.0%\n",
      "iteration: 1301 | loss: 1.0136952816704725 | accuracy: 100.0%\n",
      "iteration: 1302 | loss: 1.0200560296015415 | accuracy: 100.0%\n",
      "iteration: 1303 | loss: 1.0170975108048024 | accuracy: 100.0%\n",
      "iteration: 1304 | loss: 1.018154548291973 | accuracy: 100.0%\n",
      "iteration: 1305 | loss: 1.0142422870694274 | accuracy: 100.0%\n",
      "iteration: 1306 | loss: 1.2712629742248747 | accuracy: 100.0%\n",
      "iteration: 1307 | loss: 1.030162010854108 | accuracy: 100.0%\n",
      "iteration: 1308 | loss: 1.0178856441887845 | accuracy: 100.0%\n",
      "iteration: 1309 | loss: 1.0291068704911903 | accuracy: 100.0%\n",
      "iteration: 1310 | loss: 1.0716977926850295 | accuracy: 100.0%\n",
      "iteration: 1311 | loss: 1.264032227677151 | accuracy: 100.0%\n",
      "iteration: 1312 | loss: 1.0082796307589226 | accuracy: 100.0%\n",
      "iteration: 1313 | loss: 1.0540543674153906 | accuracy: 100.0%\n",
      "iteration: 1314 | loss: 1.3071176218511227 | accuracy: 100.0%\n",
      "iteration: 1315 | loss: 1.0489870074382903 | accuracy: 100.0%\n",
      "iteration: 1316 | loss: 1.0137536403128062 | accuracy: 100.0%\n",
      "iteration: 1317 | loss: 1.2184006695555658 | accuracy: 100.0%\n",
      "iteration: 1318 | loss: 1.0679322430692497 | accuracy: 100.0%\n",
      "iteration: 1319 | loss: 1.003193293491582 | accuracy: 100.0%\n",
      "iteration: 1320 | loss: 1.0171765813492528 | accuracy: 100.0%\n",
      "iteration: 1321 | loss: 1.0288179920786555 | accuracy: 100.0%\n",
      "iteration: 1322 | loss: 1.0216406000614449 | accuracy: 100.0%\n",
      "iteration: 1323 | loss: 1.0028663936575037 | accuracy: 100.0%\n",
      "iteration: 1324 | loss: 1.0095606661944558 | accuracy: 100.0%\n",
      "iteration: 1325 | loss: 1.0082663844437891 | accuracy: 100.0%\n",
      "iteration: 1326 | loss: 1.0259519266270527 | accuracy: 100.0%\n",
      "iteration: 1327 | loss: 1.0184284613958383 | accuracy: 100.0%\n",
      "iteration: 1328 | loss: 1.0048797854337874 | accuracy: 100.0%\n",
      "iteration: 1329 | loss: 1.0083360039677445 | accuracy: 100.0%\n",
      "iteration: 1330 | loss: 1.0378758341212937 | accuracy: 100.0%\n",
      "iteration: 1331 | loss: 1.167565949085744 | accuracy: 100.0%\n",
      "iteration: 1332 | loss: 1.0037825900206112 | accuracy: 100.0%\n",
      "iteration: 1333 | loss: 1.0154464465430686 | accuracy: 100.0%\n",
      "iteration: 1334 | loss: 1.008336825955976 | accuracy: 100.0%\n",
      "iteration: 1335 | loss: 1.0164358717354856 | accuracy: 100.0%\n",
      "iteration: 1336 | loss: 1.0034930567000182 | accuracy: 100.0%\n",
      "iteration: 1337 | loss: 1.0097052396048547 | accuracy: 100.0%\n",
      "iteration: 1338 | loss: 1.3373220286639491 | accuracy: 100.0%\n",
      "iteration: 1339 | loss: 1.1965902350320416 | accuracy: 100.0%\n",
      "iteration: 1340 | loss: 1.005571733467715 | accuracy: 100.0%\n",
      "iteration: 1341 | loss: 1.0097158205110286 | accuracy: 100.0%\n",
      "iteration: 1342 | loss: 1.0021704163657532 | accuracy: 100.0%\n",
      "iteration: 1343 | loss: 1.0047070235839857 | accuracy: 100.0%\n",
      "iteration: 1344 | loss: 1.006535887817777 | accuracy: 100.0%\n",
      "iteration: 1345 | loss: 1.0043045004961286 | accuracy: 100.0%\n",
      "iteration: 1346 | loss: 1.0041234070217717 | accuracy: 100.0%\n",
      "iteration: 1347 | loss: 1.4740450204832747 | accuracy: 100.0%\n",
      "iteration: 1348 | loss: 1.017998472593226 | accuracy: 100.0%\n",
      "iteration: 1349 | loss: 1.0089582773869883 | accuracy: 100.0%\n",
      "iteration: 1350 | loss: 1.0087913330844307 | accuracy: 100.0%\n",
      "iteration: 1351 | loss: 1.0358242249577514 | accuracy: 100.0%\n",
      "iteration: 1352 | loss: 1.097871378500864 | accuracy: 100.0%\n",
      "iteration: 1353 | loss: 1.0055929460720603 | accuracy: 100.0%\n",
      "iteration: 1354 | loss: 1.005409712247462 | accuracy: 100.0%\n",
      "iteration: 1355 | loss: 1.0874269501699168 | accuracy: 100.0%\n",
      "iteration: 1356 | loss: 1.0443761268815408 | accuracy: 100.0%\n",
      "iteration: 1357 | loss: 1.014375106105112 | accuracy: 100.0%\n",
      "iteration: 1358 | loss: 1.0496614444292687 | accuracy: 100.0%\n",
      "iteration: 1359 | loss: 1.094002435030765 | accuracy: 100.0%\n",
      "iteration: 1360 | loss: 1.0493640421057269 | accuracy: 100.0%\n",
      "iteration: 1361 | loss: 1.007103693326608 | accuracy: 100.0%\n",
      "iteration: 1362 | loss: 1.0126627594484259 | accuracy: 100.0%\n",
      "iteration: 1363 | loss: 1.0023696770975445 | accuracy: 100.0%\n",
      "iteration: 1364 | loss: 1.0331300829271308 | accuracy: 100.0%\n",
      "iteration: 1365 | loss: 1.0503919914582778 | accuracy: 100.0%\n",
      "iteration: 1366 | loss: 1.0129045709838793 | accuracy: 100.0%\n",
      "iteration: 1367 | loss: 1.0096204242764153 | accuracy: 100.0%\n",
      "iteration: 1368 | loss: 1.0077637884972128 | accuracy: 100.0%\n",
      "iteration: 1369 | loss: 1.0119544621021777 | accuracy: 100.0%\n",
      "iteration: 1370 | loss: 1.0249706384681119 | accuracy: 100.0%\n",
      "iteration: 1371 | loss: 1.0049464702287665 | accuracy: 100.0%\n",
      "iteration: 1372 | loss: 1.2033931846967931 | accuracy: 100.0%\n",
      "iteration: 1373 | loss: 1.0159021516380844 | accuracy: 100.0%\n",
      "iteration: 1374 | loss: 1.0052637898579693 | accuracy: 100.0%\n",
      "iteration: 1375 | loss: 1.0068680849424518 | accuracy: 100.0%\n",
      "iteration: 1376 | loss: 1.0046221755523095 | accuracy: 100.0%\n",
      "iteration: 1377 | loss: 1.0071546961725806 | accuracy: 100.0%\n",
      "iteration: 1378 | loss: 1.023015470582987 | accuracy: 100.0%\n",
      "iteration: 1379 | loss: 1.0057161046211212 | accuracy: 100.0%\n",
      "iteration: 1380 | loss: 1.0050933379544862 | accuracy: 100.0%\n",
      "iteration: 1381 | loss: 1.848047231766505 | accuracy: 50.0%\n",
      "iteration: 1382 | loss: 1.0133232836814143 | accuracy: 100.0%\n",
      "iteration: 1383 | loss: 1.3081368125319461 | accuracy: 100.0%\n",
      "iteration: 1384 | loss: 1.0092767197371972 | accuracy: 100.0%\n",
      "iteration: 1385 | loss: 1.0080374867626112 | accuracy: 100.0%\n",
      "iteration: 1386 | loss: 1.0029438144902372 | accuracy: 100.0%\n",
      "iteration: 1387 | loss: 1.0051348088741208 | accuracy: 100.0%\n",
      "iteration: 1388 | loss: 1.0616134676211393 | accuracy: 100.0%\n",
      "iteration: 1389 | loss: 1.0037061943682941 | accuracy: 100.0%\n",
      "iteration: 1390 | loss: 1.1363225244605146 | accuracy: 100.0%\n",
      "iteration: 1391 | loss: 1.003201741543112 | accuracy: 100.0%\n",
      "iteration: 1392 | loss: 1.0093546529425281 | accuracy: 100.0%\n",
      "iteration: 1393 | loss: 1.007268288323484 | accuracy: 100.0%\n",
      "iteration: 1394 | loss: 1.0431028542091676 | accuracy: 100.0%\n",
      "iteration: 1395 | loss: 1.00619461877019 | accuracy: 100.0%\n",
      "iteration: 1396 | loss: 1.0055465504992407 | accuracy: 100.0%\n",
      "iteration: 1397 | loss: 1.0192163675742298 | accuracy: 100.0%\n",
      "iteration: 1398 | loss: 1.0066767542461101 | accuracy: 100.0%\n",
      "iteration: 1399 | loss: 1.0072514763772202 | accuracy: 100.0%\n",
      "iteration: 1400 | loss: 1.021302839416512 | accuracy: 100.0%\n",
      "iteration: 1401 | loss: 1.0047870800199856 | accuracy: 100.0%\n",
      "iteration: 1402 | loss: 1.0097656127901855 | accuracy: 100.0%\n",
      "iteration: 1403 | loss: 1.0280634009921754 | accuracy: 100.0%\n",
      "iteration: 1404 | loss: 1.2773302223269576 | accuracy: 100.0%\n",
      "iteration: 1405 | loss: 1.017690050104996 | accuracy: 100.0%\n",
      "iteration: 1406 | loss: 1.004867204307626 | accuracy: 100.0%\n",
      "iteration: 1407 | loss: 1.0083884748454035 | accuracy: 100.0%\n",
      "iteration: 1408 | loss: 1.0024964449431426 | accuracy: 100.0%\n",
      "iteration: 1409 | loss: 1.0096316423774483 | accuracy: 100.0%\n",
      "iteration: 1410 | loss: 1.6200533376709403 | accuracy: 50.0%\n",
      "iteration: 1411 | loss: 1.0860529714263982 | accuracy: 100.0%\n",
      "iteration: 1412 | loss: 1.0585162640603107 | accuracy: 100.0%\n",
      "iteration: 1413 | loss: 1.0176706224145653 | accuracy: 100.0%\n",
      "iteration: 1414 | loss: 1.0956344719006605 | accuracy: 100.0%\n",
      "iteration: 1415 | loss: 1.0732817068191438 | accuracy: 100.0%\n",
      "iteration: 1416 | loss: 1.0124694671834806 | accuracy: 100.0%\n",
      "iteration: 1417 | loss: 1.0056496668811272 | accuracy: 100.0%\n",
      "iteration: 1418 | loss: 1.0616577179950364 | accuracy: 100.0%\n",
      "iteration: 1419 | loss: 1.1041350052755647 | accuracy: 100.0%\n",
      "iteration: 1420 | loss: 1.010063053284839 | accuracy: 100.0%\n",
      "iteration: 1421 | loss: 1.011314648428454 | accuracy: 100.0%\n",
      "iteration: 1422 | loss: 1.0132859276490627 | accuracy: 100.0%\n",
      "iteration: 1423 | loss: 1.00749427252921 | accuracy: 100.0%\n",
      "iteration: 1424 | loss: 1.0053882414777153 | accuracy: 100.0%\n",
      "iteration: 1425 | loss: 1.0211759926975537 | accuracy: 100.0%\n",
      "iteration: 1426 | loss: 1.0400052574261325 | accuracy: 100.0%\n",
      "iteration: 1427 | loss: 1.0086696497781047 | accuracy: 100.0%\n",
      "iteration: 1428 | loss: 1.0297933050662755 | accuracy: 100.0%\n",
      "iteration: 1429 | loss: 1.0859620018352927 | accuracy: 100.0%\n",
      "iteration: 1430 | loss: 1.003663034589261 | accuracy: 100.0%\n",
      "iteration: 1431 | loss: 1.0481369807940804 | accuracy: 100.0%\n",
      "iteration: 1432 | loss: 1.0220851420793624 | accuracy: 100.0%\n",
      "iteration: 1433 | loss: 1.0151482655200619 | accuracy: 100.0%\n",
      "iteration: 1434 | loss: 1.012396157875847 | accuracy: 100.0%\n",
      "iteration: 1435 | loss: 1.0393956152510846 | accuracy: 100.0%\n",
      "iteration: 1436 | loss: 1.0083379420494245 | accuracy: 100.0%\n",
      "iteration: 1437 | loss: 1.074952308900159 | accuracy: 100.0%\n",
      "iteration: 1438 | loss: 1.3674208295530301 | accuracy: 100.0%\n",
      "iteration: 1439 | loss: 1.007026585404861 | accuracy: 100.0%\n",
      "iteration: 1440 | loss: 1.051219753596435 | accuracy: 100.0%\n",
      "iteration: 1441 | loss: 1.0028953364008681 | accuracy: 100.0%\n",
      "iteration: 1442 | loss: 1.0036038516905665 | accuracy: 100.0%\n",
      "iteration: 1443 | loss: 1.0091592388926691 | accuracy: 100.0%\n",
      "iteration: 1444 | loss: 1.0026422597079925 | accuracy: 100.0%\n",
      "iteration: 1445 | loss: 1.005973833903693 | accuracy: 100.0%\n",
      "iteration: 1446 | loss: 1.0219296570268037 | accuracy: 100.0%\n",
      "iteration: 1447 | loss: 1.0240794020737303 | accuracy: 100.0%\n",
      "iteration: 1448 | loss: 1.002992737656556 | accuracy: 100.0%\n",
      "iteration: 1449 | loss: 1.0067475090290625 | accuracy: 100.0%\n",
      "iteration: 1450 | loss: 1.0072875541625852 | accuracy: 100.0%\n",
      "iteration: 1451 | loss: 1.002662698803963 | accuracy: 100.0%\n",
      "iteration: 1452 | loss: 1.0317383341757362 | accuracy: 100.0%\n",
      "iteration: 1453 | loss: 1.0144770349234864 | accuracy: 100.0%\n",
      "iteration: 1454 | loss: 1.0117369906837894 | accuracy: 100.0%\n",
      "iteration: 1455 | loss: 1.0095325902808239 | accuracy: 100.0%\n",
      "iteration: 1456 | loss: 1.0017463177560006 | accuracy: 100.0%\n",
      "iteration: 1457 | loss: 1.0031239795618385 | accuracy: 100.0%\n",
      "iteration: 1458 | loss: 1.0024902976382941 | accuracy: 100.0%\n",
      "iteration: 1459 | loss: 1.0064375600946747 | accuracy: 100.0%\n",
      "iteration: 1460 | loss: 1.0064712694742093 | accuracy: 100.0%\n",
      "iteration: 1461 | loss: 1.2895898989279337 | accuracy: 100.0%\n",
      "iteration: 1462 | loss: 1.0091714396036904 | accuracy: 100.0%\n",
      "iteration: 1463 | loss: 1.009110026545174 | accuracy: 100.0%\n",
      "iteration: 1464 | loss: 1.0029506387278377 | accuracy: 100.0%\n",
      "iteration: 1465 | loss: 1.0172652835739011 | accuracy: 100.0%\n",
      "iteration: 1466 | loss: 1.0042775152887948 | accuracy: 100.0%\n",
      "iteration: 1467 | loss: 1.0769393151565132 | accuracy: 100.0%\n",
      "iteration: 1468 | loss: 1.0075308425478038 | accuracy: 100.0%\n",
      "iteration: 1469 | loss: 1.0089154563210052 | accuracy: 100.0%\n",
      "iteration: 1470 | loss: 1.0057686918410373 | accuracy: 100.0%\n",
      "iteration: 1471 | loss: 1.0178603710996825 | accuracy: 100.0%\n",
      "iteration: 1472 | loss: 1.0029327814901392 | accuracy: 100.0%\n",
      "iteration: 1473 | loss: 1.006999872292482 | accuracy: 100.0%\n",
      "iteration: 1474 | loss: 1.1777828540786774 | accuracy: 100.0%\n",
      "iteration: 1475 | loss: 1.004259798953017 | accuracy: 100.0%\n",
      "iteration: 1476 | loss: 1.0205043388317192 | accuracy: 100.0%\n",
      "iteration: 1477 | loss: 1.0021528605590546 | accuracy: 100.0%\n",
      "iteration: 1478 | loss: 1.0029065923516682 | accuracy: 100.0%\n",
      "iteration: 1479 | loss: 1.0046591897175279 | accuracy: 100.0%\n",
      "iteration: 1480 | loss: 1.0115627305720638 | accuracy: 100.0%\n",
      "iteration: 1481 | loss: 1.0072237050227564 | accuracy: 100.0%\n",
      "iteration: 1482 | loss: 1.0056513585626676 | accuracy: 100.0%\n",
      "iteration: 1483 | loss: 1.0019498687108843 | accuracy: 100.0%\n",
      "iteration: 1484 | loss: 1.0150060784137636 | accuracy: 100.0%\n",
      "iteration: 1485 | loss: 1.1644145071219532 | accuracy: 100.0%\n",
      "iteration: 1486 | loss: 1.0111431042038594 | accuracy: 100.0%\n",
      "iteration: 1487 | loss: 1.045290312286812 | accuracy: 100.0%\n",
      "iteration: 1488 | loss: 1.0533388495032994 | accuracy: 100.0%\n",
      "iteration: 1489 | loss: 1.0611893617704136 | accuracy: 100.0%\n",
      "iteration: 1490 | loss: 1.0019682899867581 | accuracy: 100.0%\n",
      "iteration: 1491 | loss: 1.0062546175157163 | accuracy: 100.0%\n",
      "iteration: 1492 | loss: 1.0076498495741724 | accuracy: 100.0%\n",
      "iteration: 1493 | loss: 1.0024528936329655 | accuracy: 100.0%\n",
      "iteration: 1494 | loss: 1.0054230831062918 | accuracy: 100.0%\n",
      "iteration: 1495 | loss: 1.2632123409878537 | accuracy: 100.0%\n",
      "iteration: 1496 | loss: 1.0042767468005407 | accuracy: 100.0%\n",
      "iteration: 1497 | loss: 1.0097072378126886 | accuracy: 100.0%\n",
      "iteration: 1498 | loss: 1.0024198974186616 | accuracy: 100.0%\n",
      "iteration: 1499 | loss: 1.0027827335022914 | accuracy: 100.0%\n",
      "iteration: 1500 | loss: 1.0169885030484898 | accuracy: 100.0%\n",
      "iteration: 1501 | loss: 1.00412184461565 | accuracy: 100.0%\n",
      "iteration: 1502 | loss: 1.151551154861759 | accuracy: 100.0%\n",
      "iteration: 1503 | loss: 1.0040639627186692 | accuracy: 100.0%\n",
      "iteration: 1504 | loss: 1.0064470383875073 | accuracy: 100.0%\n",
      "iteration: 1505 | loss: 1.099641517971902 | accuracy: 100.0%\n",
      "iteration: 1506 | loss: 1.0138846769088068 | accuracy: 100.0%\n",
      "iteration: 1507 | loss: 1.0025285050185346 | accuracy: 100.0%\n",
      "iteration: 1508 | loss: 1.023219023966894 | accuracy: 100.0%\n",
      "iteration: 1509 | loss: 1.00231858402681 | accuracy: 100.0%\n",
      "iteration: 1510 | loss: 1.0021122625156815 | accuracy: 100.0%\n",
      "iteration: 1511 | loss: 1.0033950768292361 | accuracy: 100.0%\n",
      "iteration: 1512 | loss: 1.0040365143612606 | accuracy: 100.0%\n",
      "iteration: 1513 | loss: 1.0400351628185396 | accuracy: 100.0%\n",
      "iteration: 1514 | loss: 1.0028623025115968 | accuracy: 100.0%\n",
      "iteration: 1515 | loss: 1.0092831691697175 | accuracy: 100.0%\n",
      "iteration: 1516 | loss: 1.0045821217338968 | accuracy: 100.0%\n",
      "iteration: 1517 | loss: 1.0262124330656237 | accuracy: 100.0%\n",
      "iteration: 1518 | loss: 1.0056208227898726 | accuracy: 100.0%\n",
      "iteration: 1519 | loss: 1.0023895359582926 | accuracy: 100.0%\n",
      "iteration: 1520 | loss: 1.0059005155780731 | accuracy: 100.0%\n",
      "iteration: 1521 | loss: 1.0423913570795575 | accuracy: 100.0%\n",
      "iteration: 1522 | loss: 1.007508468395555 | accuracy: 100.0%\n",
      "iteration: 1523 | loss: 1.0331134432503286 | accuracy: 100.0%\n",
      "iteration: 1524 | loss: 1.0158149010176727 | accuracy: 100.0%\n",
      "iteration: 1525 | loss: 1.003699362859456 | accuracy: 100.0%\n",
      "iteration: 1526 | loss: 1.003114671551362 | accuracy: 100.0%\n",
      "iteration: 1527 | loss: 1.0194701386386091 | accuracy: 100.0%\n",
      "iteration: 1528 | loss: 1.0071190766727955 | accuracy: 100.0%\n",
      "iteration: 1529 | loss: 1.0064741036857618 | accuracy: 100.0%\n",
      "iteration: 1530 | loss: 1.0152352992683797 | accuracy: 100.0%\n",
      "iteration: 1531 | loss: 1.0106942233463079 | accuracy: 100.0%\n",
      "iteration: 1532 | loss: 1.0170544570529791 | accuracy: 100.0%\n",
      "iteration: 1533 | loss: 1.0027586061378972 | accuracy: 100.0%\n",
      "iteration: 1534 | loss: 1.0043867262086652 | accuracy: 100.0%\n",
      "iteration: 1535 | loss: 1.488097840940646 | accuracy: 100.0%\n",
      "iteration: 1536 | loss: 1.0115947998501778 | accuracy: 100.0%\n",
      "iteration: 1537 | loss: 1.0030830666055282 | accuracy: 100.0%\n",
      "iteration: 1538 | loss: 1.005947412848598 | accuracy: 100.0%\n",
      "iteration: 1539 | loss: 1.003758767821797 | accuracy: 100.0%\n",
      "iteration: 1540 | loss: 1.0076894149526372 | accuracy: 100.0%\n",
      "iteration: 1541 | loss: 1.009536419966747 | accuracy: 100.0%\n",
      "iteration: 1542 | loss: 1.0047643942917153 | accuracy: 100.0%\n",
      "iteration: 1543 | loss: 1.0409619141154676 | accuracy: 100.0%\n",
      "iteration: 1544 | loss: 1.004352783762889 | accuracy: 100.0%\n",
      "iteration: 1545 | loss: 1.0121590082339873 | accuracy: 100.0%\n",
      "iteration: 1546 | loss: 1.0029203740483097 | accuracy: 100.0%\n",
      "iteration: 1547 | loss: 1.0035840690688491 | accuracy: 100.0%\n",
      "iteration: 1548 | loss: 1.0042943712413956 | accuracy: 100.0%\n",
      "iteration: 1549 | loss: 1.013929848934398 | accuracy: 100.0%\n",
      "iteration: 1550 | loss: 1.2705483211216604 | accuracy: 100.0%\n",
      "iteration: 1551 | loss: 1.003539332114084 | accuracy: 100.0%\n",
      "iteration: 1552 | loss: 1.0023751686167839 | accuracy: 100.0%\n",
      "iteration: 1553 | loss: 1.0038635644666962 | accuracy: 100.0%\n",
      "iteration: 1554 | loss: 1.0058522125516116 | accuracy: 100.0%\n",
      "iteration: 1555 | loss: 1.0028312912479311 | accuracy: 100.0%\n",
      "iteration: 1556 | loss: 1.0059568341654037 | accuracy: 100.0%\n",
      "iteration: 1557 | loss: 1.0038272835911752 | accuracy: 100.0%\n",
      "iteration: 1558 | loss: 1.0064842644512828 | accuracy: 100.0%\n",
      "iteration: 1559 | loss: 1.0033447016795531 | accuracy: 100.0%\n",
      "iteration: 1560 | loss: 1.0042629472481166 | accuracy: 100.0%\n",
      "iteration: 1561 | loss: 1.0069931558005616 | accuracy: 100.0%\n",
      "iteration: 1562 | loss: 1.0039000645376381 | accuracy: 100.0%\n",
      "iteration: 1563 | loss: 1.0117240501100704 | accuracy: 100.0%\n",
      "iteration: 1564 | loss: 1.0026580176535242 | accuracy: 100.0%\n",
      "iteration: 1565 | loss: 1.0026860386184369 | accuracy: 100.0%\n",
      "iteration: 1566 | loss: 1.0027207139348047 | accuracy: 100.0%\n",
      "iteration: 1567 | loss: 1.0104538436195818 | accuracy: 100.0%\n",
      "iteration: 1568 | loss: 1.0030812087002992 | accuracy: 100.0%\n",
      "iteration: 1569 | loss: 1.0099231314309418 | accuracy: 100.0%\n",
      "iteration: 1570 | loss: 1.027520864613788 | accuracy: 100.0%\n",
      "iteration: 1571 | loss: 1.0201637723483514 | accuracy: 100.0%\n",
      "iteration: 1572 | loss: 1.0068014625186201 | accuracy: 100.0%\n",
      "iteration: 1573 | loss: 1.002071198649053 | accuracy: 100.0%\n",
      "iteration: 1574 | loss: 1.0028156135157227 | accuracy: 100.0%\n",
      "iteration: 1575 | loss: 1.0179544646015457 | accuracy: 100.0%\n",
      "iteration: 1576 | loss: 1.0038783576922985 | accuracy: 100.0%\n",
      "iteration: 1577 | loss: 1.003837023317188 | accuracy: 100.0%\n",
      "iteration: 1578 | loss: 1.0018638667246822 | accuracy: 100.0%\n",
      "iteration: 1579 | loss: 1.0218246562213138 | accuracy: 100.0%\n",
      "iteration: 1580 | loss: 1.0180610832083996 | accuracy: 100.0%\n",
      "iteration: 1581 | loss: 1.0017881784970506 | accuracy: 100.0%\n",
      "iteration: 1582 | loss: 1.0047926618090475 | accuracy: 100.0%\n",
      "iteration: 1583 | loss: 1.003935745632471 | accuracy: 100.0%\n",
      "iteration: 1584 | loss: 1.0025614496549564 | accuracy: 100.0%\n",
      "iteration: 1585 | loss: 1.0038506451788527 | accuracy: 100.0%\n",
      "iteration: 1586 | loss: 1.0066650036816498 | accuracy: 100.0%\n",
      "iteration: 1587 | loss: 1.0063520392538294 | accuracy: 100.0%\n",
      "iteration: 1588 | loss: 1.5464909416306476 | accuracy: 50.0%\n",
      "iteration: 1589 | loss: 1.0037184069644762 | accuracy: 100.0%\n",
      "iteration: 1590 | loss: 1.0067433901863148 | accuracy: 100.0%\n",
      "iteration: 1591 | loss: 1.003741704620015 | accuracy: 100.0%\n",
      "iteration: 1592 | loss: 1.056388357153887 | accuracy: 100.0%\n",
      "iteration: 1593 | loss: 1.0123084323996467 | accuracy: 100.0%\n",
      "iteration: 1594 | loss: 1.0238657146300765 | accuracy: 100.0%\n",
      "iteration: 1595 | loss: 1.0097278837473624 | accuracy: 100.0%\n",
      "iteration: 1596 | loss: 1.0396719284405218 | accuracy: 100.0%\n",
      "iteration: 1597 | loss: 1.097010965883035 | accuracy: 100.0%\n",
      "iteration: 1598 | loss: 1.066745320340992 | accuracy: 100.0%\n",
      "iteration: 1599 | loss: 1.002212164008318 | accuracy: 100.0%\n",
      "iteration: 1600 | loss: 1.0066966909241462 | accuracy: 100.0%\n",
      "iteration: 1601 | loss: 1.0022014989630086 | accuracy: 100.0%\n",
      "iteration: 1602 | loss: 1.0693025325738756 | accuracy: 100.0%\n",
      "iteration: 1603 | loss: 1.0080270494398622 | accuracy: 100.0%\n",
      "iteration: 1604 | loss: 1.0038699794947 | accuracy: 100.0%\n",
      "iteration: 1605 | loss: 1.049293966667576 | accuracy: 100.0%\n",
      "iteration: 1606 | loss: 1.0057971552802034 | accuracy: 100.0%\n",
      "iteration: 1607 | loss: 1.095440657165227 | accuracy: 100.0%\n",
      "iteration: 1608 | loss: 1.0491112643640335 | accuracy: 100.0%\n",
      "iteration: 1609 | loss: 1.00748497784936 | accuracy: 100.0%\n",
      "iteration: 1610 | loss: 1.0206127665661573 | accuracy: 100.0%\n",
      "iteration: 1611 | loss: 1.0017289463820012 | accuracy: 100.0%\n",
      "iteration: 1612 | loss: 1.0058008601821367 | accuracy: 100.0%\n",
      "iteration: 1613 | loss: 1.0179215242334072 | accuracy: 100.0%\n",
      "iteration: 1614 | loss: 1.0054622018301655 | accuracy: 100.0%\n",
      "iteration: 1615 | loss: 1.0062072174319507 | accuracy: 100.0%\n",
      "iteration: 1616 | loss: 1.0021258787356944 | accuracy: 100.0%\n",
      "iteration: 1617 | loss: 1.0056232811281178 | accuracy: 100.0%\n",
      "iteration: 1618 | loss: 1.018788536911842 | accuracy: 100.0%\n",
      "iteration: 1619 | loss: 1.0042855012888234 | accuracy: 100.0%\n",
      "iteration: 1620 | loss: 1.0024845808745477 | accuracy: 100.0%\n",
      "iteration: 1621 | loss: 1.0085031852914934 | accuracy: 100.0%\n",
      "iteration: 1622 | loss: 1.0021366167847683 | accuracy: 100.0%\n",
      "iteration: 1623 | loss: 1.141393284672992 | accuracy: 100.0%\n",
      "iteration: 1624 | loss: 1.0077231135483455 | accuracy: 100.0%\n",
      "iteration: 1625 | loss: 1.0299629843600404 | accuracy: 100.0%\n",
      "iteration: 1626 | loss: 1.0090278270808322 | accuracy: 100.0%\n",
      "iteration: 1627 | loss: 1.0145746719773767 | accuracy: 100.0%\n",
      "iteration: 1628 | loss: 1.0048376705828677 | accuracy: 100.0%\n",
      "iteration: 1629 | loss: 1.0015874817370438 | accuracy: 100.0%\n",
      "iteration: 1630 | loss: 1.003426941763583 | accuracy: 100.0%\n",
      "iteration: 1631 | loss: 1.010351657750039 | accuracy: 100.0%\n",
      "iteration: 1632 | loss: 1.0023886036845913 | accuracy: 100.0%\n",
      "iteration: 1633 | loss: 1.0061399805055493 | accuracy: 100.0%\n",
      "iteration: 1634 | loss: 1.0167197923045348 | accuracy: 100.0%\n",
      "iteration: 1635 | loss: 1.0074244426628383 | accuracy: 100.0%\n",
      "iteration: 1636 | loss: 1.0076077271881847 | accuracy: 100.0%\n",
      "iteration: 1637 | loss: 1.338596228786998 | accuracy: 100.0%\n",
      "iteration: 1638 | loss: 1.275426743534537 | accuracy: 100.0%\n",
      "iteration: 1639 | loss: 1.0038629108173793 | accuracy: 100.0%\n",
      "iteration: 1640 | loss: 1.0114578196695454 | accuracy: 100.0%\n",
      "iteration: 1641 | loss: 1.007378855516462 | accuracy: 100.0%\n",
      "iteration: 1642 | loss: 1.0027277061578361 | accuracy: 100.0%\n",
      "iteration: 1643 | loss: 1.0039364784979512 | accuracy: 100.0%\n",
      "iteration: 1644 | loss: 1.0037947519516146 | accuracy: 100.0%\n",
      "iteration: 1645 | loss: 1.0022632779920155 | accuracy: 100.0%\n",
      "iteration: 1646 | loss: 1.0075275484472213 | accuracy: 100.0%\n",
      "iteration: 1647 | loss: 1.0047428679285069 | accuracy: 100.0%\n",
      "iteration: 1648 | loss: 1.003490725375525 | accuracy: 100.0%\n",
      "iteration: 1649 | loss: 1.0040962440733208 | accuracy: 100.0%\n",
      "iteration: 1650 | loss: 1.0019485396621524 | accuracy: 100.0%\n",
      "iteration: 1651 | loss: 1.0109887240024373 | accuracy: 100.0%\n",
      "iteration: 1652 | loss: 1.0022998755850516 | accuracy: 100.0%\n",
      "iteration: 1653 | loss: 1.004419291758738 | accuracy: 100.0%\n",
      "iteration: 1654 | loss: 1.0268410944681743 | accuracy: 100.0%\n",
      "iteration: 1655 | loss: 1.0025987913736774 | accuracy: 100.0%\n",
      "iteration: 1656 | loss: 1.0063007774702688 | accuracy: 100.0%\n",
      "iteration: 1657 | loss: 1.0024775876132608 | accuracy: 100.0%\n",
      "iteration: 1658 | loss: 1.0019328412146324 | accuracy: 100.0%\n",
      "iteration: 1659 | loss: 1.0057745191454417 | accuracy: 100.0%\n",
      "iteration: 1660 | loss: 1.0158858719316644 | accuracy: 100.0%\n",
      "iteration: 1661 | loss: 1.0055363871166125 | accuracy: 100.0%\n",
      "iteration: 1662 | loss: 1.0056243075679367 | accuracy: 100.0%\n",
      "iteration: 1663 | loss: 1.0127625716817361 | accuracy: 100.0%\n",
      "iteration: 1664 | loss: 1.005465319043755 | accuracy: 100.0%\n",
      "iteration: 1665 | loss: 1.1719091986667014 | accuracy: 100.0%\n",
      "iteration: 1666 | loss: 1.005040149490476 | accuracy: 100.0%\n",
      "iteration: 1667 | loss: 1.004679314057728 | accuracy: 100.0%\n",
      "iteration: 1668 | loss: 1.0074364824567048 | accuracy: 100.0%\n",
      "iteration: 1669 | loss: 1.27357603929895 | accuracy: 100.0%\n",
      "iteration: 1670 | loss: 1.1128613109343453 | accuracy: 100.0%\n",
      "iteration: 1671 | loss: 1.0111767039984179 | accuracy: 100.0%\n",
      "iteration: 1672 | loss: 1.0222092211977643 | accuracy: 100.0%\n",
      "iteration: 1673 | loss: 1.0062381084398766 | accuracy: 100.0%\n",
      "iteration: 1674 | loss: 1.0022915550359677 | accuracy: 100.0%\n",
      "iteration: 1675 | loss: 1.0252174268136935 | accuracy: 100.0%\n",
      "iteration: 1676 | loss: 1.0034132719523106 | accuracy: 100.0%\n",
      "iteration: 1677 | loss: 1.040720029471227 | accuracy: 100.0%\n",
      "iteration: 1678 | loss: 1.0108141439926672 | accuracy: 100.0%\n",
      "iteration: 1679 | loss: 1.0127174781304455 | accuracy: 100.0%\n",
      "iteration: 1680 | loss: 1.0046470241524235 | accuracy: 100.0%\n",
      "iteration: 1681 | loss: 1.0102533573491514 | accuracy: 100.0%\n",
      "iteration: 1682 | loss: 1.0018081874772746 | accuracy: 100.0%\n",
      "iteration: 1683 | loss: 1.005710995860096 | accuracy: 100.0%\n",
      "iteration: 1684 | loss: 1.0025049948384646 | accuracy: 100.0%\n",
      "iteration: 1685 | loss: 1.008213935136324 | accuracy: 100.0%\n",
      "iteration: 1686 | loss: 1.0017854008268947 | accuracy: 100.0%\n",
      "iteration: 1687 | loss: 1.0064890831676725 | accuracy: 100.0%\n",
      "iteration: 1688 | loss: 1.0029605866517275 | accuracy: 100.0%\n",
      "iteration: 1689 | loss: 1.00334585900479 | accuracy: 100.0%\n",
      "iteration: 1690 | loss: 1.0068070348118225 | accuracy: 100.0%\n",
      "iteration: 1691 | loss: 1.0132715261621965 | accuracy: 100.0%\n",
      "iteration: 1692 | loss: 1.003994940763148 | accuracy: 100.0%\n",
      "iteration: 1693 | loss: 1.0071119802876651 | accuracy: 100.0%\n",
      "iteration: 1694 | loss: 1.0036015348276932 | accuracy: 100.0%\n",
      "iteration: 1695 | loss: 1.0045553386489416 | accuracy: 100.0%\n",
      "iteration: 1696 | loss: 1.0059595965177583 | accuracy: 100.0%\n",
      "iteration: 1697 | loss: 1.0356515331621334 | accuracy: 100.0%\n",
      "iteration: 1698 | loss: 1.0068420079259615 | accuracy: 100.0%\n",
      "iteration: 1699 | loss: 1.0118848860512493 | accuracy: 100.0%\n",
      "iteration: 1700 | loss: 1.0022428312677711 | accuracy: 100.0%\n",
      "iteration: 1701 | loss: 1.0028580797046323 | accuracy: 100.0%\n",
      "iteration: 1702 | loss: 1.0042990360753694 | accuracy: 100.0%\n",
      "iteration: 1703 | loss: 1.0036836911512017 | accuracy: 100.0%\n",
      "iteration: 1704 | loss: 1.0094780461314095 | accuracy: 100.0%\n",
      "iteration: 1705 | loss: 1.002281308600436 | accuracy: 100.0%\n",
      "iteration: 1706 | loss: 1.005998549693115 | accuracy: 100.0%\n",
      "iteration: 1707 | loss: 1.0022297069857418 | accuracy: 100.0%\n",
      "iteration: 1708 | loss: 1.0039280002139508 | accuracy: 100.0%\n",
      "iteration: 1709 | loss: 1.0090364137785515 | accuracy: 100.0%\n",
      "iteration: 1710 | loss: 1.0026214113211298 | accuracy: 100.0%\n",
      "iteration: 1711 | loss: 1.0054757639499075 | accuracy: 100.0%\n",
      "iteration: 1712 | loss: 1.0037571705726807 | accuracy: 100.0%\n",
      "iteration: 1713 | loss: 1.0212076399539998 | accuracy: 100.0%\n",
      "iteration: 1714 | loss: 1.0064619356924271 | accuracy: 100.0%\n",
      "iteration: 1715 | loss: 1.0049514331884237 | accuracy: 100.0%\n",
      "iteration: 1716 | loss: 1.211622800028462 | accuracy: 100.0%\n",
      "iteration: 1717 | loss: 1.2885574920743652 | accuracy: 100.0%\n",
      "iteration: 1718 | loss: 1.0171578366201635 | accuracy: 100.0%\n",
      "iteration: 1719 | loss: 1.0749366085830125 | accuracy: 100.0%\n",
      "iteration: 1720 | loss: 1.0107167751453814 | accuracy: 100.0%\n",
      "iteration: 1721 | loss: 1.0287957159107293 | accuracy: 100.0%\n",
      "iteration: 1722 | loss: 1.0464027135865273 | accuracy: 100.0%\n",
      "iteration: 1723 | loss: 1.0042665005333244 | accuracy: 100.0%\n",
      "iteration: 1724 | loss: 1.00942633924351 | accuracy: 100.0%\n",
      "iteration: 1725 | loss: 1.0055256122393101 | accuracy: 100.0%\n",
      "iteration: 1726 | loss: 1.0082955914035352 | accuracy: 100.0%\n",
      "iteration: 1727 | loss: 1.0063974962364677 | accuracy: 100.0%\n",
      "iteration: 1728 | loss: 1.003483967489686 | accuracy: 100.0%\n",
      "iteration: 1729 | loss: 1.0184465806302951 | accuracy: 100.0%\n",
      "iteration: 1730 | loss: 1.0101580324370956 | accuracy: 100.0%\n",
      "iteration: 1731 | loss: 1.0313238924817172 | accuracy: 100.0%\n",
      "iteration: 1732 | loss: 1.002045884129576 | accuracy: 100.0%\n",
      "iteration: 1733 | loss: 1.0046963976245273 | accuracy: 100.0%\n",
      "iteration: 1734 | loss: 1.0026055675356527 | accuracy: 100.0%\n",
      "iteration: 1735 | loss: 1.0051319601594972 | accuracy: 100.0%\n",
      "iteration: 1736 | loss: 1.0041450123561522 | accuracy: 100.0%\n",
      "iteration: 1737 | loss: 1.2894982571606608 | accuracy: 100.0%\n",
      "iteration: 1738 | loss: 1.0162902126497488 | accuracy: 100.0%\n",
      "iteration: 1739 | loss: 1.0028082830219902 | accuracy: 100.0%\n",
      "iteration: 1740 | loss: 1.002137335483845 | accuracy: 100.0%\n",
      "iteration: 1741 | loss: 1.156185506000503 | accuracy: 100.0%\n",
      "iteration: 1742 | loss: 1.0055714528605226 | accuracy: 100.0%\n",
      "iteration: 1743 | loss: 1.0043022737745813 | accuracy: 100.0%\n",
      "iteration: 1744 | loss: 1.005473509660548 | accuracy: 100.0%\n",
      "iteration: 1745 | loss: 1.0350212045024199 | accuracy: 100.0%\n",
      "iteration: 1746 | loss: 1.2252892763311896 | accuracy: 100.0%\n",
      "iteration: 1747 | loss: 1.384999855325181 | accuracy: 100.0%\n",
      "iteration: 1748 | loss: 1.0026159081273287 | accuracy: 100.0%\n",
      "iteration: 1749 | loss: 1.0065564444260995 | accuracy: 100.0%\n",
      "iteration: 1750 | loss: 1.0039992768315729 | accuracy: 100.0%\n",
      "iteration: 1751 | loss: 1.0015989184629996 | accuracy: 100.0%\n",
      "iteration: 1752 | loss: 1.0025369395433412 | accuracy: 100.0%\n",
      "iteration: 1753 | loss: 1.0081370239441791 | accuracy: 100.0%\n",
      "iteration: 1754 | loss: 1.005453851027245 | accuracy: 100.0%\n",
      "iteration: 1755 | loss: 1.1243831623606473 | accuracy: 100.0%\n",
      "iteration: 1756 | loss: 1.0202771505996802 | accuracy: 100.0%\n",
      "iteration: 1757 | loss: 1.0044077883488136 | accuracy: 100.0%\n",
      "iteration: 1758 | loss: 1.0044370985915707 | accuracy: 100.0%\n",
      "iteration: 1759 | loss: 1.0262091417274688 | accuracy: 100.0%\n",
      "iteration: 1760 | loss: 1.0176305658279572 | accuracy: 100.0%\n",
      "iteration: 1761 | loss: 1.0038916189798763 | accuracy: 100.0%\n",
      "iteration: 1762 | loss: 1.0022820500107203 | accuracy: 100.0%\n",
      "iteration: 1763 | loss: 1.002265216801601 | accuracy: 100.0%\n",
      "iteration: 1764 | loss: 1.0030924996257133 | accuracy: 100.0%\n",
      "iteration: 1765 | loss: 1.0035342969413479 | accuracy: 100.0%\n",
      "iteration: 1766 | loss: 1.002997302402092 | accuracy: 100.0%\n",
      "iteration: 1767 | loss: 1.0120015955071227 | accuracy: 100.0%\n",
      "iteration: 1768 | loss: 1.0177939909861096 | accuracy: 100.0%\n",
      "iteration: 1769 | loss: 1.004869559165904 | accuracy: 100.0%\n",
      "iteration: 1770 | loss: 1.002412552083189 | accuracy: 100.0%\n",
      "iteration: 1771 | loss: 1.004730470139184 | accuracy: 100.0%\n",
      "iteration: 1772 | loss: 1.0642175766803113 | accuracy: 100.0%\n",
      "iteration: 1773 | loss: 1.0076849956084997 | accuracy: 100.0%\n",
      "iteration: 1774 | loss: 1.0087278214120778 | accuracy: 100.0%\n",
      "iteration: 1775 | loss: 1.0102455403418706 | accuracy: 100.0%\n",
      "iteration: 1776 | loss: 1.0020292540263251 | accuracy: 100.0%\n",
      "iteration: 1777 | loss: 1.0071108534465065 | accuracy: 100.0%\n",
      "iteration: 1778 | loss: 1.0080290228517306 | accuracy: 100.0%\n",
      "iteration: 1779 | loss: 1.0027208467277187 | accuracy: 100.0%\n",
      "iteration: 1780 | loss: 1.0055356948955207 | accuracy: 100.0%\n",
      "iteration: 1781 | loss: 1.0061128848185659 | accuracy: 100.0%\n",
      "iteration: 1782 | loss: 1.0080801019859083 | accuracy: 100.0%\n",
      "iteration: 1783 | loss: 1.0035363066530518 | accuracy: 100.0%\n",
      "iteration: 1784 | loss: 1.0288704574820544 | accuracy: 100.0%\n",
      "iteration: 1785 | loss: 1.0020331358571406 | accuracy: 100.0%\n",
      "iteration: 1786 | loss: 1.0530160589278228 | accuracy: 100.0%\n",
      "iteration: 1787 | loss: 1.0052997522230904 | accuracy: 100.0%\n",
      "iteration: 1788 | loss: 1.0026240288038346 | accuracy: 100.0%\n",
      "iteration: 1789 | loss: 1.023765198338695 | accuracy: 100.0%\n",
      "iteration: 1790 | loss: 1.0021015337536991 | accuracy: 100.0%\n",
      "iteration: 1791 | loss: 1.001694320752552 | accuracy: 100.0%\n",
      "iteration: 1792 | loss: 1.0028291317715479 | accuracy: 100.0%\n",
      "iteration: 1793 | loss: 1.0024706782624393 | accuracy: 100.0%\n",
      "iteration: 1794 | loss: 1.07221175630514 | accuracy: 100.0%\n",
      "iteration: 1795 | loss: 1.0016509595077272 | accuracy: 100.0%\n",
      "iteration: 1796 | loss: 1.0439677268892762 | accuracy: 100.0%\n",
      "iteration: 1797 | loss: 1.0046345353806065 | accuracy: 100.0%\n",
      "iteration: 1798 | loss: 1.0019868082474308 | accuracy: 100.0%\n",
      "iteration: 1799 | loss: 1.0022682745308928 | accuracy: 100.0%\n",
      "iteration: 1800 | loss: 1.0108580535372234 | accuracy: 100.0%\n",
      "iteration: 1801 | loss: 1.0020824276867608 | accuracy: 100.0%\n",
      "iteration: 1802 | loss: 1.0024153180806004 | accuracy: 100.0%\n",
      "iteration: 1803 | loss: 1.4035298290723743 | accuracy: 100.0%\n",
      "iteration: 1804 | loss: 1.0314322325228824 | accuracy: 100.0%\n",
      "iteration: 1805 | loss: 1.0115783161127947 | accuracy: 100.0%\n",
      "iteration: 1806 | loss: 1.006754551910233 | accuracy: 100.0%\n",
      "iteration: 1807 | loss: 1.021099941622369 | accuracy: 100.0%\n",
      "iteration: 1808 | loss: 1.0079256898912727 | accuracy: 100.0%\n",
      "iteration: 1809 | loss: 1.0106665640173071 | accuracy: 100.0%\n",
      "iteration: 1810 | loss: 1.0022740561236907 | accuracy: 100.0%\n",
      "iteration: 1811 | loss: 1.0145045034458575 | accuracy: 100.0%\n",
      "iteration: 1812 | loss: 1.0051419791909444 | accuracy: 100.0%\n",
      "iteration: 1813 | loss: 1.002582935446163 | accuracy: 100.0%\n",
      "iteration: 1814 | loss: 1.0108827999258299 | accuracy: 100.0%\n",
      "iteration: 1815 | loss: 1.002385133314019 | accuracy: 100.0%\n",
      "iteration: 1816 | loss: 1.011916676233257 | accuracy: 100.0%\n",
      "iteration: 1817 | loss: 1.003063003984818 | accuracy: 100.0%\n",
      "iteration: 1818 | loss: 1.0328214452349866 | accuracy: 100.0%\n",
      "iteration: 1819 | loss: 1.0024396141444032 | accuracy: 100.0%\n",
      "iteration: 1820 | loss: 1.0634234329260808 | accuracy: 100.0%\n",
      "iteration: 1821 | loss: 1.003663223317585 | accuracy: 100.0%\n",
      "iteration: 1822 | loss: 1.0130537463364553 | accuracy: 100.0%\n",
      "iteration: 1823 | loss: 1.0069552122945016 | accuracy: 100.0%\n",
      "iteration: 1824 | loss: 1.0045526323025282 | accuracy: 100.0%\n",
      "iteration: 1825 | loss: 1.0236844466707071 | accuracy: 100.0%\n",
      "iteration: 1826 | loss: 1.0039623991118172 | accuracy: 100.0%\n",
      "iteration: 1827 | loss: 1.004340987777365 | accuracy: 100.0%\n",
      "iteration: 1828 | loss: 1.006566128862309 | accuracy: 100.0%\n",
      "iteration: 1829 | loss: 1.016900174743539 | accuracy: 100.0%\n",
      "iteration: 1830 | loss: 1.0028670735105893 | accuracy: 100.0%\n",
      "iteration: 1831 | loss: 1.010475635213895 | accuracy: 100.0%\n",
      "iteration: 1832 | loss: 1.008460261239927 | accuracy: 100.0%\n",
      "iteration: 1833 | loss: 1.0025005015701143 | accuracy: 100.0%\n",
      "iteration: 1834 | loss: 1.3222683929418166 | accuracy: 100.0%\n",
      "iteration: 1835 | loss: 1.002663801151067 | accuracy: 100.0%\n",
      "iteration: 1836 | loss: 1.0032646074839087 | accuracy: 100.0%\n",
      "iteration: 1837 | loss: 1.0019345836909987 | accuracy: 100.0%\n",
      "iteration: 1838 | loss: 1.0026480695908393 | accuracy: 100.0%\n",
      "iteration: 1839 | loss: 1.0093773606032213 | accuracy: 100.0%\n",
      "iteration: 1840 | loss: 1.0080042290578481 | accuracy: 100.0%\n",
      "iteration: 1841 | loss: 1.004406268810745 | accuracy: 100.0%\n",
      "iteration: 1842 | loss: 1.002816424330759 | accuracy: 100.0%\n",
      "iteration: 1843 | loss: 1.0032664795917776 | accuracy: 100.0%\n",
      "iteration: 1844 | loss: 1.0153673347374308 | accuracy: 100.0%\n",
      "iteration: 1845 | loss: 1.163014422550242 | accuracy: 100.0%\n",
      "iteration: 1846 | loss: 1.0138196661011696 | accuracy: 100.0%\n",
      "iteration: 1847 | loss: 1.0030738787312443 | accuracy: 100.0%\n",
      "iteration: 1848 | loss: 1.004524385787639 | accuracy: 100.0%\n",
      "iteration: 1849 | loss: 1.0077995525239862 | accuracy: 100.0%\n",
      "iteration: 1850 | loss: 1.0043263468438004 | accuracy: 100.0%\n",
      "iteration: 1851 | loss: 1.0404943321190565 | accuracy: 100.0%\n",
      "iteration: 1852 | loss: 1.011599847663819 | accuracy: 100.0%\n",
      "iteration: 1853 | loss: 1.0327338418907481 | accuracy: 100.0%\n",
      "iteration: 1854 | loss: 1.0043695306315406 | accuracy: 100.0%\n",
      "iteration: 1855 | loss: 1.0040098241771829 | accuracy: 100.0%\n",
      "iteration: 1856 | loss: 1.0023430796885737 | accuracy: 100.0%\n",
      "iteration: 1857 | loss: 1.002117946575954 | accuracy: 100.0%\n",
      "iteration: 1858 | loss: 1.004014685451183 | accuracy: 100.0%\n",
      "iteration: 1859 | loss: 1.006953455720819 | accuracy: 100.0%\n",
      "iteration: 1860 | loss: 1.0059913793909667 | accuracy: 100.0%\n",
      "iteration: 1861 | loss: 1.0065761533565816 | accuracy: 100.0%\n",
      "iteration: 1862 | loss: 1.0090251339907252 | accuracy: 100.0%\n",
      "iteration: 1863 | loss: 1.0177718145020673 | accuracy: 100.0%\n",
      "iteration: 1864 | loss: 1.0065832604434353 | accuracy: 100.0%\n",
      "iteration: 1865 | loss: 1.0907932734882442 | accuracy: 100.0%\n",
      "iteration: 1866 | loss: 1.0032767041616277 | accuracy: 100.0%\n",
      "iteration: 1867 | loss: 1.0123180967137835 | accuracy: 100.0%\n",
      "iteration: 1868 | loss: 1.0036657790947814 | accuracy: 100.0%\n",
      "iteration: 1869 | loss: 1.0103110631583052 | accuracy: 100.0%\n",
      "iteration: 1870 | loss: 1.0183394001667765 | accuracy: 100.0%\n",
      "iteration: 1871 | loss: 1.0081678762048962 | accuracy: 100.0%\n",
      "iteration: 1872 | loss: 1.0034879322675816 | accuracy: 100.0%\n",
      "iteration: 1873 | loss: 1.0034430034838373 | accuracy: 100.0%\n",
      "iteration: 1874 | loss: 1.0039148191517722 | accuracy: 100.0%\n",
      "iteration: 1875 | loss: 1.0036573688246329 | accuracy: 100.0%\n",
      "iteration: 1876 | loss: 1.0126149830711548 | accuracy: 100.0%\n",
      "iteration: 1877 | loss: 1.0037978222215955 | accuracy: 100.0%\n",
      "iteration: 1878 | loss: 1.004127049305404 | accuracy: 100.0%\n",
      "iteration: 1879 | loss: 1.0119729241070177 | accuracy: 100.0%\n",
      "iteration: 1880 | loss: 1.0019960369112308 | accuracy: 100.0%\n",
      "iteration: 1881 | loss: 1.009567406868355 | accuracy: 100.0%\n",
      "iteration: 1882 | loss: 1.009497850174318 | accuracy: 100.0%\n",
      "iteration: 1883 | loss: 1.2510189190778573 | accuracy: 100.0%\n",
      "iteration: 1884 | loss: 1.0034593716053577 | accuracy: 100.0%\n",
      "iteration: 1885 | loss: 1.0906714540340594 | accuracy: 100.0%\n",
      "iteration: 1886 | loss: 1.0990518270353595 | accuracy: 100.0%\n",
      "iteration: 1887 | loss: 1.0229561857480065 | accuracy: 100.0%\n",
      "iteration: 1888 | loss: 1.0034717832164333 | accuracy: 100.0%\n",
      "iteration: 1889 | loss: 1.0035431152163765 | accuracy: 100.0%\n",
      "iteration: 1890 | loss: 1.0094754584579513 | accuracy: 100.0%\n",
      "iteration: 1891 | loss: 1.003194497156933 | accuracy: 100.0%\n",
      "iteration: 1892 | loss: 1.010150594737106 | accuracy: 100.0%\n",
      "iteration: 1893 | loss: 1.0275941402736992 | accuracy: 100.0%\n",
      "iteration: 1894 | loss: 1.028366567509999 | accuracy: 100.0%\n",
      "iteration: 1895 | loss: 1.0027466116813655 | accuracy: 100.0%\n",
      "iteration: 1896 | loss: 1.006959184178829 | accuracy: 100.0%\n",
      "iteration: 1897 | loss: 1.0206850713009705 | accuracy: 100.0%\n",
      "iteration: 1898 | loss: 1.086617705252783 | accuracy: 100.0%\n",
      "iteration: 1899 | loss: 1.0025777959387734 | accuracy: 100.0%\n",
      "iteration: 1900 | loss: 1.0050900469366517 | accuracy: 100.0%\n",
      "iteration: 1901 | loss: 1.0034197025162235 | accuracy: 100.0%\n",
      "iteration: 1902 | loss: 1.008470848007982 | accuracy: 100.0%\n",
      "iteration: 1903 | loss: 1.0049087418292924 | accuracy: 100.0%\n",
      "iteration: 1904 | loss: 1.0027083254362739 | accuracy: 100.0%\n",
      "iteration: 1905 | loss: 1.0021793153198595 | accuracy: 100.0%\n",
      "iteration: 1906 | loss: 1.0024840000217698 | accuracy: 100.0%\n",
      "iteration: 1907 | loss: 1.0076179691266267 | accuracy: 100.0%\n",
      "iteration: 1908 | loss: 1.0140306820595688 | accuracy: 100.0%\n",
      "iteration: 1909 | loss: 1.0099999059088824 | accuracy: 100.0%\n",
      "iteration: 1910 | loss: 1.0163025192455095 | accuracy: 100.0%\n",
      "iteration: 1911 | loss: 1.0022813114763327 | accuracy: 100.0%\n",
      "iteration: 1912 | loss: 1.0032135834247602 | accuracy: 100.0%\n",
      "iteration: 1913 | loss: 1.0019825585403308 | accuracy: 100.0%\n",
      "iteration: 1914 | loss: 1.0085364173856821 | accuracy: 100.0%\n",
      "iteration: 1915 | loss: 1.1780002469378663 | accuracy: 100.0%\n",
      "iteration: 1916 | loss: 1.005776299441656 | accuracy: 100.0%\n",
      "iteration: 1917 | loss: 1.0042255977549437 | accuracy: 100.0%\n",
      "iteration: 1918 | loss: 1.0034343989188212 | accuracy: 100.0%\n",
      "iteration: 1919 | loss: 1.011498241304577 | accuracy: 100.0%\n",
      "iteration: 1920 | loss: 1.0183581659412158 | accuracy: 100.0%\n",
      "iteration: 1921 | loss: 1.0245875474603163 | accuracy: 100.0%\n",
      "iteration: 1922 | loss: 1.343040658994734 | accuracy: 100.0%\n",
      "iteration: 1923 | loss: 1.0088138696150062 | accuracy: 100.0%\n",
      "iteration: 1924 | loss: 1.003800269666779 | accuracy: 100.0%\n",
      "iteration: 1925 | loss: 1.00750463367542 | accuracy: 100.0%\n",
      "iteration: 1926 | loss: 1.1796577001136128 | accuracy: 100.0%\n",
      "iteration: 1927 | loss: 1.0055264991302137 | accuracy: 100.0%\n",
      "iteration: 1928 | loss: 1.0178711319444715 | accuracy: 100.0%\n",
      "iteration: 1929 | loss: 1.0109538720266948 | accuracy: 100.0%\n",
      "iteration: 1930 | loss: 1.005635641828324 | accuracy: 100.0%\n",
      "iteration: 1931 | loss: 1.003289456862255 | accuracy: 100.0%\n",
      "iteration: 1932 | loss: 1.0094961147386028 | accuracy: 100.0%\n",
      "iteration: 1933 | loss: 1.0184542246718862 | accuracy: 100.0%\n",
      "iteration: 1934 | loss: 1.090000670569883 | accuracy: 100.0%\n",
      "iteration: 1935 | loss: 1.0038441498560942 | accuracy: 100.0%\n",
      "iteration: 1936 | loss: 1.0021203248484767 | accuracy: 100.0%\n",
      "iteration: 1937 | loss: 1.0032022393064493 | accuracy: 100.0%\n",
      "iteration: 1938 | loss: 1.0054300422882103 | accuracy: 100.0%\n",
      "iteration: 1939 | loss: 1.0068434712186438 | accuracy: 100.0%\n",
      "iteration: 1940 | loss: 1.0113431809538629 | accuracy: 100.0%\n",
      "iteration: 1941 | loss: 1.0102841419226263 | accuracy: 100.0%\n",
      "iteration: 1942 | loss: 1.0023454695829337 | accuracy: 100.0%\n",
      "iteration: 1943 | loss: 1.0128596765670246 | accuracy: 100.0%\n",
      "iteration: 1944 | loss: 1.0477570940683278 | accuracy: 100.0%\n",
      "iteration: 1945 | loss: 1.0020564760081057 | accuracy: 100.0%\n",
      "iteration: 1946 | loss: 1.0079111750892729 | accuracy: 100.0%\n",
      "iteration: 1947 | loss: 1.0045244897069543 | accuracy: 100.0%\n",
      "iteration: 1948 | loss: 1.012475414955016 | accuracy: 100.0%\n",
      "iteration: 1949 | loss: 1.0043604229853085 | accuracy: 100.0%\n",
      "iteration: 1950 | loss: 1.2078603909578027 | accuracy: 100.0%\n",
      "iteration: 1951 | loss: 1.004955921421568 | accuracy: 100.0%\n",
      "iteration: 1952 | loss: 1.0078891251938487 | accuracy: 100.0%\n",
      "iteration: 1953 | loss: 1.0024421900818905 | accuracy: 100.0%\n",
      "iteration: 1954 | loss: 1.0261590601369635 | accuracy: 100.0%\n",
      "iteration: 1955 | loss: 1.0038756413161765 | accuracy: 100.0%\n",
      "iteration: 1956 | loss: 1.002470757427675 | accuracy: 100.0%\n",
      "iteration: 1957 | loss: 1.0024890097037098 | accuracy: 100.0%\n",
      "iteration: 1958 | loss: 1.0036889043899868 | accuracy: 100.0%\n",
      "iteration: 1959 | loss: 1.0060916521226588 | accuracy: 100.0%\n",
      "iteration: 1960 | loss: 1.005381295083255 | accuracy: 100.0%\n",
      "iteration: 1961 | loss: 1.0027430309771237 | accuracy: 100.0%\n",
      "iteration: 1962 | loss: 1.003693995332329 | accuracy: 100.0%\n",
      "iteration: 1963 | loss: 1.0874037171456565 | accuracy: 100.0%\n",
      "iteration: 1964 | loss: 1.0052811090651665 | accuracy: 100.0%\n",
      "iteration: 1965 | loss: 1.0057713284762777 | accuracy: 100.0%\n",
      "iteration: 1966 | loss: 1.0100942282858674 | accuracy: 100.0%\n",
      "iteration: 1967 | loss: 1.0051703544797168 | accuracy: 100.0%\n",
      "iteration: 1968 | loss: 1.0113314236226223 | accuracy: 100.0%\n",
      "iteration: 1969 | loss: 1.0132662330964681 | accuracy: 100.0%\n",
      "iteration: 1970 | loss: 1.0217346395059608 | accuracy: 100.0%\n",
      "iteration: 1971 | loss: 1.0500628273067616 | accuracy: 100.0%\n",
      "iteration: 1972 | loss: 1.0054498649104935 | accuracy: 100.0%\n",
      "iteration: 1973 | loss: 1.0031842805240725 | accuracy: 100.0%\n",
      "iteration: 1974 | loss: 1.0681050748907048 | accuracy: 100.0%\n",
      "iteration: 1975 | loss: 1.0932679449645137 | accuracy: 100.0%\n",
      "iteration: 1976 | loss: 1.0062783188462512 | accuracy: 100.0%\n",
      "iteration: 1977 | loss: 1.0040434894677892 | accuracy: 100.0%\n",
      "iteration: 1978 | loss: 1.0174838338358438 | accuracy: 100.0%\n",
      "iteration: 1979 | loss: 1.006624104629461 | accuracy: 100.0%\n",
      "iteration: 1980 | loss: 1.012645818111803 | accuracy: 100.0%\n",
      "iteration: 1981 | loss: 1.0072627877916633 | accuracy: 100.0%\n",
      "iteration: 1982 | loss: 1.0041906926427622 | accuracy: 100.0%\n",
      "iteration: 1983 | loss: 1.0223215317847876 | accuracy: 100.0%\n",
      "iteration: 1984 | loss: 1.0082012131049558 | accuracy: 100.0%\n",
      "iteration: 1985 | loss: 1.0083385600782935 | accuracy: 100.0%\n",
      "iteration: 1986 | loss: 1.0760967926302398 | accuracy: 100.0%\n",
      "iteration: 1987 | loss: 1.003211253483611 | accuracy: 100.0%\n",
      "iteration: 1988 | loss: 1.0040918825877776 | accuracy: 100.0%\n",
      "iteration: 1989 | loss: 1.0027036897588726 | accuracy: 100.0%\n",
      "iteration: 1990 | loss: 1.0879848207092166 | accuracy: 100.0%\n",
      "iteration: 1991 | loss: 1.0023151036329243 | accuracy: 100.0%\n",
      "iteration: 1992 | loss: 1.0047856569774567 | accuracy: 100.0%\n",
      "iteration: 1993 | loss: 1.0739189959599729 | accuracy: 100.0%\n",
      "iteration: 1994 | loss: 1.0055326737018597 | accuracy: 100.0%\n",
      "iteration: 1995 | loss: 1.0181552042058981 | accuracy: 100.0%\n",
      "iteration: 1996 | loss: 1.0058458923117803 | accuracy: 100.0%\n",
      "iteration: 1997 | loss: 1.0032549129151833 | accuracy: 100.0%\n",
      "iteration: 1998 | loss: 1.0061806091085623 | accuracy: 100.0%\n",
      "iteration: 1999 | loss: 1.0021465618161218 | accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Batch Iterations\n",
    "low_batch = True\n",
    "if low_batch:\n",
    "    niter = 2000\n",
    "    batch_size = 2\n",
    "else:\n",
    "    niter = 2\n",
    "    batch_size = None\n",
    "lr = 0.05\n",
    "verbose = False\n",
    "\n",
    "for i in range(1, niter):\n",
    "    ## Forward Path\n",
    "    ### The larger the l2_coef, the lower the weights, the less complex model\n",
    "    total_loss, acc = loss(batch_size, l2_coef = 1e-5, margin=2)\n",
    "\n",
    "    model.zero_grad()\n",
    "    total_grad = 0\n",
    "    ## Backprop or Backward Path\n",
    "    total_loss.backprop()\n",
    "\n",
    "    ## Update `p_new = p_old - lr*grad`\n",
    "    ### Learning rate decay\n",
    "    # lr = 0.1 - i/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= lr*p.grad\n",
    "        # total_grad += p.grad\n",
    "    \n",
    "    print(f'iteration: {i} | loss: {total_loss.data} | accuracy: {acc*100}%')\n",
    "    if verbose:\n",
    "        print(f'total_grad: {total_grad} | w0: {model.layers[0].neurons[0].w[0]}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.6404521912462298, 2.1095478087537702)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjYAAAGhCAYAAAByPf5TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABx8ElEQVR4nO3deXRj9YEn+u/vXq1eZNmW992ugqqC2kKKvYHQMCGT4iRNXs/JdCYnnaTJeaGnoTMhgeT0yzvJ0A2ZziRD5gxk0qnOIemZzmRyoPo1B0gg3V0QCDAQQtgKXN7LLu+WJS/a7v29P65lW7aWK1vLlfz9nFOnbOlK+lk/Sfer3yqklBJEREREZUApdgGIiIiIcoXBhoiIiMoGgw0RERGVDQYbIiIiKhsMNkRERFQ2GGyIiIiobDDYEBERUdlgsCEiIqKyYSt2AQpNxjRE5haKXQwiIiIyyVFfC2FTTR2754JNZG4Bb97xdQghoKoKNE0HF18uLtaFdbAurIX1YR2si+K69KH/F84mn6lj2RVFREREZYPBhoiIiMoGgw0RERFZmk2NmT6WwYaIiIgsq7ttNKvj8x5snnzySdxxxx24/vrrcezYMdx66634n//zf0LX9Yy3feyxx3DLLbfg8OHDOHnyJJ588sl8F5eIiIgsIh5qhM38XKe8z4r64Q9/iNbWVnz5y19GfX09XnrpJfzlX/4lxsbGcM8996S83VNPPYV7770Xn/vc53DNNdfgmWeewRe+8AVUV1fj2muvzXexiYiIqIjiocbR1ZbV7YTM87y1+fl51NXVJVx2//334+///u/xyiuvwOFwJL3dhz70IVx00UV48MEH1y/77Gc/i2AwiJ/+9Kc7Lk94apbTvS2GdWEdrAtrYX1YB+uisDaHGvfxKqhX3QPhrjd127x3RW0NNQBw8OBBhMNh+P3+pLcZGxvD4OAgTp48mXD5yZMn8bvf/Q7z8/P5KCoREREV2dZQAwBQLNQVlcyrr74Kr9eL+vrk6WtwcBAA0Nvbm3B5X18fpJQYHBxMGpjMEkKk/Z2Kh3VhHawLa2F9WAfrIn+62kYAAM6udriOVQIApK89q/soeLB544038Oijj+JP//RPoarJl0deXFwEAHg8noTLa2pqEq7fKVVVkv5MxcW6sA7WhbWwPqyDdZE/Hc3DAAQqejtgP1IBAFAaO40r7XbT91PQYDMzM4M777wThw8fxu23357x+K2pON6vudu0rGnGjKx4fykVH+vCOlgX1sL6sA7WRf50tY1AwmipsR12Q0oJ6WuHrmlQ9/Uhm7N+wYJNMBjE7bffDpfLhYcffhj2NOlrc8uMz7exN0QgEACwvSUnW1LKhHDEgWDFxbqwDtaFtbA+rIN1kT/dbaOANMbUxLuf9Pp2QAK2/X3GQcJ8S1lB2tTC4TA+//nPY3Z2Fj/4wQ9QW1ub9vj42Jr4WJu4gYEBCCG2jb0hIiKi0pNsoLBeb4ypiYcazW5u88u4vAebWCyGu+66C2fPnsUPfvADtLVlno/e0dGB3t5ePPHEEwmXP/744zhy5MiuBg4TERFR8WUTaqK6+S0V8t4V9Y1vfAP//M//jC996UsIhUL47W9/u37dvn37UFVVha9+9as4ffo03n777fXr7rzzTnzhC19AZ2cnrr76avzyl7/E888/jx/84Af5LjIRERHlUTah5tzSAPo8l5i+77wHm1/96lcAgL/+67/edt2PfvQjXHHFFdB1HZqmJVz3oQ99CKFQCN/73vdw6tQpdHV14Tvf+Q5XHSYiIiph2YYag/nhw3lfedhquPKw9bAurIN1YS2sD+tgXeTGTkJNLObGxXX/Cg610tRjcEI+ERER5d1OQw0ALMfMh0kGGyIiIsqr3YSa06MhiCziCoMNERER5c1uQ40d2a1dx2BDREREeZGrUBNaNf+YDDZERESUc7kKNS88k93jFmV3byIiIipf3W2jcHQZC/LmItSoWewRyRYbIiIiyplchxqfozqrx2ewISIiopwodqgBGGyIiIgoB6wQagAGGyIiItolq4QagMGGiIiIdsFKoQZgsCEiIqIdslqoARhsiIiIaAesGGoArmNDREREWdppqDk9GgKAvIUagC02RERElAUrhxqAwYaIiIhMsnqoARhsiIiIyIRSCDUAgw0RERFlUCqhBmCwISIiojRKKdQADDZERESUQqmFGoDBhoiIiJIoxVADMNgQERHRFqUaagAGGyIiItqklEMNwGBDREREa0o91AAMNkRERITyCDUAgw0REdGeVy6hBmCwISIi2tPKKdQADDZERER7VrmFGoDBhoiIaE8qx1ADALZiF4CIiIgKK9tQE4u5AcDyoQZgiw0REdGeUs6hBmCLDRER0Z7Q3TYKAGUdagC22BAREZW9vRJqAAYbIiKisraXQg3AYENERFS29lqoARhsiIiIytJeDDVAAQYPj4yM4NSpU3j99dfR39+P3t5ePP744xlv98lPfhIvv/zytsufeOIJ9PX15aOoREREZWGvhhqgAMGmv78fZ86cwdGjR6HrOqSUpm/7vve9D/fcc0/CZe3t7bkuIhERUdnYy6EGKECwufHGG3HTTTcBAO699168+eabpm/r8Xhw7NixPJWMiIiovOz1UAMUYIyNonAYDxERUb4x1BgsvUDfyy+/jGPHjkHTNBw9ehR33XUXTpw4sev7FUKk/Z2Kh3VhHawLa2F9WIcV66KrbQQA4OwyQozrWCUAQPraIQCo+4xQozt8EAD6gwPQNDeEAB4bWQs1Yi3UPG3cZ4Oz9EINYOFgc+LECXzkIx9Bd3c3pqencerUKXz605/Gj3/8Yxw/fnxX962qStKfqbhYF9bBurAW1od1WLEuOpqHAQhU9HYAAOxHKgAASmOncUBvDwBAuBuhADjr74cQlbDZgJ8NrkIIAZfqBQA893MdQgBNbk9h/4iMzIdJywabO++8M+H3G264ASdPnsRDDz2Ev/mbv9nVfWuaDsB4gcZ/puJiXVgH68JaWB/WYcW66GobgYTRUhPTdLiOVUJKCelrh65pRkuNlNAdPiCmrbfUAHpCS01M0xJaaqz2dwLmJx5ZNthsVVFRgeuvvx4///nPd31fUsqEpsRsZmpR7rEurIN1YS2sD+uwYl10t40C0hhTI6VMHFMjt4ypkWnG1MjEMTUW+fN2zHptamlY5cVERERUTBwonFrJBJuVlRWcOXMGhw8fLnZRiIiIioahJr28d0Wtrq7izJkzAIDx8XEsLS3hqaeeAgBcfvnlqKurw1e/+lWcPn0ab7/9NgDglVdewalTp3DzzTejtbUV09PT+OEPf4iZmRk8+OCD+S4yERGRJTHUZJb3YDM3N4e77ror4bL47z/60Y9wxRVXQNd1aJq2fn1DQwMikQi+/e1vw+/3w+124/jx4/j617+OI0eO5LvIRERElsNQY46Qe2zgSnhqFm/e8XUIIdZHuO+xp8ByWBfWwbqwFtaHdRS7LvZ6qPnmZ06gocZt6tiSGWNDRES0F+31UJMtBhsiIiKLYqjJHoMNERGRBTHU7AyDDRERkcUw1Owcgw0REZGFMNTsDoMNERGRRTDU7B6DDRERkQUw1ORGyWyCSUREVK6yCTXnlgYAgKEmBbbYEBERFRFDTW4x2BARERUJQ03uMdgQEREVAUNNfjDYEBERFRhDTf4w2BARERUQQ01+MdgQEREVCENN/jHYEBERFQBDTWEw2BAREeUZQ03hcIE+ohKhazrCSxFAAs4qBxQbv5cQlQKGmsJisCGyOF3TMf3uLOYG56FFdQCAYlNQ1+VF08EGqHa1yCUkolQYagqPwYbIwnRdYuiFUSzPriReHtMxOziPpdkV9F3XDZWtN0SWw1BTHPw0JLKw+aGFbaFmnQRCiyHM9M8WtlBElBFDTfEw2BBZ2OzgfMZj5gYXIHVZgNIQkRkMNcXFYENkUbouEVmKZDxOi2iIhWMFKBERZcJQU3wcY0NkUUJkcaySxcG7sOJfxfyQH6FACIpNQU1LNbwdNRzATASGGqtgsCGyKCEEKn0VWJ5bAdL0NDmrHVAdmYOFrunwnw9gYcyPWFiDw21DbVctalqqMwYjKSUmfjeJucEFQGC9PEvTy5h8ZwY913SiwuvO4q8jKi8MNdbBYENkYQ3761MPHl4/xgeRoXknshrF4K9GErq2woEwglPLcNe60HN1F2xpwtFM/5wRaoBtIUuLahj61QguvnkfbE5+pNDeYybUaHYfADDUFADH2BBZmKe5Gk0HG4xfNmeXtZ/re2pR21mT9j6klBh+YRSR5eTjdVb9IYy9cj7l7XVNx8x7c2keANCiOhZG/WnLQVSOGGqsh1+viCyu6UADKusrMDswj6WZZQBARZ0bvr46VDdVZWytWZpZRigQTn2ABIJTxjHuGte2q1fmV6FFtYzl9J8PoGG/L+NxROWCocaaGGyISkBVQyWqGip3dNvAhWDCuJikBBC4EEgabOKrHWdi9jiicsBQY13siiIqc3rMXODQY8mTj7PKnvnGwti/imgvYKixNgYbojLnrHKmb60BjI01q5MHE5fHBbd3e0vO1tvX99TurIBEJaSrbQQAQ42VMdgQlbnarprEgccpRJaj0GLJx9K0HmlOu65OVWMlqpurdlhCotLQ0TwMgKHG6hhsiMqc3WVH88HGjMdNnZ3BuX8ZQiyyPdxU1leg99ru7d1NAqjr8aL7yo6Mg5iJSlm8pcbZZYQYhhrr4uBhoj2g8WIfVIeCybdm0s5wCgXDOP/bCXRc1rbtukpfBS66qQ8r86sIB8MQqoLqxkquXUNlLz6mpqK3AzFNZ6ixOLbYEO0R9T11qGn3pD9IAguj/pR7TwkhUFlfgbruWtR21DDUUNmLh5p4S43rmDE7kaHGuhhsiPaQ5dnljMdIaaxdQ7TXbZ39ZD9SAYChxuoYbIj2kkyzo4gIwPZQE2+pkT6GGqvLe7AZGRnB1772NXzkIx/BoUOHcPLkSdO3feyxx3DLLbfg8OHDOHnyJJ588sk8lpSo/FX6KjLPkBLIPL07hVAwjMWJIILTS6bXzyGymlTr1CiNnQAYaqwu7x3k/f39OHPmDI4ePQpd1yGlua+MTz31FO6991587nOfwzXXXINnnnkGX/jCF1BdXY1rr702z6UmKk/1PXWYH/anPkAA3jYP7G676fcqAKwuhjD++iRW5jY27FRsCnx9dWg60JBx93Aiq0gVauItNeo+hhqry3uwufHGG3HTTTcBAO699168+eabpm734IMP4pZbbsEXv/hFAMCVV16JoaEhfPe732WwIUuQukRgMojltZN5ZV0FPC3VEIqAFtWwOB5AZDUKm0NFTasRFrKlazr85wPwn1+EFtXhrLSjrqcWlfUVO5pe7fa60HxJIybfmt5+pQAcbjs6jrdmdZ+riyEMnBmCricGIT2mY/rdWYSXIug80cbp4GR56VYUFgDQ2wPdXg9Ihhory3uwUZTse7vGxsYwODiI//Af/kPC5SdPnsRXvvIVzM/Po66uLldFJMraysIqhl8cQywUW+/amT03D5tThafVg4URP6Qu1/domvjdFOp6vGg90gLFZOtFeCmMwV+NILq6MUNp1b8K//kAalqr0XGi3fR9bdZ4kQ/OKgdm+ufWBwkrNgV13V40XdwAu8sOTTPfjTTx+qQRalI08CyOB7DU5UV1ExfwI+vKtE2Cuq8Pwt0IxDSGGouz5FzNwcFBAEBvb2/C5X19fZBSYnBwkMGGiia8FMHgcyPQ9bWT/6YTeiysYX5oYeOCTdfND/mhxyQ63799jZitdE3H4K9GEQ1tmXa9dn+LE0HY3phE29GWHf0NNa0e1LR6EItokJoO1WmDooisW1XCwfB6i1VKApgbWmCwIcsys/eT7vBBAdAfZKixOksGm8XFRQCAx5O45kZNTU3C9Tu19cObTeTWUQp1MdM/Z4SaHcww8o8tovEiX9JdtDcLTAQRXY2mPWZ+2I/mg427WkvGnua2ZuoivJy+jAAACYQC4V3VrdQldE2HYlNK4jWSD3v17863rSsKb579JGC01OgOH4QQOOvvBwBomhtCAI+NrIUasRZqnjbus8HJUFNMlgw2cVvfyPHBjLt9g6uqkvRnKq5SqAspJRbG/DufNi2McFNVV5H2sMWJQOay6BJLM8uo78r95pNm68JmV83dn03ZUf2GgmFMvTuD+bWuPaEI1HV50XSgAa4qZ9b3V6pK4b1Rioy9nwQqejsAbKxTE5/9hN4eCHcjFGA91AhRCZsN+NngKoQQcKleAMBzP9chBNDkzrAIJu2Q+fO+JYPN5pYZn8+3fnkgYHzYb23JyVZ8/ICqKlmNJaD8KZW60KIapLaLxWAkEF6OZPxbtai55yIW0Uw9b1KXWLwQwNzgAsLBMBS7Am9bDep7arcNas6mLty1Lig2JePU7prW6qzrd3l+BYPPDSeM35G6xNzwAhZGF9F3XTcqat1Z3WcpKpX3RqnpahuBhNFSE9N0uI5VQkoJ6WuHrmlGS429Hohp691PQlRC0yQeHTbGptmFBzFNS2ipYV3li/nPXUsGm/jYmsHBQfT19a1fPjAwACHEtrE32ZJSJrT6ZDOtlXKvlOpCqAJCEcbA4B3dAWBzqBn/TkelA5hdzvhedlRmnpatazqGfz2GpZlNqw6HgKl3ZzDTP4eeqzuN9W2QfV0IRcDXV4fpd2dTH6MK1HZ7s6pbqUsM/3oMerIQKQFd1zH861Ec+OD+sp5KXkrvjVLS3TYKSGNMjZQycUyN3LROzabZT5rmhs2GjVADDyATx9SwiqzBku2bHR0d6O3txRNPPJFw+eOPP44jR45w4DAVjRAC3o6abFpFE0kYt8+grtubMdTY3TZUNVRmvK+JN6YSQ82msuiajqFfj6bcG8qMpgMNG3tQbXleFFWg5+pO2F3ZTXVfvBBMXyYJREMxBCaXsiwt7XVmBgqnWqfmZ4ObQg04UNiq8t5is7q6ijNnzgAAxsfHsbS0hKeeegoAcPnll6Ourg5f/epXcfr0abz99tvrt7vzzjvxhS98AZ2dnbj66qvxy1/+Es8//zx+8IMf5LvIRGk1XlSPxfOLyVsT0hFAZX0FKuoyd59U1LpR2+XFwog/5TFtR1syjjeLRbS09wEY680sjC6iYX990uujq1HMDs5jfsQPLaxBdaio6/Kivq8ODrcdQhHofH8blrq8mBtaQGgxDMUmUNPqQV13Leyu7D9mVuZW1qfKpySA5bll1LTypELm7CbUPDYSghDCGCgsGWqsLO/BZm5uDnfddVfCZfHff/SjH+GKK66AruvQNC3hmA996EMIhUL43ve+h1OnTqGrqwvf+c53uDgfFZ2zyonea7sw/NL5hHVsIAHVocJd68bS1NLGiXnt/8r6CnRf0WF68Hv78RbYXTbMnJtLGNdjr7Cj7WgzPM2ZP1CXZpZNdZsFLgSTBptQIIyB54ahRbX1kKFFNMycm8P88AJ6f68b7hoXhBCobqxCdWNupnSzRZ9ybTehJj6l26V6jTE1DDWWJuQe67gNT83izTu+DiHE+qC8PfYUWE6p1oXUJQIXjJWHJYDKOjc8rR4oikAoEMbsuTksz60gGopBCAG314X63lpjdeIsZvZpMR1LU0vQYjoclfasVh1eGPVj7NWJjMe5vS7s/0BvQl3ouo53nz6HyEo0edIQgN1lx4EP7sv5VGT/+UWM/p/xjMd1Xd6OmrbynYVSqu8Nq8lFqLELD2yqimefMr6EM9QU1jc/cwINNeYmC1hy8DBRKRCKQE2bJ+mJNbwcwcKo38gDa+ejpdllLM0sw9Naja4T7aYHvao2Zccnb5fHxGaWAnDVbJ86vTS9jEi6dWqk0U0VnFyCpyW3H/KeVg9UxyS0iJbyGJvTtqvHTbclBpWPnISatTE1z/1cR4OTg4StjsGGKMfCyxGMvnx++4ff2u+BiSCmzs6g+VBj3svi9rrg8roQ8odSHySNzTG3WppZNjXOZWlmOefBRlEEuq9ox+Dzo0ZLxeYyCCNUdl1hPhxutTK/sq0rcfbcPGwuG7quaEdlhnWGqDTkMtS88DTQXOHhdO4SYMlZUUSlbG5wIWO3wezAPPQCfUC2H2uBUEXKmVz1fXVJ14Mx2/WRry6SSl8l9n+gB942z3rZhQC87TXYf0MvKut3Fj7CQWMPrlh8uwqJ9eAUC8Uw+KsRhILh3f8BVFTpQo1tf192oeYZriZcSthiQ5REKBjG3MA8Fs4vQo/pcLiNXbXre2qhZlhtNzAZzDj6VY/pWFlYRZUv83Tt3aqodWPfdT2YeGMSy7Mb+zrZnCoaLvLB15d8+YSKWnfmUbwSeV0kz+VxofNEO9rfp0OL6lDtCpQtq/DGVyQ2a/q92W07kW+9v5n3ZtFxWeY9vciaMoUaAFmFGo6nKS0MNkRbBKeXMPzrsYQukMhKFJNvTWNhxI++67q37c8kpYTUJIQqIE22xOxqBeMsub0u9P1eN8LLEUSWI1BUBRW17rSBwMw4F8W+8/E/2VDUxEATWY1i9twcFkb80KLGHlK1nTXw7auHs9KR8n6kLuE/H0gf2CTgPx9A2/HWHe2eTsXFUEMMNkSbxCIaRl4cSzlFOrwcwdirE+i52thLJhQMY6Z/Fv6xAKQuodgUqA4189gUAM7qwu915Kx0pD3xb6YoAp0n2jD8wmjCIGgA611Dne9v29aCkm+hQBgDzw5Di21MQddjOuaGjK0Weq/tStmKpMV0U9PfpS6hRzUou9hglAqPoYYABhvaI6KhGOaHF7A0vQwpJSrq3KjvqYVzy0aKC6P+9AvvSSA4tYTwUgTL8ys4/5uJhBO+HtMz7psEAVQ3VcFRkd1qvMVQ3ViFvut7MPXODIJTG6v8VjVUoulgQ8EH2UopMfziWEKo2bjSeP6Hfz2GA7fsT9raotoUU1tiCEVAMbnBJ1kDQw3FMdhQ2Vu8EDRmKW06ma0srGL23DxaLm1KWJhuadrcEv1DL4yknwqdigBUm4rWw83Z37ZIKmrd6Lm6E7FwDLGwBptT3dYVVyhLM8uILEfSHhMLxxCYCMDbvn3rCqEIeNs9WBhbTN2iJoCaNg+7oUoIQw1txllRVNZWF0MYeSlJ19LarxfenMLiRGDjYpMTlcyGGsWW+BbzNFdj3w09cFaZ6w6yEpvTBpfHmbNQo8d0LM0sIzi1hGjI3PO5PLuSeZ8ugYRB0ls1XORLO7ZIKAKNF/lMlYeKj6GGtmKLDZW12XNzGY+ZfncWNa3GB1tFrRtLJnbVNkUAvr46VDdVQeoSzioH7O7idz9FVqKYHzH2dBKKQHVTFbztnoKNldE1HZNvz2B+aD6h28/TUo3WI81pu+jMT0FPfZ2r2onea7ow8tIYYmEtYUsMm1NF1xUdcHk2uijDS2HMD/sRXopAsSnwtFSjhgv5WQJDDSXDYENlbXE8wwwYAKv+EKKrUdjddtT1eDH93mzuCrC28aVVzJybw4U3phIGNy+OBzD9zgVceasL3iYVAgISMjfhbgtdl/iXnxmrGm8VmAxiZX4V+z7QA0eKAGh+Cnr6FZcr6ytw8JaLsHghiJX4lhj1FQmBRUqJybenMfPeXMK+X/6xRdgr7Oi9pqskW97KBUMNpcJgQ2XN7A7cWkyHHYCjwoHWo82YeH1y9w8uYakVbP3nF41QA2wLB9GQxK//vzA++OVeVNc6EMvT3kTnnhrG0nSKxe8kEIvEMPnWNDrf34ZQIIzoahSqXYW71tho09NcDZvLtrG4XhKKTUk6vmYroQh42zzGAoBJzJ6bN0LNWtk2/x9djWLwV8O4+KZ927obKf8YaigdBhsqa44Ku7GJYxpCEbC7Nt4Kvt462N12TJ+dwWp8KwIBVDdXIXjB3OBiCMDutqOqMf8L8JkhpcTUOzNprgdiYR2Dv17A0X/dlLdyjLwn0k+Fl0aLSGgxhFBgIwDZ3TY0HWxEXZcXXSfaMfj8SNKtFgCg80RbVmEjHAxjbtiPcDAEoSrwNFfD01KF6XdTP1/GPlkxLIwtor6n1vRj0e4x1FAmDDZU1up7a3HhzenUBwjA2+7ZtppwTUs1PM1VCC2GoEU0uDwu2Fw2vPdPgwgtptl3aY2iCHRd3p7zXa93KhwMI7yUfjaRlMDom0G8/94e2PK0TUJgKmyqi2tzqAGMEHH+NxOIhWNovMiHfTf0YPrdWWPg99r9VTdVoelAg+mVkONhb/rd2YSwFZgIQrEr0KOZR5L7zzPYFBJDDZnBYENlra6nDvMjiwgvJTmhrk29bjrQkHCxlBL+sUXM9M+tn2BVu4r63lo07KvD2KsTaR+zuqkKLYeb4CrCAnypaBFz070iyxJKYydimpbzHYyVufOwuZS0KxlnMvnWNLztNXDXuNB1eTu0qAYtokF1qBm3uthqbmjBCDXAtteGmVADGAs6UmEw1JBZDDZU1lSbgr7f68L4by9gcSKYcF1FXQU63tcCx5aVeC+8Ob1tNpUW1TD93ixc1U40HWwwunW2dKkIAXS8v83U+I5Cs1eYeKsLoLI1f2OC9Pp2tF89jMGn5yB3kQfmRxbQfNDYGX11MYTZc/NYml6ClIC7xoX6vjp42z1pW8ukLjdCzU4JlMQii+WAoYaywWBDZc/mtKHrig5EVqPG+iZSwu11J0zpjQtOL6WeIi6NLRQqQzFcdFMf5ocWsDy/AiEEqhorUd9da4np3Mk4Khyob1MwN6GnHd+y///qzms5Lv70YQz+4l92dR/htVa0mf45XHgzcYbXysIqVl4ZR2AigM4T7SmnZK8srKYdgGyKBOq72Q2Vbww1lC0GGyo70VAM8yMLCE4uQdd0uL3G9gkVtW44OtK3pswNLmQc3Do/6kfzJY1oPVI6qwd3t42i5monnn90FbqObX+fUAVqLq7H/juvAyodUPM0xqYWA7j6vsvwwv/zKiA3FkQUIv3aMxsFBYSqYHl22Qg1QNK6WpwIYubcXMqF9rToLruQ1qbxVzdX7e5+KC2GGtoJBhsqK8HpJQy/OJawc3YoEMbCiB8N++vRfElj2i6K5bmVjINbpSaNlhsLTeVOJ35yaLq8E7/nW8Erj05heSpi7Jm0NrOo9fe7cNUPPwHFWwlhU6HFcj/GBgBs+4FuAHWHvOj/0RsYf3ER2oqG2k4XGhsjeO2Z9AOcIQFPcxVmBuYzbjQ6e24ODfvqk7baOCp2t/6Mt82D9uOtlhkcXo6ShRq9vh0AGGooLQYbKhuR5QiGf516+4SZ/jk4Ku2o76nb9WOVyuls68mh82ON6PiDBkwMOOF/LwBbWyNaf78bVYf2F6Q8mt0H237AgwFc9v9chRNz5wEAq68Z0+gnzp3D9EiKUCUAu9MGT6sH51+7kDGAxsIawkuRpF2OLo8Tbq9rYzq/CVWNlfC0VMPTXM2xNXnGUEO7wZWlqGzMDs5nXFRu+t3ZtMdUNVZmTC2KTYHLk35lWytI1YwvGzrQcmUjDv/HD+Lg/328YKEmzgg3xskpfrKKl+2KT/fA40v+saQ6VPRc0wUl3tJkQrrjWg83ZZVQPS3V8PXWMdTkGUMN7RZbbKhkxcIxzA/7EZgMQupG91Cmb/HR1RhCi2G4vcmDia+3DovnA0mvi6vvqbX8arPpxiYAiSeI+MkhTgjzwWEn9lX1rbfcxPoHoNe3Q5k7b5TxtSXc+B/6MPzLUYy8FUXQr0C1q6jt9KK2ywubw5jS7a5xYWV+Ne3jKKqAszJ1l1OlrxK913Rh9P+cN/aMSkcYaxtRfjHUUC4w2FBJSjaWxixdS71GSWV9BZoPNWLy7emkYzgq6t1oOtiQ9LZWsZNQEz85CAHYbAo0Tc/PGBvbKs4tDaQNN6uvLaHvg93oODAOABge79x2P76+OozOj6d+IAHUdmcOoFUNlThwy36898sBRJZSr1Bd1+W17Iy3csFQQ7li7a+dREmsj6XZQagBAEdl+hNU48U+9FzdiSrfxnYIjko7Wg83ofearoLtgr0Tuwk1hRB/rPhjp+uWiv8N8b9ps5o2D7ztyfd4ggCcVQ40HzAXQBVFwb7rejZa8eLdU2LjsVqPtpi6L9oZhhrKJbbYUElZvBDE+G8vbB8gbIYwVgW2uzJ/865uqkJ1UxWklJDS2CLB6nYTauInByC/XVEf7XQhFnObbrlxdLUhMjKO7rbRhJYbIQQ63t8Gt9eN2XNziK6tSaOoAnXdtWg60ADVYX4lYpvThn039GBpehn+84uIRTQ43HbUdnlNb9FAO8NQQ7nGYEMlY/rdWaOLaCcEoKgKWi7NboNHIQRKYUZvLkKNHR5AADZVRUzTTO3pFFnQEFvSYatU4KhLHySiCOD0aCin4aZhfz18++oQWY5A6kbL2k5b1IQQ64GWCoOhhvKBwYZKwsrC6s5DDYCKWjfaj7dYav+mXMlZqMnC8lAU0/+8gtD4xqBbV4uKhg9UoKoveYuYHZ6chxvACCTOqvKr13KXq1DzwjPG/THUUJx1BwsQbTIbX5AtC23HW9B5og0X/X4f9l3fUxJTtLNVjFATPBvB6P8IIjSROJMoNKlh7H8GEXgrnOKWG48Vf+xUY25s+/uyGnNDpYWhhvKJLTZUEsysCLxubSxNue/jk8tQEz9BAIAQaWZEaRI1ry5DyCQ501jEGOdPL8N/3gGo25Po1TeZa7lRo7Ow7e9DDDDdckOlgaGG8o0tNlR2FFVByyXZjaUpNfkINT5HNRqc1Whye9DgrIbPsf1fXUCBosmUjWcCgNCBugWR9PbxxzLTchP/O9hyUz4YaqgQGGyoJFT5Kkx1Rbm9LvRd1510Gf1yka9QY8pSJHM9CADLyfd8YrjZuxhqqFAYbKgk1PfVZeyK8rRUY/8HeuGuKb+xNHFFDTUATE8RS3Pc5sfbOr5n65o68RMdsPE3xsWfA7I+hhoqJI6xoZJQ4XWj5dImXHhzKumKwC6PEx2XtRalbIVS9FADQNS5IcfSbzkBaRxXLqSUCE4tITi1DKnrcHlc8LZ7EAtriEVisLtsnJWVBkMNFRqDDZWMhv31cHmcmO6fxfLMCgDA5lRR31sH3756qBbfv2k3rBBqAAA1TqDSDiyn3n4AbhtQa7SaxRf6E6WwGFAS4aUIhn49isjmLjgJTPxuMuE4t9eFlkubUNVQuf1O9jCGGioGBhsqKfEF1HRNh9QlFJtSsidNsywTamAEFOXSRui/nQSSbRzpUCEuaYCcWYE+HgACa2NtPE4o7dWAr6Jk6isW0TDw3DBiYWNV43Rdoav+EAZ/NYKuKzpQ08qTL8BQQ8VTvl9xqawpqrHrc6mcJHfKSqEmTrhsUC5rgej1Gq0zqgDcNogeL8RlzZDnA5DvzG6EGgAIhKG/PQu9fz6vO4fn0sLIAmKhmPllBgCc/8142k1W9wqGGiomttgQWZSZUJPNcvNA7k4Qwq5CdNQAHTUJl+uTS8DkcuobXliCrCmN8SjzI4tZ30aL6licCKJ2y/OiRTVEV6MQigJHpb2sAzlDDRVbQYLN0NAQ7rvvPrz66qtwu9348Ic/jLvvvhsuV/rZK5/85Cfx8ssvb7v8iSeeQF9fX76KS1R0Vg416cjzGQYWrx9j/ZPVehdUNgQQWgytB75oKIapd6axMLq4vnGro9KBxovqUdvlLbuAw1BDVpD3YBMIBPCpT30Kra2t+O53v4v5+Xncf//98Pv9+Na3vpXx9u973/twzz33JFzW3t6e4mii0leyoSampx9UHLcUhR6RUBzWPqnbXTZokSTjiDIQazvBR1ej6P+XISMgberOiixHcP61CwgFw2g93Jyr4hYdQw1ZRd6DzU9+8hMEAgGcPn0adXV1AABVVXH33Xfj85//fMaWF4/Hg2PHjuW7mESWUKqhBgBS78Owq0OLpq67dtvsp4wk1ncHn/jd5LZQs9nsuXnUtHhQ6avYZUmLj6GGrCTvg4efffZZXHXVVeuhBgA++MEPwuFw4MyZM/l+eKKSUdKhBgBsCuBUMx/nskFx5L84u1XbWQNHpd385qvCmPZdUedGNBTF4kQw/cBjAcwOzeeiqEVVDqFGRjXoE0HoQwvQxwKQoR10Q5Jl5L3FZmBgAB/72McSLnM4HOjs7MTAwEDG27/88ss4duwYNE3D0aNHcdddd+HEiRO7KtPWfu1y6+cuZXu1LrraRgAAzi7jhOA6ZqyHIn3tEADUfX3QHT4IAP3BtS0HNDeEAB4bWTs5iLVQ87Rxnw3O7E8QYu0sLiAAkV2zihACaPdAH1hIe5zSXr3eXQPAWNBvS7Wn+z3hNSLy95qxOWzo+70eDL80htWF1aQLQ27mcNvRfWUnFEVBaDH1DufrJLA6v2q6/FZ8byR73W5+zQIw9brdzWt2N6SUkKMB6MN+o27X6lgOLkA0VUK5uH79tbqb9wYVVkHG2Hg8nm2XezweLC6mn3Vw4sQJfOQjH0F3dzemp6dx6tQpfPrTn8aPf/xjHD9+fMdlUlUl6c9UXHu1LjqahwEIVPR2AADsR4yuCaVxbffq3h4IdyMUAGf9/WsnuArYbMDPBo0To0v1AgCe+7kOIYAm9/b3XDYUVcB8U8Wm23XWILqwCjkfSnq9qHPD1lEDm7oxcyqmCdgSFlcUsNk2Wn5kVMCmGr/rQsC26XUSgcjr68Zd7cSB3+/DyvwqFi8EIXUd9koHtIiG+RE/YmFj5WFfbx3qu2uh2o1yqjYTLVcwxuOYKX+qY7SohvByBIqqwFnlKGj4Sfa63fyaBWDqdZur1+xOxEb80If8GxdsyityahlSStgOJ26ou9P3Bu2W+ee8aNO9pZQZ34R33nlnwu833HADTp48iYceegh/8zd/s+PH1tbWmVBVZf1nKq69WhddbSOQML7xxjTd+MYrJaSvHbqmGS019nogpiV84wX0hG+8MU1L+Na7+bmUugQiGqAIwJ5+QUMBAUUV0DUJmc0CLpsolzZCjgWgjweNxwUAh2q01LR7oEuJmLYxKFdKiVhso7yqKhGLbVyvSAlt7XghJWKb/zbIHb9u9JgO//giludWIQRQ6atETWs1lCQhwuV1weVNnMXZeJFv23HxsrhqnBCqgNTSPIfCGI+TqfzJ3hvR1SguvDUF//nAxmyrCjsaL/ahrrs27wEn2etWr29bf81CSugOX8rXbbx18dmnjHrd+potBBnToQ360x6jT68g5l+FqHbm5L1Bu2H+Oc97sPF4PAgEtk8BDQaDWU/ZrqiowPXXX4+f//znuyrT1lBVKguGlau9WhfdbaOANMYmSCkTx9TITWNqZIYxNTJxTE38KZQxHXJ0EfLCEhAPDpV2iA4PRGNl8pOfMNrjJeTOB/gKAdFZA6XDs7E6sXNjMUUpse0zautjpfpdYMtrRO7sNROcWsLIy+ehx/T1L4JzQwtQHSq6r+xAZX12A3ojyxHMDS1gcSIIqelw1ThR1VCJ4ORS6htJoK6nNm35k703IitRnPuXIcQiW2ZbrUSN2VaBMFqP5G+2VbLX7ebXLIC0r9t0r9lC0mdWAD3DAwtAn1yGUuXMzXuDCiLvwaavr2/bWJpIJILR0dFtY2/M2EsnPipf+R4oLGM69NcmgZUt06+Xo5Bn54DlKERvba7/rARCCMBlvTVAV/yrGP716MbJadNHihbRMPj8CC66sdf0xpaBySBGXjpvfDat3Vd0bTaU6lC3TxlfG8fRcVkrXNXZL1Y48bvJbaFms9mBedS05me21ebXbbpVsIE0oQZFGty+VUTLOG4KEpD+EPS3Z4zfa12QjZXAHu02LxV5r53rrrsOL774IhYWNgYUPv3004hEIrj++uuzuq+VlRWcOXMGhw8fznUxiQqmELOf5ODC9lCz+fqxAKQ/+TiYcjd9djb9uUyXmOk3N1spshI1Qo0uE0+Qaz9rEQ3VTVVweZwQioBiU+Btr8G+G3pQ2+nNuuzR1SgCF4oz26qsQg0A2BVzvRvLUciZFciZFcTem4f2wnnIuZW8F492Lu9fpz7+8Y/j7/7u73DHHXfgjjvuwNzcHB544AHceuutCV1RX/3qV3H69Gm8/fbbAIBXXnkFp06dws0334zW1lZMT0/jhz/8IWZmZvDggw/mu9hEeVGQUBPTIdN1gazRxwNQvelX/y43Wkw3gkE6ElgY86PtWHPGsSpzQ5n3vlqeW8Ghf31R0rE72VpdNBFGJYxZXDlUdqEGgPBVQPbPZ7UXGABAl9DfmoFyvBliBy1ulH8FGWPzyCOP4L777sOf/dmfweVy4eTJk7j77rsTjtN1fX2AIAA0NDQgEong29/+Nvx+P9xuN44fP46vf/3rOHLkSL6LTZRzBVunZjli7sPazJTkMqNFza0kLDUJqUsINX2wCWRaqwbGIOWV+VVUNVSaLWZKCdPk0x2Xw8HD5RhqgLX9zto9kGOZtwHZRgL6WADqoYbcF4x2rSAd4D09PTh16lTaYx544AE88MAD6793dXVlvA1RqSjs4nucipqKza5mHlcBQLEppkKEnmnw6Rpp8rhMKmrdpmdb5UK5hpo40eOF1HRgYlMLp4nXBwBgZgV6VINYDENGNAiHCtS5TYdPyh/rjewjKjMFX1G4ym5M7U53MhUAavdWNxRgBBZvmwf+8UDqk5cA6kxuUOmucSG6Gs14InRW52apZdWuor67FrMD6cfQ1PfWpb3ejHIPNYDRsqXur4ds90BOLgPhGKQigAuZu3IBQL40vh4yJQDYFIgeL5RW6/2tewmDDZUcKSWCU0uYG1pAOBhZP1nVdXthc1rrJV2MbRKEqkC0VEGOpxlLIgGlrfALollB48UNxmJ7yVo9BKDaFPj21Zu6r/qe2vRjdgRQ1VAJR0Xu9pBovqQRq4shLM9uGcC6lsM6LmuDs2p3j7cXQs1mwm2H6PECMD5f9KnlzFPBAWDrayimQ/bPQ9cllPa9+f6yAs5Zo5KiazqGXxzD8K/HEJxaQmQ5gtBiCJNvT+PsL85hZd46sxWKufeT6PECntQDG0WvFyLN9eXM5XGi79ou2ONT0TctJOuosKP397rhqLCbuq+qxkp4U53AhNFClOs1ZRRVQc81XWg/3rK+EKBqV1DbUYP9H+hFbUfNru5/r4WarYQQEM2768qTQwvGbvdUFNb6ekuUweRb0xuLnm35sqTHdAy+MIoD/2o/bA5zS9rnS7E3tBSqAuVoE+RE0Gi5iW/qV+uC0u6BqHPv9E8rCxV1FThwy34EJ5ewMm/sA1Xpq0BVQ4qFC1MQQqDj/W1wVjsxe24OWnTjZFbdWInWw81w5mHmjKII1HXXoq47t2sR7fVQs67GCUxkmD2Xjg7I6WUIdkkVBYMNFZyuGVNuo6tRqHYVNa0eqCaa6rWIhrmh9Bss6lEdC6N+NKx1JURXo1gYW0QsFIPqUOFtrzHdTL+ysIrZgXkEJ4OQuoSrxoX63jp42z1pT37FDjVxQhEQ7R6g3WMMXs3jhpGlSAgBT0s1PC27O/kIIdB0oAEN++uxuhCCrutwVjlNt/pYBUONQQbCkGdnd3cnAsAqdwgvFgYbKqi5oQVceHNqYyl7CYz/9gIa9tWj+ZKmtBN6lmaXTc0uCUwE4eurw+Rb05jpnwNg7A4tAUy9MwNvew3a39eSdl2R+eEFnH/tQsIMiZWFVay8Mo7F8QC6Lm9POvvBKqFmK6vN1HjhGeDqm3Z2W/fxKqy+Zm5wZyEpqpKX1X4LYevrFthZqIkr1VADAPrQQvZr22wlAdis9Z7bSzjGhgpmbmgB47+9YIQaYP3DQ0pgun8OY69NpLytFtOxMJZ+N/g4XdMx+fZGqIk/Rvzx/OcXMfZq6sda9YeMULOpjJt/DlwIYvrd7d/orBpqAEBfjkB7fQrasyPQzqz9e30S+lIkJ/e/E/G/0Q7P+t8fF39+4uIn1/hzGX9uHV1t68877c7mULNV/HUbF3/dlhsZigH+3KzvJHy7X7eIdobBhgpCj+m48OZU2mMWRvwIBbavrKpFNQw8O2QshpaJAByV9oRQk8zieCDlKq6zg3MZl4KZHZhPWMPE0qFmdgXylQuAP5QY1PxhyFcvGJsBFlj8b0sWbuLPS/x5ij9vDDeUd+EcdR/VuyEqS6srspww2FBBLF4IbrTUpCKA+WH/tosvvDWNUMDktygJ2N32zE3JAlgYTd4CFJxcynh7LaohtBaMrBxqZESDfGsm/TFvz0Bu3aixAMol3EgpEbgQxMj/OY+B54Yx+so4gtNL3LC3hMhQDPqwH/qQ39wNHAqQaqxejRPKAV/y66ggGGyoIKIr0cwL4kogspq4caMW1bAw4jfd513b5TXGk5jo3o6l+HZm9nwkdWnpUAMA8ry55eJ1k8flWqmHm1g4hnP/MoThF8ewOB7A8uwK/OcXMfT8KIaeH4XGKb+Wp58PQH9pHHJk0fQ2I6K5Gsr7mqFc0gD4KoylFRoqoBxuhHK0CcLGU2sx8dmnglAdqqlWFNWeOE17dTFkejn65kMNaD/eArvLRIsNAJsz+ZRwt9eVMRgJIbD/IqMlxKqhBgDk1kXcUinibsWlGm6klBh+cWyjSzP+mlv7f2lmGWOvjhekLLQzcmYZciD9TMttVAHRWmWsd+OrgHpJA9TjzVAPNUDUuTnz0AIYbKggPC3VplpsUi52loHNqaLx4gYIIVDT5jH1WLUd3qRX1ffWpQ9GAmi/WIXDJXIWanyO6vzMJDG7R1GRu01KMdyszK8aa+CkeeoCE0GEg3tvs9FSIKWEPmxuQsI6uwL1WBOExVY4p0QMNlQQdpct/WJiwmgp2boDsrvGlXmq8tqy9Zsfy9eXfq8cT2u10TKT7LrmKnhTrd4qAHeVwMGrHTkNNXljdo+iPCwil61SCzeL44HMAVoAi7tZ6I3yZyUGrEQzH6cAoqECyoF6OK7pgLDAe4XSY7Chgmk90oyqxuRTIJ2VDvRc3bWtGVe1q6jt9KY/gUigfkuQabm0aeMysekfgJo2Dzrfn3pqqxACHZe1ouXSJthcG9/MhCLQedCG3/tDNzwHOwAkhhrb/j5rhRoASqe55fWVTu+OH0PqEvqFILRXL0B7fgzaS+PQh/yQO5hhUkrhxuz4GS1W+IHZZILZ8U9CQDnUAKW5CiLN2ldkHWxPo4JZmVvZvnHfmvByBKv+VVQ3bd+jpeXSRqwsrCCUYmBf86EGVNYlLowmhEDbkWY07KuHf8yP6GoMqtNYedhl4huXEAIN++vh21eHcDAMXZfYv38adkfy7iczi5gBhQ01AIxvl02VwNRy6oPaqnc8NVXGdOivTwJLm775xnTI0UXI8QCUI01Z70nlc1RjNhJcX8TPCDfG4OaPdrphs63i3NIA9lUZIVKNzsK2vw+x/gHo9e1Q5s6vL+Ln6GpDN0YxPN65o78vHVMrC0vkdANMyiGXyW1XXDxNlhrGTyoIXZcYefl86oHAEhh5+Tx0bfu3KNWuou+6HjQdbEgY8FtRX4HuKzvQeHFDysd1VNjReHED2o61oPlgo6lQs5kQAi6PC4cumSm5UBOnXFwP0VWz/d2uCKDXC3Vf+m67dPR3ZxNDzWaahP7GNGSSOs0kWcsNYDyXVmm5qe30ZhykLhSx43FjlF/CaQNqk3dHJxy3yy03qPAYRakgAhMBaBnWStFjOvznA6jr8m67TrUpaDrQgMaLfdCiOhRFQCnQlMp0U7qtHmqAtd2Ku72QXTWQ/pCxIabbDlHj3NUMDhmKAbOr6Q+K6ZBTO9sMMFnLTRQBnB4NWaLlxgjNvqSrUMe1XNK4baZfLkhdIjAZROBCELom4axyoK7LC0clW4eyofR4oS9OAqmyd4UdopkrCJcatthQQawshEwNtFxdSH+iFELA5lAZanZACAGl1g2lpRqK17XraakyQ12tHzdv7rhkrN5y03SwAc2XNEJRt44NU9B2rBm+tc1YtYiGxYkAFsYWU654bVZ4OYJ3nzmHkZfOY2FsEYvjAUy/N4uzvziHyXem9+TCgPEF9rS3ZqC/MwN9yty+cqLaCeVIU/LuploXlGNNHFdTgthiQwVh+hxqoTUgyinU5IXZqeRmj0vByi03Qgg0XuSDr7cOgaklxMIx2F02VDdVQVEV6Jqxlcj8sD/hROv2utB+vAVub3Z7LukxHYPPjSAaWuv+27J2zvTZWdgcmWcFlhN9bBFy0L/+uwSA6RXIQRXKkUaIDK1YosYF5fJWY4uR5QggBEStC6LEdmenDYyiVBBVDZWZF82TQFWDNXZHZqjJLNMJY/24VEvPZ8HqLTeKTYG3zQNfbx1qWj1QVGV9Ab+5wYVtrQer/hDOPTucdevNwtgioqvRtO+lqbMzphe1LHX65FJCqEkQ0aC/PgUZzTwrTayFGaXdA6WtmqGmxDHYUEFUNVbCkWHmjVBF0llRhcZQY1KNE3BnbvQVLbmpU6uHm60CE0EsTaeejSZ1iYnfTWZ1n34TO9xrEQ1Ls2lmwZUJKSVkkr3lEkR1yAtLBSkPWQeDDRXE+orAaUhNFn0xM4Ya84QQUC72pf0UET1eCHfuvv2WUriZHZxPf4AElmdXEF6OmL7PWMTc2kBadA/sUbUUAcKZW2NkuqUOqCwx2FBBSCmxaGKjxQtvTGLslXFMvDGFVf/uBllmi6Eme6LGCeVYM+DdMo3eZYO4uN70AoHZsHK4kbrE3PAC3vvlQMo1m7aKLJkPNo4Ku6kNXu0mWtJKntkF9rgR6Z7DYEMFEQtriJhYvjwW1rAwtojZgTn0//Mghl8cg16ADyaGmp0T1U6oR5uhXNEG5WgTlMtaoFzeCqU5f92KVgw3ui4x/OIoxl+7gFDA/P5Q2czwq+uqzThWzVHlQEVtdoOSS5LZ/ZrMLsRnkpQSMqJBMjBZFoMNWdPah3fgQhCjed4hmaEmN4TLBuF1QVQ5CrLDsdXCzcy7swhm2e2hOtWsQoinpRoVde60rTath5v2xA7TosJuai+0XC2wJzUdsaEFaL8+D/3X56E/Pwbt1Qno08t7coq9lTHYUEHYnOqOm8cDE0GEAvnplmKoKW1WCTdSl5nH1CTRuL8+8yavmwhFoOfqTng2n6zXbq46VHRd3g5P8955DSq9tem75irtECn2p8uG1HRov52CNugHIptaapaikO/Mpp6ZRUXBYEMFIYSAr69+hzc2prnmGkNNebBCuAkFwxlX1l63diKu66ldX8AvG6pdRfcVHTjwr/ah9UgTmg82ouvydhz60EUZB+iXG+F1QTncCDiSdDfVuaEcbcoqOKYihxeBYOqxUPJ8AHqaGXBUWHtghBlZha+vDsHppbRTYFMxfdIwiaGmcGREg7ywBBk0xp0IrwuiqRIih1sNFH0Rvyx6Imo7alDXU7tt49ZsOSodO/+yUEZErRvKlW3AfAhyZW2BvTp3VmvRyNUo5EQQcj4EQAIeJ5TWaohqJ6SmQ17IPFtTvjsLWeeGKNCq6JQaa4AKRigCPVd1ouVwE+xZLoBld+VuyjBDTeHoU0vQXzxvrDcytwrMrUIOLEB/cRxybudbLSRTzJYbZ5Vj27YK2whjocqOy9p2HWookRACot4NpaMGSrsnq1CjTQahvzwBeT4IrESBlRgwuQz9N5PQR/zGZZqJ5KoDcpJr5lgBgw0VlFAEGvbV48C/2odD//oi7P/9vsw3koA3R9OGk4Uavb6doSYPpD8EeXYueWuGLqG/NQ2ZxVRnM4oVbhSbgtruDOM9JFDfu3e2OigF+kQQeDf12Cg5vAh9wfz4PjMtO5R/DDZUFEII2Jw2uD1ONF7kS3tsXU8tnDnctXhrqNksfrKLY6jZOX0k87go3cTaRtnaWi/xOgOM+ozF3NvCzXp5trwesgk3zQcb4Kp2pgw3tV1eeHK0CnMubf374sHOjPj7Ii4eKEuBDIYh+00M+J5aAsyO0zGxYCDlH4MNFV3zJY1oOtCwcULY9H99by3ajjQXq2gJNp8gKT0Z04FMCyxKAGU0VVa1q+i7rhsN++qh2Dc+Wh2VdrQda0b78RbLTcNO1y0LbLRmAUZL17mlgbRhHyidwK8Pm5yQsBIDfCan5HN8jSVw8DAVnRACbYeb4eurw8LYImLhGGwOFTXtnpyOraHs6RENcsgPLKwY01wVAfgqjHEM6Ta3NLt4mVz7Z63z/Y6pdhUtlzah6VAjoitRCEXA7rZZLtAA5kONZveZ6pYFSifUyIgGzGcxxqvDA8yuABle1qJp91PLafcYbMgybE4bfH0cg2AFUpfQzs0DE1sGQ2oSmFqGPrUMcdAHJdUaIXbFCCuZGmPsSk6m41qNogg4c7Creb7s5VADAMhmlqUioLjtkAd8kG/Ppj5OFRCtJfQclDG2mxERAKP7SAbC0IMRaGdnt4earcefnYVMsU2GUBWgqTJjS0yuVoUl8/Z8qAGM4G2SaK6EUBWojZWwXZxier1dMdbMMbvNA+UVa4Foj5PhGPQhPzC9DEjA9HdZCciJIMS+5K1sSpcX+uxq8m4pAcChQrSV2AmxxDHUGITTBtQ4gcUMe3opAqLLu/6r2u6BrHNDv7AELEeM13GtG6KxsixbHktVQVpshoaG8NnPfhbHjh3DVVddhfvuuw+hkLkpdI899hhuueUWHD58GCdPnsSTTz6Z59IS7R0yHIP+2iQwtZzVInPrt08zTkG4bMbO31VJxkl5jF3BRbIVYykvGGoSKV0ZlpAQgDjauO01KpwqlK4aKIcaoBxsgNJcxVBjMXlvsQkEAvjUpz6F1tZWfPe738X8/Dzuv/9++P1+fOtb30p726eeegr33nsvPve5z+Gaa67BM888gy984Quorq7Gtddem++iE5U9fXAhu/EGW2UIQ6LSDvWyVshAeG3lYQHhdULkcPo+ZcZQs52odUMc9EG+OwfoW17IdgXK4UaIamdxCke7kvdg85Of/ASBQACnT59GXZ3RZK2qKu6++258/vOfR19f6gXaHnzwQdxyyy344he/CAC48sorMTQ0hO9+97sMNkS7JKMaMLOyo5aadSZ2VwYA4XFCeHiSKAaGmtSUxkrIOjfk1JKxF5QQQK0LwlfBVpgSlveuqGeffRZXXXXVeqgBgA9+8INwOBw4c+ZMytuNjY1hcHAQJ0+eTLj85MmT+N3vfof5+ex30iWiTVaiuws1ABCOQe+fgwxkGKtARcFQk5mwKVDaPFAO+KBcXA+F42VKXt5bbAYGBvCxj30s4TKHw4HOzk4MDAykvN3g4CAAoLe3N+Hyvr4+SCkxODiYEJaysXVNCSuuMbFX5b0uROJjbH24tL9nOLbk5OLDOxCBDEYgJ5YgGiqgHPRZ56SwuRgyfd1uv27zldb4jMi2DF1tIwAAZ5cRYlzHjKn50tcOAUDdt7aVhMOHc8G1lZg1N4QAHhtZCzViLdQ8bdxng7O8Qk02xNoLSkAAojwWlSxXBRlj4/FsX7HV4/FgcTH1yo/x67betqamJuH6nVBVJenPVFyFqAsBAdva4wghoKqbBgYKAZtN3fSrgM22caxN3XydXvKvHVnjgmZTzC+ml/KO1v6bWYG0z8N2sGH3hdslo7426iembdTl2hHrdS2jiXWrb7ltBKLodZ3t43c0DwMQqOjtAADYjxibbiqNazuW9/YAAIS7Ee/5+9dCUwVsNuBng6sQQsClegEAz/1chxBAk5srbwNY2+y0+EF37zH/nBdtureU0tQ3kK3HxJdf3803KE0zPshVVVn/mYqrUHUhIRFbexyblIhpGwNnVSmhxTZ+l1Kul0luOVZKlMVrR2mvzry0vNe5seJqhi4nfWIJsa6aoq/nsb2+JGKbApyqSsTW6lqREtqmY4XceI0AxmummHWd7Xujq20EEkZLTUzT4TpWCSklpK8duqYZLTVSQnf40L/wHgCjpQbQE1pqYpqW0FJTDq/33RAQUFQBXZOQu+7DpeyZf87z/unj8XgQCGzf6C4YDKYdOLy5Zcbn29ioLn5fyVqBzNoaqsplr5pSI6XEyvwqFkb8iKxGodpVeNs88LRU5687QybW99aqT/t7hmNLUkeNMWhybsu07bVVg8XF9VCa18Zl9M8ZM5sy/N36zAqUNgt8u8+ibjf/LLDlM0EW7zMi28+p7rZRQBpjaqSUiWNq5JYxNcE0Y2pk4piasnit75Yw9v6QkHw+LC7vwaavr2/bWJpIJILR0dFtY282i4+tGRwcTAhAAwMDEEJsG3tDpUXXdIy+Mo7ARHBj6X0BLI4H4Kx2oPeaLtjd3Ccq34QioFzSYHQjTQSBpYixKFm9G6Jty35QUT3zlyaxdhxlFAvHEAqEIYSA2+uCsssNFMt9oLDZVn6ivAeb6667Dg8//DAWFhZQW1sLAHj66acRiURw/fXXp7xdR0cHent78cQTT+Dmm29ev/zxxx/HkSNHdjxwmKxh4neTRqgBNk6Wa/+HlyIYemEU+2/s5QdZAQghIBorgcZKCLHR9bHtW6nLxMeFNHncHhYNxXDhjUn4xwPrr3nFpqC+pxZNBxug7GA8T7mGGqnpkBNLRugOxYwB7/VuKB0erjFDKeV9RNzHP/5xVFdX44477sBzzz2H06dP4z/+x/+IW2+9NaEl5qtf/SoOHTqUcNs777wTTz75JL7zne/gpZdewl/91V/h+eefx5133pnvYlMeRUMxzI/4Ux8ggVAgjOBk+r2KqLDEWpdU+oMA+Nx5L0upioVjOPcvQwmhBgD0mI6Z/jkM/3oMcuticRmUbaiJ6dBfm4QcXDBCDWAspDe7Av03k9Cnl4tbQLKsgoyxeeSRR3Dffffhz/7sz+ByuXDy5EncfffdCcfpup4wgA8APvShDyEUCuF73/seTp06ha6uLnznO9/h4nwlLnAhaKpLY3E8AA83SbQMUWEHWqqAC2kCpwTkbyYhDzdCsCtxm8m3ZxANpV4/aGlmGQujftR110LqEoGpIFbmVyGlREWdG57mxPFn5RpqAEDvnweWk2yyGp+Fd3YW0uOEYAshbVGQV0RPTw9OnTqV9pgHHngADzzwwLbL/+AP/gB/8Ad/kK+iURFoUW1jXE0qEtB2Ow2Zck7ZXwfdpgBj2ycErAvFoL8+BeX9rRC7HDdSTrSYjoVRf8ZQPzu4AFeNCyMvjiEaim3McpWAzWVD1+XtqKyvMBVqNLsx8aLUQo2MaMBMhhaZ+CasvbWFKRSVDH7qUME5Kh2mWmwcFfzGbzVCCCg9XsCR5qNDAghrkFPsKtgsuhIx1c0UDoQx+NwIouG17heJ9fdLLBTD4PMjqK8YBlCeoQaAsayAiR45uZB6E1bauxhsqOA8zVVQ7RleehKo6/YWpDyUpWAEiGRuTZMcI5VAKOY+biUkdD3NDDRdov/VSPmGGsD8Wgqcdk1JMNhQwSmqgtYjzWmPqev2wuVxFahElJWoyd3AzR63Rzgq7ZlbIeNdtGlO2FIC4/0xaDG9PEMNAJiZ8SQAUcOZUbQdgw0VRW2nFx3vb4PqVBMuF4pA48U+tB1rKVLJKCOHyaF5RV592GqEEGi4qD79QWYbKjRA3WcEllIPNVKXkLMr0McWoU8EIcMxY0BwXYYvNhIQnFxASfCTh4qmtqMG3jYPlqaXEQvFIGwKqpsqodrVzDem4qmyAxV2Y3fwNESLienhe0xddy1WF8OYH1pIHEC/9nPbsRZM/G4y41gcoQo4KtWSDzX69DJk/3zCfmWyH0BzJURfHeTSFBBJ3vIneryJC0gSrWGwoaISioCnpXrTonDsNLc6IQSU3lrob06nPqjSbiz6RwmEEGg72oya1mrMDS5gZWEVQgDVTdWo762Fu8aFlYUVLIwupmy9ESrQfUMtRIuxoWWphho5swz5zmzyKyeXISM6xPEmYDRgDESPh70qO5TOGogGvr4oOQYb2jO620a3DbiMi6/3ERc/QQAbJ4e4+MlhLxP1biiHfNDfW/u2vbn1weuEcrAhf/t9mfTCM8DVN238fno0hI92bnRvnFsawL4qIxTY9gOx/o06dx+vwuprG4Ofu9tGMTzemZNyCSFQ3ViF6sbkLVqN+31YPB+Arm1PNkIxdpc+9PmjAEo41EgJ/dxC+oPmVyFWPRAX1UP21RotN4oo+garZH0cY0N7QrJQk2wRMyDxBGHlk0OxiYZKKFe1Qxz0QXTUQHR7oVzWAvVoM4QjsTtRSgm5EII+tAB9YMHogshyhd1sxOsnXl/x+ovXZ/zkH69rI9wYr4P46yL+Oom/buLrxuSbs9qJnmu74KwwgqFQjH8A4PDYcOPD18Db58l7qJG6hJxZhj64AH1wAXJhNXctqv5Qyi6mzfRJY9sVoSoQbjtDDZki5B5r+w9PzeLNO74OIQS7Pywi33XBUGNe2r2idkiuRKG/NWOMydm02BzsCpQDPoi67LdgkBENWI0aewdVOVLuKTYbMU6M8ZabKIyFBeMtNzabsQ7KvirjdaBGZ9dbbpS58wCw3nITGRkHgJy13KTT3TYKXZOYW6rB3HAIol5F4yVVaPvIpVDtSv5DzUII+jszxoamm+vMbYNySSNE5e7WmNIvBCHfm898oMcB9bg1JhLk471B5n3zMyfQUGPus4LBhsGm6PJZFww12cn1h7eMaNBfmUi947cAlGPNEB5z03blShT64AIwt2lhNocK0emBaK1OGnDyEW4KwdHVZuz8fbzK6Lop0EBhGQxDf20y9QwtuwLlspZdtZ7ImRXob89kPrDODfVw444fJ5cYbIorm2DDdj0qWww1xScngqlDDQBIQB/2Qz3SlPm+VqLQf3MB2Dr2JKJBnluAXIlC3b99OrXPUY3ZSHB9zI0dHkQRWB9zE4u5YbOtJh1zo9e3Q5k7vz7mJv56KhTXMWOArPS1G1sqFGBMjT6SeuAyACCqQ47vciuDOpfR2pZp9hcHoNMOcIwNlSWGGmuQ6TbMjFsIQQ/FoE8vQ3tjCtqrF6C9OQ05u5LQgqf3z20PNZtNLEEuhpJelcsxN4USfzylsXCzn2RUS2wNS3XcLleVFqoC0eFJcwAAlw2ioWJXj0N7E4MNlR2GGgsxufqw/N2UMfV3PgQsRYC5VehvzUB/bRIyqkGuRgF/OOP96BPBlNeVUriJP470GY+r7ivQ7Kd0rWtbjtttt7HoqgFaUzyfLhuUo01Fn1lHpYldUVRWGGosxq6amv2C1Vjyy4MR6O/MQmk1WQ9LkbRX56pbqhD0+nZj3G5vD3R7PSALMKU70x5ucTYl5YBts4QQUPfXQ7ZWQ04uQa7GjJachgqg3p1w/zKmQ86uAGENsCsQvoptM++I4hhsqGww1FiPaKqEHAvs7k4WQpD1JmdOmfiGv9twU0jqvj4IdyMQ0wqyTo2wq0CtC1hI3qW3flxz7sKdqHRA9NUlvU5KCXk+ADm8aIzHWVsvSZ6bB9qqofTW7jpgUflhVxSVBYYaaxJt1YBt9x8zMqyZCi2i3tyYjN10SxWKbX8fdIfxmu0PFm7xPaXLuzHFOxlVGPWagpQyZ7Mb5VgActC/Mcg4frcSwPkg9H4TU8Zpz2GwoZLHUGNdwmmMlUB8s1OBjZOmKozWgUx5RQBCmth7Sslufyorh5vNA4XP+vsTypPv162ocUK5pMGoHyCxzpwqlKPNxiaVW8i5FWivT0F/bhT6s6PQXr0AfWppW8iR4Rj0sYCx8N94wFiTKAkZ0yGH/ekLe2EJMsOeZbT3sCuKShpDjfWJKgeUK9qAuVXIhRAgJVDtgGishBwPGpelIwFU2CCaqiCXo8aqtdseBFAONWa9tspOu6XyLf6a7Q8OQAgBTSvsNgmivgLKVe2Q0yvAUhiAgPC6AJ87adePPrQAObqly3EpAnl2DnJ2FcohnzG1f2AemFibURXvVhpYgOjwQHR7E8fVTC+b2u1cTi1B9Oxi6jmVHQYbKlnZhppS2EOnXAkhAF8FhG9LV1FzlfGtPN0JTBEQDZUQioBypBFyesVYH2clanSL+Cog2qoh3DtbDXej5SZoOtwUwsZ+ZRUA9IK/boWqrLWApW8Fk3Mr20PNZrMrRoBdjgCTy5tuuPF//PYJASWsJe5BlkrY3Mw72jvYFUUlp7ttlKGmTAiHCtHtTX9MXy3E2jgdIQSUpkqox5uhXtMB9cp2iB4vsByFPhE0TrIZFn2T4RjkYghyKZLQTeJzVJvulsq39e6vtZaax0as+7rVz6eeYh8nxxYTQ03SY7Z0S9mUzKFGICdjuKi8sMWGSkp8I0KGmvIhOjyAqhgtN7FN66g4VIgeL5QUM3CMGTNByBH/+sJ9EjCmIvd6obQk1qtcjkIf2rIdg1OF6KyBaKmCEGIt3GRuuSmEWMwNIYCfDRrlteLrVkoJpFgUMUHExPo40uh+Eu3G3ykaKiAHM+wALrk6MW3HYEMlg6GmPAlhzLKRLVWAPwQZ0SCcKuB1pZ3KK0cWIUcWt18R0yHfm4euA8ra7B25HDH2P9q6cnFYg+yfB0Kx9S0CMoWbQnpsJAQhBOzCA0iLvm5ztW+SWNs2Y2jBGNuj6ca6OukWDfS6gGpHjgpA5YJteFQSGGrKn1AERJ0bSnMVRG3yQapxMhxLHmo2HzO4ALnWAqS/l347BjkWgNy0uF+6bqlCiT+eS/UCsObrVghjd/WMzKwgLAFMLhnjbUIxI9BsDjViy/9eY/YW17GhrRhsyPIYamgrmWG8BgBAl5Azy8ag1UD6FYmB7dsxFDPcrL9uxdrr9mmjPFZ83Yp2E2Uy212UrvXHbTcGoDdXQTnWBOVI0/rYK6LN2BVFlsZQQ0mFoplnzAgAqzFIs/sNBbeHn1TdUoUQf90+93MdPnuVsSlowNgvS3hdQG36rrpCEY2VkHOrwMxK8gOqHVD21UI3ucFmSqtRKMeajNWRidJgsCHLYqihlFQT39SlcZxQhLlhICkC0NZwU0gvPA00Rh2IvjJu7Lm1VkQ5FgBcKpRLGiHMdAXlkRACykEfpCcIeT6wMf3apkC0VkN0eiBUBcpBH/R3Zo1wEw+l8f8zjaXB2nH+ENDAwcKUHoMNWRJDDaUjGiogxzNPMxYNFcZJ08R6KCLNflSbw02hvPAM4Iu5EP3Nhe1bCgBASIP+20ko729NuhJwIQkhINo9kG3VRrCREnDaEnbnFqoC9dJGY5r91LKx87tThWiqMgJPNHN3odQzL1RNxA5KshyGGsrI4zT+pVPvhqiwG10XTRm+5Ssi48aOm8fc5NsLzxiPp4/4jZCQiiZ3v8loDgkhIFw2CLc9IdQkHFPlgNJXC+WAD0pPLUSFHagyubiijbGGMmOLDVkKQw2ZIYSAckkD9DemgKUkewXVOKEc2FghWNlXB30lBqyNUUmgCCiXNkA4Mo/diLfcrJMStmAM9rkIhCahO1WEGx2Qzt2NA/E5qiE1HXJmJWNLk5xcgtxX2rtcK63V0E0MCJdvzkDzOqH01RW9C46si8GGLKOrbQSQDDVkjnCoUN7XAsytQp9aMhaBc6rGgn5bBtYKVYFytAlyetnYjmE1ZmzH0FAJ0VqV1XYM8deUjGjQ35o2Bh1v6upyn181NpCUMBYZbK4yHiPbQa8x3dwaMbo0/qmlG2xEtdNYy8hE9yL8YeivTUI53sxwQ0kx2JAldDQPG+cBhhpaY6xqGzamYS9FjO4iX4WxSvDaZpfxPajUrXtQJSHi3U0ZupzWHz8YNlpDVmKATUD4KiEaKozByFJC/90UsLzWWrQ1gMTXzAnFIIf9kBNBY0ZPNvtZmZ3KLAC5GII+sbQeskSd29g/q9JaJ34ZikFeCEL612Z31TiNAcYuG0RfLeCyGV1rKXb8XqdL6O/NQX1fSwFKTaWGwYaKrqttBICAs6sdUkqGGjKCQ/88cGEpoTVELi9Cji0as4Hq8rMKsJQS+nvzwOSWx55dhRxSoRxtMgLNcpIusFQiGvS3ZqBc1mK+y0gRQI0TWEzSfbaZywb5xkzi3zC5BHlhCeLi+pRbUhSaPrkE+e5cwmUyEIYcC6yXMz4AWX9jGsi063swArkcsVx4o+Lj4GEqqviYmoreDgBsqSGDPB8wQg2wvTVEB/S3piFXswgW2Tz28KIRapI9dliD/voU9Pj12ViOZg4p8TLEw1Wm49fW6tl+B2v/vTsHGTT3mPkk/aFtoSbh+nfnIP3G+1kIkbnFJn67bMIl7RkMNlQ08VDj7DJCjOuYMXOFoWZvk7qJmT46ICd2EC4yPbamG6EqnbAGrOzghCoAmakVIl6OieBGuEonU3eVMLf7dr7pY+m3v9h2jMlFFVPNvKK9jV1RVBRbZz/Zj1QY31IZamgpknmxNgBydhnoq83tY8+HNtaMSSeWuXxJpZu6vX5IFlO4zSxqN7d9RWC5GoWcWIKcWTbGA1XYjQHODZU5DwtS043nNZP5EKQujbFQvgrIJCtBJ1CEsQkm0RYMNlRwW0NNvKVG+toByVCz56XZrHJHx2VBaiYDi83ESrnb7hzmNoxcjW2s3psLW54mOb9qzObaXPxA2BjvcmEJyuFGCDMrO299mFAMcmEV0AFRZQc8TqNbyUxQjNN0QDFmksnRxbR1LFqquFcUJVWQYHPmzBl85zvfwcDAAJqbm/HHf/zH+MQnPpHxdhdffPG2y3w+H55//vl8FJMKINU6NUpjJ3RNY6ghoMLkx1JlFjOMTBIum7ntF6rsgFMF/FmMX7EpECZmb2UVBMyo2Hie5NogZqTKZIth6AMLUC+qN333MqoZu6fPbuwDJdceV7m4Hqh2GFPRMwVRVax3rQmHCuXSRmMQcbLno84F0Zvj1joqG3kPNq+99hruuOMOfOQjH8G9996L3/zmN7jvvvvgcDjwh3/4hxlv/8lPfhInT55c/91uz/2HGRVGqlAjfUb3k7qPoYZgTOWuc2XsvlBa81D3NU4jsGRoMVFaPUC1A/rZOWA2xeaPmwlAOegz183jthndLDkKOKJt43mSF4KZ73dyCbLHa2rdHanp0F+fSj5DbCUK/fUpY72ZluqMY5dEa3Xi2kNeF5TLWyEvJHaZKa3VxqrSJbwgIeVX3oPNf/tv/w2HDh3CX/3VXwEArrzySly4cAEPPvggPvaxj0FR0jcltrS04NixY/kuJuVZuhWFBQD09gBSMtQQAEDpq4O+eCH1t/w6F2Cm9SNLQggo++ugvzmT+qB6N1BjdLOolzQY41VmVgBNh9QBLIYSdwr3OqF0eyFqzI0HEaoC0VQJeSEHg6NrXRCbtpOQZnbXljBmY5l4fuXUcvpp77qEPrgA5YDPCCcRbftMMwFjIcP27RuMCqcNotsLdHszl5toTV47KCORCF588UV8+MMfTrj81ltvxczMDN5+++18PjxZRKZtEuItNbqDoYYMosIO5XgL4N2yH5RibLaoXNKYt2/sor4CyiUNwNYtFgSA1ioohxoSWxbcdiidNVB6aqH21UJ9XwuUK9qgvK8ZypVtUI82mw416/fZ7TW/QF8yqoDo8EC5dMvzZLIVSJo97oKJGVcLIUBKKMebgdokz0Oty2jVMbGlBZEZeW2xGR0dRTQaRW9vb8Ll+/btAwAMDAzg0ksvTXsf3//+9/Htb38bbrcb1157Lb785S+jtbV1V+Xa+oHIJs38MRbf2z6lW/qMlhp1Xx8EBIS7EULT0R88B01zQwjgsZG1UCPWQs3Txn02OBlq8kWs7Z0sIACR+8G5WZWlyg7lWDPkShRyJQoIYaxUW4ABo6KhAsLnNqZnx7dfqHeb3hZBuG1Gl9JOH9+pAu9vgf6bC8ZWEdlw26C+vyXpAGBR7TC19otS5YCpj8VQkjV0khBhDaLGCeVIk9HCtbZnl/A4s1uNuYis9N6g9PIabBYXjXUJPJ7EJsb47/HrU/noRz+KG264AT6fD++99x4efvhh/NEf/RH+4R/+ATU1NTsul7rpDa/uYPQ/mdPRPAxArC++Zz9iNG0rjZ3GAb09AADhbgQA9AcHIEQlbDbgZ4OrEELApXoBAM/9XIcQQJN7e3M15Z6iCgAWCfzVTuNfMTRk2BU8j9RKB+Q1nYidmze2lTA7C2w1BiWsQ6ne/vEuOmoQzbDZpKhxwpZp5/Q1MVUBYplncCkOFUr8s7bKafwrUZZ6b+wp5p/zrINNMBjE9PR0xuM6Ojo2ipMi+mdqKfnmN7+5/vOJEydw2WWX4bbbbsNPf/pT3H777SZLvJ22NqVTVZX1nym3utpGIGG01MQ0Ha5jlcb6HL526JpmdD9JCd3hW2upGYCmuSGlntBSE9O0hJYa1ld+CQgoqoCuSUhz84MojwQE7BfVQ+v2Qi5HoJ2dM7U4oBYIQSabXVZph2ivhky1aJ8qoFxUZ/p9JpoqIUczrLnjtkF3qean0lsU3xvFZv45zzrYPP300/jKV76S8bjTp0+vt6psbZkJBIw3wtaWnEwOHDiAnp4evPXWW1ndbispZUKokiYWzSLzuttG13fp3rb309Z1aiTQHzwHISoTQw08gEwcU8NqKgAhAQhISD7fOyClBPxhyKhmjBlZG2S8Y2v1AVUYrVZ2cy3MEiJl/YneWsBlhxxbTJz9Ve+G0lsLVNhN171oWduRO916M101QJrylAy+N0pG1sHmtttuw2233Wbq2EgkArvdjsHBQVx33XXrl587dw4A0NfXl+3DM4RYXKaBwptDzbmlAeNnzQ2bDYmhBhwoTKVFn1yCHPKv73MkAWO2T68XSlNuNqIUdW5IE/tNia2DrjdfJwREWzVkaxWwFAV0HXDZ1ndMz6o8LhuUw2vrzSQJN6Ind387kVl5HWDicDhw5ZVX4sknn0y4/PHHH0dDQwMOHTqU1f298847GB4exuHDh3NZTMqRnYSa+Oynnw0a01AZaqgU6eMBY5PHrZs3RjTIs3PGGJkcEM1VmfdRaqgwFVKEEBDVDoga145Czfr91LigXNkOsb8OqHMbU8w7PFAub4XSufOxkEQ7lfd1bP70T/8U/+7f/Tv8xV/8BW699Vb85je/wf/+3/8b3/jGNxLWsLn55pvR2tqKRx55BABw6tQpjI2N4fLLL0ddXR36+/vxve99D83NzaYW9qPC2k2oeWwkBCGEMftJMtRQaZFRDXJgIf0xAwuQjZW7ntElHCqUSxqgvzmdfMhBlQNKFqsG54qwKRCt1UA+Fk0kylLeg83x48fx0EMP4dvf/jZOnz6N5uZm/MVf/MW2cKJpGnR9Y3BZT08PfvGLX+CJJ57A8vIyamtrcf311+PP//zPsx6bQ/m1m1ATX6fGpXqNgcIMNVRi5NRy5nGNuoScXjZO/rsk6txQ3t8KORGEnF5bkddtM1bubarc0T5PROVEyD02aCU8NYs37/i6sWro2qyoPfYU5FQuQo1deGBTVTz7lNGMz1BTPEJg0/ui2KUpDXr/nLFKcLrnSxhbBij76rK6b9aHdbAuiuubnzmBhhq3qWO5uzftWE5CzdqYmud+brTWMdRQqZDhGOTksjGY18yJTuXaJ0SFwGBDO5LLUPPC08a3oQYnp3ST9UkpIYf9mddvSbgRzO3sTUS7xs5YylpOQ83amBquKEylQo4Gsgs1gNFaUyJbBxCVOgYbyko+Qg33fqJSIWM65Gj6rWCS0iT0d2dzXyAi2obBhkzLR6jhmBoqJXJu1fQO2dvMrkKuZt4OgYh2h8GGTGGoIQIQzbzhYzpybjVHBSGiVBhsKCOGGqI1DnXntxXYeWsPEZnGYENpMdQQbRD17p1P25aAqOAAYqJ8Y7ChlBhqiBIJVYHo9u7sxnbF2EuJiPKKwYaSYqghSk60VUP0eI2uJWDj/wyUi+ohMm1gSUS7xgX6aBuGGqLUhBAQnTWQLVWQ0ytAKAbYFMDnBvxhyBE/EN3Y9w4Vdih9tRBsrSEqCAYbSmAm1Gh2HwAw1NCeJuwqRNuW13alA7K1ClgMAzEdcKpAlQNCsKWGqFAYbGgdQw3R7gkhAK+r2MUg2rM4xoYAMNQQEVF5YLAhhhoiIiobDDZ7HEMNERGVEwabPYyhhoiIyg2DzR7FUENEROWIwWYPYqghIqJyxWCzxzDUEBFROWOw2UMYaoiIqNwx2OwRDDVERLQXMNjsAQw1RES0VzDYlDmGGiIi2ksYbMoYQw0REe01DDZliqGGiIj2IgabMsRQQ0REexWDTZlhqCEior2MwaaMMNQQEdFex2BTJhhqiIiIGGzKAkMNERGRgcGmxDHUEBERbWCwKWEMNURERIkYbEoUQw0REdF2DDYliKGGiIgoOQabEsNQQ0RElFreg83zzz+PL37xi7jppptw8cUX4xvf+Ibp20ajUfzn//yfce211+Lo0aP45Cc/ibNnz+axtNaW61Djc1Qz1BARUVnJe7B59tln8c477+DEiRPweDxZ3fb+++/H//gf/wN33nknHnroIdhsNvzxH/8xZmZm8lRa68pHqCEiIio3eQ8299xzD5544gncf//9qK42fzKdmprCT37yE3zxi1/Ev/k3/wbXXHMN/ut//a+QUuKRRx7JY4mtJ12ose3vY6ghIiJak/dgoyg7e4hf/epX0DQNH/7wh9cvq6qqwo033ogzZ87kqniWlynUAGCoISIiWmPZwcMDAwPw+Xzwer0Jl/f19WFoaAi6rhenYAXEUENERJQdW7ELkEogEEjadVVTU4NoNIqVlRVUVVXt6L6FEGl/t4KuthEAgLPLGEPjOlYJAJC+dtj2GaFGd/ggAPQHjVCjaW4IATw2shZqxFqoeRpocFo31AiIjf+FLHJp9jbWhbWwPqyDdVE6sg42wWAQ09PTGY/r6OiAw+HYUaHikgUOKXf/glJVJenPVtHRPAxAoKK3AwBgP1IBAFAaO4HeHgCAcDdCAXDW37/2PFXAZgN+NrgKIQRcqhcA8NzPdTRXZDdou1gUVQCwXsjci1gX1sL6sA7WRbGYf86zDjZPP/00vvKVr2Q87vTp0zh48GC2d7/O4/EgEAhsuzwQCMBut6OiomLH961pRjeWqirrP1tFV9sIJIyWmpimw3WsElJKSF87RE83ICV0hw+IaQktNYCe0FIT07T1lhqr/Y1bCQgoqoCuSUjwm1AxsS6shfVhHayLYjP/nGcdbG677Tbcdttt2d4sa319fZibm4Pf708YZzMwMICenp4dD0oGjFafza1BuWgFyoXutlFAGmNqpJSJY2r2bRpTIzOMqZEbY2os8qelJyQAAQlZGuUtZ6wLa2F9WAfromRYrx9mzbXXXgtFUfDkk0+uX7a8vIx/+qd/wvXXX1/EkuUHBwoTERHtXt4HD4+Pj+ONN94AAKyurmJ0dBRPPfUUAOCWW25ZP+7mm29Ga2vr+ho1TU1N+PjHP45vfetbsNlsaG1txd/+7d8CAD71qU/lu9gFxVBDRESUG3kPNi+99FLCmJznnnsOzz33HADg3XffXb9c07RtU7jvvfdeVFRU4L/8l/+CYDCIo0eP4pFHHkFDQ0O+i10wDDVERES5I6RVBpgUSHhqFm/e8XUIIdYHDxfrKWCoMQiBTXVR7NLsbawLa2F9WAfrori++ZkTaKhxmzrWsmNsyh1DDRERUe4x2BQBQw0REVF+MNgUGEMNERFR/jDYFBBDDRERUX4x2BQIQw0REVH+MdgUAEMNERFRYVh2d+9ykSzU6PXtCcfEQ00cQw0REdHOsMWmAOKhxox4qImLhxoiIiLKjMGGiIiIygaDDREREZUNBhsiIiIqGww2REREVDYYbIiIiKhsMNgQERFR2WCwISIiorLBYENERERlg8GGiIiIygaDDREREZUNBhsiIiIqGww2REREVDYYbIiIiKhsMNgQERFR2WCwISIiorLBYENERERlg8GGiIiIygaDDREREZUNBhsiIiIqGww2REREVDYYbIiIiKhsMNgQERFR2WCwISIiorLBYFNA7uNVCb/b9vcl/H5uaWD959OjoYTrXngmf+UiIiIqFww2edTdNgpHVxuAjVCj17cD2Ag1mt0HIHmoscMDYCPU+BzV+S80ERFRCWOwyZOdhJpYzM1QQ0REtAsMNnnAUENERFQcDDY5xlBDRERUPAw2OcRQQ0REVFy2fD/A888/j0cffRSvv/46xsbG8IlPfAJf+9rXTN324osv3naZz+fD888/n+ti7hpDDRERUfHlPdg8++yzeOedd3DixAksLi5mfftPfvKTOHny5Prvdrs9l8XLCYYaIiIia8h7sLnnnnvwla98BQDw0ksvZX37lpYWHDt2LMelyh2GGiIiIuvI+xgbRSnfYTwMNURERNaS9xab3fr+97+Pb3/723C73bj22mvx5S9/Ga2trbu6TyFE2t/N6GobgbPLCDGuY5UAAOlrhwCg7jNCje7wQQDoDxqhRtM2hRqxFmqeNu6vwbl3Q42A2PhfyCKXZm9jXVgL68M6WBelw9LB5qMf/ShuuOEG+Hw+vPfee3j44YfxR3/0R/iHf/gH1NTU7Ph+VVVJ+rNZHc3DqOjtBADYj1QAAJRG43f09gAAhLsRCoCz/v614FSB06OrEELApXoBAM/9XIcQQJPbs+O/pZwoqgCQfcik3GNdWAvrwzpYF8Vi/jnPOtgEg0FMT09nPK6jowMOhyPbu0/wzW9+c/3nEydO4LLLLsNtt92Gn/70p7j99tt3fL+apgMwQk38Z7O62kbg6GpHTNPhOlYJKSWkrx26phktNVJCd/iAmIb+4AA0zQ0AeGxkBYDRUhPTtISWmmzLUG4EBBRVQNckJPhNqJhYF9bC+rAO1kWxmX/Osw42Tz/99Ppg4HROnz6NgwcPZnv3aR04cAA9PT146623dnU/UsqE7icpzT1h3W2jcHS2QUqZOKZGbhlTI40xNbGYEWoSxtTIxDE1Jh+6vAkJQEBC8vkoNtaFtbA+rIN1UTKyDja33XYbbrvttnyUxRSzISSXuttGASCrgcJJQw04UJiIiCifSmrK0jvvvIPh4WEcPny4YI/JUENERFQ68j54eHx8HG+88QYAYHV1FaOjo3jqqacAALfccsv6cTfffDNaW1vxyCOPAABOnTqFsbExXH755airq0N/fz++973vobm5GX/4h3+Y72IDYKghIiIqNXkPNi+99FLCmJznnnsOzz33HADg3XffXb9c0zTo+sYg2p6eHvziF7/AE088geXlZdTW1uL666/Hn//5n8Pjyf8sIoYaIiKi0iNkMQatFFF4ahZv3vF1CCHWZ0VtfQoYagpLCGyqi2KXZm9jXVgL68M6WBfF9c3PnEBDjdvUsSU1xqYQGGqIiIhKF4PNJgw1REREpY3BZg1DDRERUeljsAFDDRERUbnY88Gmq20EAEMNERFROdhzs6JkTENkbmH9d5sag7AZs96Fc22bBWVtFrzdbvwvjPwX1WPYvBHXckxCrGXD0KpxmbqDncIJMJ7XPfVStDDWhbWwPqyDdVEsddVOqIq5tpg9F2yIiIiofO35rigiIiIqHww2REREVDYYbIiIiKhsMNgQERFR2WCwISIiorLBYENERERlg8GGiIiIygaDDREREZUNBhsiIiIqGww2REREVDYYbIiIiKhsMNgQERFR2WCwISIiorJhK3YBik3TNPzt3/4tzpw5g3PnzkHTNFx00UX49//+3+Oqq64qdvH2pOeffx6PPvooXn/9dYyNjeETn/gEvva1rxW7WGVtaGgI9913H1599VW43W58+MMfxt133w2Xy1Xsou05IyMjOHXqFF5//XX09/ejt7cXjz/+eLGLtSc9+eST+Md//Ee89dZbWFxcREdHB/7tv/23+PjHPw5FYbuAVe35YBMKhfDf//t/x0c/+lF89rOfhc1mw2OPPYZPf/rTePjhh/GBD3yg2EXcc5599lm88847OHHiBBYXF4tdnLIXCATwqU99Cq2trfjud7+L+fl53H///fD7/fjWt75V7OLtOf39/Thz5gyOHj0KXdchpSx2kfasH/7wh2htbcWXv/xl1NfX46WXXsJf/uVfYmxsDPfcc0+xi0cpCLnH3zWapmFpaQk1NTXrl0kp8bGPfQyVlZX48Y9/XMTS7U26rq9/G7rxxhtxww03sMUmj77//e/joYcewj/90z+hrq4OAPCP//iPuPvuu/HEE0+gr6+vyCXcWza//u+99168+eabbLEpkvn5+fX3RNz999+Pv//7v8crr7wCh8NRpJJROnu+LU1V1YRQAwBCCBw4cADT09NFKtXexibewnr22Wdx1VVXJXyAf/CDH4TD4cCZM2eKWLK9ia9/69gaagDg4MGDCIfD8Pv9hS8QmcJ3UBK6ruO1117jN1XaEwYGBra91h0OBzo7OzEwMFCkUhFZ06uvvgqv14v6+vpiF4VSYLBJ4sc//jGGhobw6U9/uthFIcq7QCAAj8ez7XKPx8MxTkSbvPHGG3j00UfxqU99CqqqFrs4lEJZDh4OBoOmupE6Ojq29ZG+/PLL+Ou//mt85jOfwYkTJ/JVxD1lN/VBxSOlhBCi2MUgsoSZmRnceeedOHz4MG6//fZiF4fSKMtg8/TTT+MrX/lKxuNOnz6NgwcPrv9+9uxZ3HHHHbjpppvwpS99KZ9F3FN2Wh9UGB6PB4FAYNvlwWCQ3bFEMN4Lt99+O1wuFx5++GHY7fZiF4nSKMtgc9ttt+G2227L6jajo6P4kz/5Exw6dAj/6T/9J35TzaGd1AcVTl9f37axNJFIBKOjo/jYxz5WpFIRWUM4HMbnP/95zM7O4n/9r/+F2traYheJMuAYGxhNjJ/5zGfg8/nw0EMPsTuE9pTrrrsOL774IhYWFtYve/rppxGJRHD99dcXsWRExRWLxXDXXXfh7Nmz+MEPfoC2trZiF4lMKMsWm2yEQiH8yZ/8Cebm5nDvvffi3LlzCdcfO3asOAXbw8bHx/HGG28AAFZXVzE6OoqnnnoKAHDLLbcUs2hl6eMf/zj+7u/+DnfccQfuuOMOzM3N4YEHHsCtt97KrqgiWF1dXZ9mPz4+jqWlpfXX/+WXX550CjLlxze+8Q388z//M770pS8hFArht7/97fp1+/btQ1VVVfEKRynt+QX6zp8/j9///d9Pef27775bwNIQADz66KMpx+SwPvJj85YKLpcLJ0+e5JYKRZLuM+lHP/oRrrjiigKXaO+68cYbMT4+nvQ61oV17flgQ0REROWDY2yIiIiobDDYEBERUdlgsCEiIqKywWBDREREZYPBhoiIiMoGgw0RERGVDQYbIiIiKhsMNkRERFQ2GGyIiIiobDDYEBERUdlgsCEiIqKy8f8Dmd7DtYPJeJQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = 0.25\n",
    "# plot coordinates\n",
    "x_min, x_max = X[:,0].min() - 1, X[:,0].max() + 1\n",
    "y_min, y_max = X[:,1].min() - 1, X[:,1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# ravel: Flatten the Matrix\n",
    "# c_ : pair 2 arrays, like zip for 2 list\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Give the whole mesh space to the model to predict and get the scores\n",
    "inputs = [ list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx,yy, Z, cmap=plt.cm.Spectral, alpha= 0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd31d78f929449d598c868a4ab28a111e30d94109ecabb7e57cae0b5fb90da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
