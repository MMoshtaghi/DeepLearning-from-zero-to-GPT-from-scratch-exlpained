{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" %matplotlib inline sets the backend of matplotlib to\\nthe 'inline' backend. When using the 'inline' backend,\\nyour matplotlib graphs will be included in your notebook,\\nnext to the code.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "    '''\n",
    "    Build a set of all nodes and edges in a graph,\n",
    "    the order does not matter\n",
    "    '''\n",
    "    nodes, edges = set(), set()\n",
    "    \n",
    "    def build(v): # a heuristic to get the nodes without order\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v)) # define edge like this\n",
    "                build(child)\n",
    "\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "# first have an scheme of your desired graph based on the nodes and operations\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR : left to right\n",
    "\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular (record') node for it\n",
    "        # a visual node\n",
    "        dot.node(name=uid, label=f'{n.label} | data: {n.data} | grad: {n.grad}', shape='record')\n",
    "        if n._op:\n",
    "            # if this value is result of some operation, create an operation node for it\n",
    "            dot.node(name=uid+n._op , label=n._op) # n._op is a string\n",
    "            # and connect the op node to the value node\n",
    "            dot.edge(uid+n._op , uid)\n",
    "            for child in n._prev:\n",
    "                dot.edge( str(id(child)) , uid+n._op)\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch\n",
    "notice: Pyhton by default uses double (64 bits) precision for its floating point operations.\n",
    "\n",
    "but default tensor's type in PyTorch is float32.\n",
    "\n",
    "so to make everything identical, cast tensors to double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below we have a Tensor similar to our Value objects:\n",
    "# Tensors are multi-dimensional Values.\n",
    "x1 = torch.Tensor([2.0]).double()\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want PyTorch to compute gradient w.r.t to a Tensor,\n",
    "# you should explicitly set it to True.\n",
    "x1 = torch.Tensor([2.0]).double();  x1.requires_grad = True\n",
    "x1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.Tensor VS torch.tensor\n",
    "In PyTorch torch.Tensor is the main tensor class. So `all tensors are just instances of torch.Tensor`.\n",
    "\n",
    "When you call torch.Tensor() you will get an `empty tensor without any data` just like the Value objects.\n",
    "\n",
    "it is no problem creating an empty tensor instance of torch.Tensor without data by calling:\n",
    "\n",
    "`tensor_without_data = torch.Tensor()`\n",
    "\n",
    "if you also give it data, it must also `explicitly give the dtype as well`.\n",
    "\n",
    "**But on the other side:**\n",
    "\n",
    "`torch.tensor` is a `function` which returns a tensor, and `data must be given as the input`\n",
    "\n",
    "`tensor_with_data = torch.tensor(data=['must be filled'])\n",
    "\n",
    "it also `automatically infer the dtype`of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all tensors are just instances of torch.Tensor\n",
    "# with the same attributes\n",
    "x1 = torch.tensor([2.0]).double();  x1.requires_grad = True\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], dtype=torch.float64, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor(data=[]) is a function\n",
    "x1 = torch.tensor([2.0], requires_grad = True).double()\n",
    "x1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the same NN Architecture in micrograd with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "tensor([0.7071], dtype=torch.float64)\n",
      "0.7071066904050358\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.Tensor([2.0]).double();  x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double();  x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double();  w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double();  w2.requires_grad = True\n",
    "b = torch.Tensor([6.881373587]).double();  x1.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "print(o)\n",
    "print(o.data)\n",
    "# if you want the item of data\n",
    "print(o.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
      "0.7071066904050358\n",
      "tensor([-1.5000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(o) # data is tensor\n",
    "print(o.item())\n",
    "print(x1.grad) # grad is a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5000003851533106\n",
      "0.5000001283844369\n",
      "1.0000002567688737\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad.item())\n",
    "print(x2.grad.item())\n",
    "print(w1.grad.item())\n",
    "print(w2.grad.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A NN that subscribes to the PyTorch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have our class Value\n",
    "class Value:\n",
    "    \n",
    "    def __init__(self, data, _prev=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_prev) # the order does not matter, use set instead of list\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # After the forwrd path (when we have the data vlaue of all nodes)\n",
    "        # we start the backprop (to get the gradients for each node)\n",
    "        # the output node of each operation knows the operation and children\n",
    "        # since we dont have a backprop for leaf nodes, and \n",
    "        # each operation has different local gradient\n",
    "        # we can't define a general method in the class.\n",
    "        # so at the time of doing the operation,\n",
    "        # we can both define the local gradiant function\n",
    "        # and store the whole chain rule function in an attribute to call later.\n",
    "        # then call the local this function attribute from end node to the begining.\n",
    "        self._backprop = lambda: None\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f'Value({self.label} | data:{self.data} | grad:{self.grad})'\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        # check if 'other' is an instance of 'Value'\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data + other.data, (self, other), _op='+')\n",
    "        \n",
    "        def _backprop():\n",
    "            # partial derivatives for each input:\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "\n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        return self + -1*other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return -1*(self + -1*other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # check if 'other' is an instance of 'Value'\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "\n",
    "        out = Value(self.data * other.data, (self, other), _op='*')\n",
    "        \n",
    "        def _backprop():\n",
    "            # partial derivatives for each input:\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        # only support int or float powers for now!!\n",
    "        # in case of other of type 'Value', we would need another method\n",
    "        assert isinstance(other, (int, float))\n",
    "        \n",
    "        out = Value(self.data**other, (self,), _op=f'**{other}')\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += (other * self.data**(other - 1)) * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value( self.data if self.data > 0.0 else 0.0 , (self,), _op='ReLU' )\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += out.grad * (out.data > 0.0)\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def exp(self):\n",
    "        out = Value(np.exp(self.data), (self,), _op='exp')\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += out.data * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def tanh(self):\n",
    "        tanh = (np.exp(2*self.data) - 1)/(np.exp(2*self.data) + 1)\n",
    "        out = Value( tanh, (self,), _op='tanh')\n",
    "        \n",
    "        def _backprop():\n",
    "            self.grad += (1 - tanh**2) * out.grad\n",
    "        \n",
    "        out._backprop = _backprop\n",
    "        return out\n",
    "    \n",
    "    def backprop(self):\n",
    "        topo_sort_list = []\n",
    "        visited = set() # the order does not matter, use set instead of list\n",
    "        \n",
    "        def build_topo(root):\n",
    "            if root not in visited:\n",
    "                visited.add(root)\n",
    "                # Appending to topo_sort before its children are processed\n",
    "                # will give us out-to-left sort,\n",
    "                # but not out to leaf sort in case of b (bias leaf), try it & see it\n",
    "                for child in root._prev:\n",
    "                    build_topo(child)\n",
    "                # Appending after its children are processed\n",
    "                # will give us leaf-to-out sort\n",
    "                topo_sort_list.append(root)\n",
    "        \n",
    "        build_topo(root=self)\n",
    "        \n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo_sort_list):\n",
    "            node._backprop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can call an object of a class to give us an output:\n",
    "for that we need to use `__call__` method:\n",
    "\n",
    "n = Neuron(x) # object\n",
    "\n",
    "a = n(x) # calling object\n",
    "\n",
    "here we use it to give us the value of the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    # nin: number of inputs\n",
    "    def __init__(self, nin: int):\n",
    "        self.w = [ Value( random.uniform(-1,1), label=f'w{i}' ) for i in range(nin)]\n",
    "        self.b = Value(random.uniform(-1,1), label='b')\n",
    "    \n",
    "    def __call__(self, x: list) -> float:\n",
    "        # w * x + b -> a scalar value\n",
    "        activation = sum( ( wi*xi for wi,xi in zip(self.w, x) ) , start=self.b) # pair up w & x point wise\n",
    "        return activation.tanh()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "class OneMLPLayer:\n",
    "\n",
    "    def __init__(self, nin: int, nout: int):\n",
    "        '''\n",
    "        so we need a bunch of Neurons:\n",
    "        Data structure? -> order matters -> list\n",
    "        '''\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x: list) -> list:\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self) -> list:\n",
    "        # list comprehension with double for:\n",
    "        # write the for loops in a way you write it as usual !! \n",
    "        return [parameter for neuron in self.neurons for parameter in neuron.parameters()]\n",
    "        \n",
    "\n",
    "class MLP:\n",
    "    '''\n",
    "    we want a bunch of layers -> order matters -> list\n",
    "    input layer: nin - hidden layer: hs - output layer: nout\n",
    "    '''\n",
    "    def __init__(self, nin: int, hs: list, nout: int) -> None:\n",
    "        layer_width = [nin] + hs + [nout]\n",
    "        self.layers = [OneMLPLayer(nin= layer_width[idx], nout=layer_width[idx+1]) for idx in range(len(layer_width)-1) ]\n",
    "\n",
    "    def __call__(self, x) -> list:\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [parameter for layer in self.layers for parameter in layer.parameters()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value( | data:0.8018681265098966 | grad:0.0)\n",
      "[Value( | data:-0.8805757909710763 | grad:0.0), Value( | data:0.8555983793423952 | grad:0.0), Value( | data:0.8832098486745505 | grad:0.0)]\n",
      "Value( | data:-0.14642457747197496 | grad:0.0)\n"
     ]
    }
   ],
   "source": [
    "x = [2.0, 3.0 -1.0]\n",
    "n = Neuron(3)\n",
    "l = OneMLPLayer(3, 3)\n",
    "model = MLP(nin=3, hs=[4, 4], nout=1)\n",
    "print(n(x))\n",
    "print(l(x))\n",
    "print(model(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data points\n",
    "x_batch = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "# desires targets\n",
    "y_batch = [\n",
    "    1.0,\n",
    "    -1.0,\n",
    "    -1.0,\n",
    "    1.0\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value( | data:0.07831801032455092 | grad:0.0),\n",
       " Value( | data:-0.8984201737040973 | grad:0.0),\n",
       " Value( | data:-0.6123353581808755 | grad:0.0),\n",
       " Value( | data:-0.21870671677024864 | grad:0.0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = [model(x) for x in x_batch]\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(loss | data:2.4953460872200695 | grad:0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses = [(ygt-y_pred)**2 for ygt, y_pred in zip(y_batch, y_preds)]\n",
    "loss = sum(losses)\n",
    "loss.label = 'loss'\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE loss is always +\n",
    "\n",
    "and to have a better prediction , we need to reduce it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(w0 | data:0.915783763752259 | grad:0.36176150770990945),\n",
       " Value(w1 | data:-0.1905455982987767 | grad:0.3856647427725668),\n",
       " Value(w2 | data:-0.4812601281172937 | grad:-0.4661916955473663),\n",
       " Value(b | data:0.006421811569115254 | grad:0.21113592376717188),\n",
       " Value(w0 | data:-0.39839420483993737 | grad:-0.045486377588545),\n",
       " Value(w1 | data:-0.6410730140883056 | grad:-0.07673443261662213),\n",
       " Value(w2 | data:0.8690608121468422 | grad:-0.11964108063112913),\n",
       " Value(b | data:-0.9669803000002632 | grad:-0.08677935140010419),\n",
       " Value(w0 | data:0.8823970304213575 | grad:0.21388048271275503),\n",
       " Value(w1 | data:0.7836271370251264 | grad:0.2007740466804331),\n",
       " Value(w2 | data:0.22612836684268367 | grad:-0.27577500365992424),\n",
       " Value(b | data:-0.18171296341253096 | grad:0.19501937778275297),\n",
       " Value(w0 | data:-0.013915501429131227 | grad:-1.5341603789801292),\n",
       " Value(w1 | data:0.7833508632156321 | grad:-1.5169831948252233),\n",
       " Value(w2 | data:-0.49347382234372605 | grad:1.786051907179295),\n",
       " Value(b | data:-0.8340878974003199 | grad:-1.3435489045032347),\n",
       " Value(w0 | data:-0.190886004280034 | grad:0.006964782259348715),\n",
       " Value(w1 | data:0.9075409889999237 | grad:-0.0021255041695565618),\n",
       " Value(w2 | data:-0.9343718626168644 | grad:0.0005685534842971836),\n",
       " Value(w3 | data:-0.4925966485326483 | grad:0.007295277127542125),\n",
       " Value(b | data:-0.8871448917940319 | grad:0.00042978026484761763),\n",
       " Value(w0 | data:-0.8562554049601998 | grad:-2.8325411515027783),\n",
       " Value(w1 | data:-0.9445025016481856 | grad:2.911914228852325),\n",
       " Value(w2 | data:-0.6202797055088429 | grad:-2.611187457039119),\n",
       " Value(w3 | data:0.44519469091861463 | grad:-2.213574146109575),\n",
       " Value(b | data:0.4742826584492381 | grad:-2.837512282584819),\n",
       " Value(w0 | data:-0.36913829182359237 | grad:0.16350956290901603),\n",
       " Value(w1 | data:-0.8108764829781616 | grad:-0.14911356853301602),\n",
       " Value(w2 | data:0.3814614115284558 | grad:0.12469524110696012),\n",
       " Value(w3 | data:0.48478597488181285 | grad:0.13635323046309253),\n",
       " Value(b | data:0.38720418080032504 | grad:0.13732234920514852),\n",
       " Value(w0 | data:-0.35869126907510207 | grad:2.5647976353086515),\n",
       " Value(w1 | data:-0.14155628694021005 | grad:-2.6247269043600916),\n",
       " Value(w2 | data:-0.17442429950362448 | grad:2.3180993687250018),\n",
       " Value(w3 | data:-0.5548877779031949 | grad:1.9403448430031713),\n",
       " Value(b | data:0.24116135048622556 | grad:2.5471081829797457),\n",
       " Value(w0 | data:-0.2060707377869342 | grad:3.628259647591734),\n",
       " Value(w1 | data:0.879331092097664 | grad:-1.271749537671063),\n",
       " Value(w2 | data:-0.25631952574162153 | grad:-3.3553696902666905),\n",
       " Value(w3 | data:-0.8632298393820024 | grad:1.976640403123913),\n",
       " Value(b | data:-0.7389279570132599 | grad:-3.6290893027367197)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = model.parameters()\n",
    "print(len(param))\n",
    "param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Negative gradient` means that if we `increase this weight`, the `loss will go down`\n",
    "\n",
    "`Posetive gradient` means that if we `decrease this weight`, the `loss will go down`\n",
    "\n",
    "And we have this information for all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph of 4 forward pass of our MLP network !!!!\n",
    "draw_dot(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "update the parameters (weights and biases) by `modifing them in the oposite direction of the gradient`, by a rate:\n",
    "\n",
    "`p_new = p_old - lr*grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(w0 | data:0.915783763752259 | grad:0.36176150770990945)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].neurons[0].w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "for p in model.parameters():\n",
    "    p.data -= lr*p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(w0 | data:0.8796076129812681 | grad:0.36176150770990945)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.layers[0].neurons[0].w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value( | data:0.9695570006106607 | grad:0.0), Value( | data:0.3908102914553565 | grad:0.0), Value( | data:0.12469364441718465 | grad:0.0), Value( | data:0.8794516540068404 | grad:0.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Value( | data:3.214747740544048 | grad:0.0)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = [model(x) for x in x_batch]\n",
    "print(y_preds)\n",
    "loss = sum((ygt-y_pred)**2 for ygt, y_pred in zip(y_batch, y_preds))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Value(w0 | data:-0.7800143933499435 | grad:-0.058292146708676336),\n",
       " Value(w1 | data:0.16289228661893906 | grad:-0.059958376679883206),\n",
       " Value(w2 | data:-0.20967497944127667 | grad:-0.16440308701852463),\n",
       " Value(b | data:-0.0422469166235695 | grad:-0.09451187988759993),\n",
       " Value(w0 | data:-0.9054239788137961 | grad:0.026104385222427542),\n",
       " Value(w1 | data:-0.3260986947394393 | grad:0.03193490177448111),\n",
       " Value(w2 | data:-0.4879378645782313 | grad:0.06066615724202481),\n",
       " Value(b | data:-0.7991330160394869 | grad:0.041045106917326775),\n",
       " Value(w0 | data:-0.4969658375301522 | grad:-2.3236541276505727),\n",
       " Value(w1 | data:-0.6746652129527699 | grad:-0.6154446437948274),\n",
       " Value(w2 | data:0.5534059556319595 | grad:-1.5059625586659788),\n",
       " Value(b | data:-0.03229857135030967 | grad:-1.7708956190618947),\n",
       " Value(w0 | data:-0.18837168223493062 | grad:0.9342769425612104),\n",
       " Value(w1 | data:0.6794315371587631 | grad:1.3795131167272128),\n",
       " Value(w2 | data:-0.04520585545773326 | grad:1.7576365226168393),\n",
       " Value(b | data:-0.4774789169318232 | grad:1.5612578454642863),\n",
       " Value(w0 | data:-0.6995514990526885 | grad:0.6838793303151273),\n",
       " Value(w1 | data:-0.9215003684239618 | grad:0.8349726359287226),\n",
       " Value(w2 | data:-0.798747136809469 | grad:0.3908233625840449),\n",
       " Value(w3 | data:0.9413459291759123 | grad:0.5140195219225285),\n",
       " Value(b | data:-0.4876062057111272 | grad:-0.8442451980453646),\n",
       " Value(w0 | data:0.6876410911467499 | grad:1.7527301258105836),\n",
       " Value(w1 | data:-0.9688072286742089 | grad:2.3276113115575465),\n",
       " Value(w2 | data:0.3116134179766312 | grad:1.0062145141601648),\n",
       " Value(w3 | data:-0.24645816243599428 | grad:1.1616141719700042),\n",
       " Value(b | data:-0.3406553558461354 | grad:-2.3585965052260804),\n",
       " Value(w0 | data:-0.9612017579040277 | grad:1.182500695659233),\n",
       " Value(w1 | data:-0.08989611985822776 | grad:1.84266189143069),\n",
       " Value(w2 | data:0.6111415773296669 | grad:0.7510446372949404),\n",
       " Value(w3 | data:-0.862547280058954 | grad:0.5257790269719547),\n",
       " Value(b | data:-0.5222798810548124 | grad:-1.8788698569549849),\n",
       " Value(w0 | data:-0.6575722897374552 | grad:-1.7218983581718659),\n",
       " Value(w1 | data:-0.6211052807999002 | grad:-2.338313296454312),\n",
       " Value(w2 | data:-0.3672276832513828 | grad:-1.0150382469542034),\n",
       " Value(w3 | data:0.3457110328359698 | grad:-1.0819227763383965),\n",
       " Value(b | data:-0.5014366983469096 | grad:2.372380905644153),\n",
       " Value(w0 | data:-0.3763716877875687 | grad:2.7512126068286964),\n",
       " Value(w1 | data:-0.5789400052096956 | grad:0.38477454473917805),\n",
       " Value(w2 | data:-0.6924290584769159 | grad:1.2443281088737377),\n",
       " Value(w3 | data:0.797413004821236 | grad:2.173311246563142),\n",
       " Value(b | data:0.448774336025068 | grad:4.120476992850625)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "param = model.parameters()\n",
    "print(len(param))\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd31d78f929449d598c868a4ab28a111e30d94109ecabb7e57cae0b5fb90da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
