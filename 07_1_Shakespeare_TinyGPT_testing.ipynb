{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"dark\")\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''\n",
    "\n",
    "# # for creating a responsive plot\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'cuda' # 'cuda' , 'cpu'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "BATCH_SIZE = 64 # how many independent sequence we process in parallel\n",
    "CONTEXT_L = 256 # the max context length for input and output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique chars of the text by set()\n",
    "chars = sorted(list(set(text)))\n",
    "NCLASS = len(chars)\n",
    "print(''.join(chars))\n",
    "NCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "# encoder tokenizer\n",
    "ch_to_i = { ch:i for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [ ch_to_i[ch] for ch in s ]\n",
    "\n",
    "# decoder tokenizer\n",
    "i_to_ch = { i:ch for ch,i in ch_to_i.items()}\n",
    "decoder = lambda si: ''.join([i_to_ch[i] for i in si])\n",
    "\n",
    "print(encode('hi there!'))\n",
    "print( decoder(encode('hi there!')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Google use Sentence Piece for tokenization.\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "2. tiktoken is a fast BPE tokeniser for use with OpenAI's models.\n",
    "\n",
    "https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "enc_data = torch.tensor(encode(text), dtype=torch.long, device=DEVICE)\n",
    "print(enc_data.shape, enc_data.dtype)\n",
    "print(enc_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap and Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_heatmap(tensor, text=True, nrow=None, ncol=None, fig_size=(10,10)):\n",
    "    if (nrow is None) or (ncol is None):\n",
    "        nrow = tensor.shape[0]\n",
    "        ncol = tensor.shape[1]\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(tensor.detach().numpy(), cmap= 'Blues')\n",
    "    # manually write text on each cell (seaborn annot doesn't look good)\n",
    "    if text:\n",
    "        for i, j in itertools.product(range(nrow), range(ncol)):\n",
    "            # x:col, y:rows, the origin is top left corner, makes bottom <->top\n",
    "            plt.text(x=j, y=i, s=f'{tensor[i,j].item():.2f}', ha='center', va='center', color='grey')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D & 3d Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x= emb_lkt[:,0].data, y=emb_lkt[:,1].data, s=200)\n",
    "    for i in range(nclass):\n",
    "        plt.text(x=emb_lkt[i,0].item(), y=emb_lkt[i,1].item(), s=i_to_ch[i], ha='center', va='center', color='white')\n",
    "    plt.grid('minor')\n",
    "\n",
    "# def plot_3d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "#     tensor = emb_lkt.data.detach().numpy()\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     ax = Axes3D(fig)\n",
    "#     ax.scatter(xs= tensor[:,0], ys=tensor[:,1], zs=tensor[:,2], s=200)\n",
    "#     for i in range(nclass):\n",
    "#         ax.text(x=tensor[i,0], y=tensor[i,1],z=tensor[i,2], s=i_to_s[i], ha='center', va='center', color='white')\n",
    "#     # displaying the plot\n",
    "#     plt.grid('minor')\n",
    "#     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset, prepare Context window\n",
    "\n",
    "1. split rate 90%, 10%\n",
    "\n",
    "2. Dev or Validation set is for hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n90 = int( 0.9*len(enc_data) )\n",
    "train_data = enc_data[:n90]\n",
    "val_data = enc_data[n90:]\n",
    "\n",
    "TXT_LENGTH = len(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we never feed all the text into the Transformer all at once, that would be computationally very expensive, and prohibitive.\n",
    "\n",
    "We actually only work with chunks of text sampled from the dataset. we call it context and we have a context length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = 8\n",
    "train_data[:window+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] -> 47\n",
      "[18, 47] -> 56\n",
      "[18, 47, 56] -> 57\n",
      "[18, 47, 56, 57] -> 58\n",
      "[18, 47, 56, 57, 58] -> 1\n",
      "[18, 47, 56, 57, 58, 1] -> 15\n",
      "[18, 47, 56, 57, 58, 1, 15] -> 47\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] -> 58\n"
     ]
    }
   ],
   "source": [
    "# we want the model to predict what char comes after\n",
    "# any number of chars from 1 to CONTEXT_L as input\n",
    "\n",
    "x = train_data[:window]\n",
    "y = train_data[1:window+1]\n",
    "for t in range(window):\n",
    "    context = x[:t+1] # +1 is bc t starts from 0 and we would get empty window without it\n",
    "    target = y[t]\n",
    "    print(f'{context.tolist()} -> {target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating both Batch dimension and time (or context) dimensions\n",
    "\n",
    "we have batch & time (in context window) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_reproduciblility import set_all_seeds, set_deterministic\n",
    "\n",
    "set_all_seeds(seed=1337)\n",
    "set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[39, 56, 41, 47],\n",
      "        [43,  1, 57, 53]])\n",
      "y=tensor([[56, 41, 47, 59],\n",
      "        [ 1, 57, 53, 50]])\n",
      "if context=[39] -> target=56\n",
      "if context=[39, 56] -> target=41\n",
      "if context=[39, 56, 41] -> target=47\n",
      "if context=[39, 56, 41, 47] -> target=59\n",
      "if context=[43] -> target=1\n",
      "if context=[43, 1] -> target=57\n",
      "if context=[43, 1, 57] -> target=53\n",
      "if context=[43, 1, 57, 53] -> target=50\n"
     ]
    }
   ],
   "source": [
    "def get_batch(stage:str, batch_size:int, context_length:int, verbose=False):\n",
    "    data = train_data if stage=='train' else val_data\n",
    "    # a BATCH_SIZE number of int for context_window_starts\n",
    "    # this random init for the context window throughout the dataset is good\n",
    "    # as the tone and style of the text might change from the start to the end of text\n",
    "    cw_starts = torch.randint(low=0, high=len(data)-context_length , size=(batch_size,) )\n",
    "    x = torch.stack([ data[ cw_start : cw_start+context_length ] for cw_start in cw_starts])\n",
    "    # shift the window by for y\n",
    "    y = torch.stack([ data[ cw_start+1 : cw_start+context_length+1 ] for cw_start in cw_starts])\n",
    "    if verbose:\n",
    "        print(f'{x=}')\n",
    "        print(f'{y=}')\n",
    "        \n",
    "        # in each independent context window, a character is not allowed to look at the characters after itself (causal relationship)\n",
    "        # to visualize: (we will use a trick called masking to do that for us in self attention)\n",
    "        for b, t in itertools.product(range(batch_size),range(context_length)):\n",
    "            context = x[b, :t+1]\n",
    "            target = y[b, t]\n",
    "            print(f'if context={context.tolist()} -> target={target}')\n",
    "            '''\n",
    "            we should see \"independent batch dimension\" rows\n",
    "            and (Context window length) columns\n",
    "\n",
    "            then we take each row and create multiple sequence\n",
    "            with max size of CONTEXT_L\n",
    "            '''\n",
    "    x, y = x.to(device=DEVICE), y.to(device=DEVICE)\n",
    "    return x, y\n",
    "\n",
    "# create the batch of independent context windows\n",
    "xb, yb = get_batch('train', batch_size=2, context_length=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    token_embedding_lkt = nn.Embedding(num_embeddings=NCLASS, embedding_dim=6, device=DEVICE)\n",
    "    positional_embedding = nn.Embedding(num_embeddings=4, embedding_dim=6, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.6825,  0.0427, -0.5355, -1.1698,  0.6652,  0.4814],\n",
       "          [ 0.6836, -0.2636,  1.2229,  0.8503,  0.0778, -0.9458],\n",
       "          [-0.5517, -2.4753,  2.1252, -1.8597, -1.5283, -0.9075],\n",
       "          [ 0.8876,  0.0059,  0.2721,  0.3031,  0.0387,  0.1451]],\n",
       " \n",
       "         [[ 0.1119, -0.7661, -0.6197, -0.6849, -0.7264, -1.7105],\n",
       "          [ 0.9018, -1.3492, -1.4882, -0.2612,  0.2189, -2.1828],\n",
       "          [ 0.1194,  0.5861,  2.1792, -2.5281, -0.5484,  0.6380],\n",
       "          [-1.3081, -1.2529, -0.2132, -1.0740, -0.3529, -0.2993]]]),\n",
       " torch.Size([2, 4, 6]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    token_emb = token_embedding_lkt(xb) # (batch_size, context_length, emb_dim)\n",
    "token_emb, token_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3165,  2.5683, -0.4802, -0.1947, -0.1885,  0.4579],\n",
       "         [-0.2376, -1.0092,  2.3632, -0.5188, -0.5385,  0.7833],\n",
       "         [ 1.2441, -0.0370, -0.1270,  0.1630,  1.4123,  0.4552],\n",
       "         [ 2.0497, -0.5445,  1.3161,  1.8521,  1.0068, -0.5362]]),\n",
       " torch.Size([4, 6]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pose_emb = positional_embedding(torch.arange(end=4, device=DEVICE)) # (batch_size, context_length, emb_dim)\n",
    "pose_emb, pose_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3660,  2.6110, -1.0157, -1.3645,  0.4767,  0.9393],\n",
       "          [ 0.4459, -1.2728,  3.5861,  0.3314, -0.4607, -0.1625],\n",
       "          [ 0.6924, -2.5123,  1.9982, -1.6967, -0.1160, -0.4523],\n",
       "          [ 2.9372, -0.5386,  1.5882,  2.1551,  1.0456, -0.3911]],\n",
       " \n",
       "         [[ 0.4284,  1.8022, -1.0999, -0.8796, -0.9149, -1.2526],\n",
       "          [ 0.6642, -2.3584,  0.8751, -0.7800, -0.3196, -1.3995],\n",
       "          [ 1.3635,  0.5491,  2.0522, -2.3651,  0.8639,  1.0932],\n",
       "          [ 0.7416, -1.7974,  1.1029,  0.7781,  0.6539, -0.8355]]]),\n",
       " torch.Size([2, 4, 6]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = token_emb + pose_emb # (batch_size, context_length, emb_dim)\n",
    "X, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Wq = torch.randn(6, 8 )\n",
    "    Wk = torch.randn(6, 8 )\n",
    "    Wv = torch.randn(6, 8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = X @ Wq # (batch_size, context_length, qk_dim)\n",
    "keys = X @ Wk # (batch_size, context_length, qk_dim)\n",
    "values = X @ Wv # (batch_size, context_length, v_dim)\n",
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 26.2234, -23.4735, -11.3390, -15.3823],\n",
       "          [-12.3156,   1.5337,   3.4252,   0.8565],\n",
       "          [-11.2435,   6.0103,   7.8472,   3.3077],\n",
       "          [-24.6950,  19.0554,  13.7950,  19.4077]],\n",
       " \n",
       "         [[ -2.2381,  22.7478,  14.5210,  18.2890],\n",
       "          [-16.7659,  17.0041,  -2.8188,  19.9568],\n",
       "          [ -1.3789, -11.9098,   8.3356, -15.5845],\n",
       "          [ -7.8504,   7.6658, -10.8747,  12.2220]]]),\n",
       " torch.Size([2, 4, 4]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_pairwise_sim = queries @ keys.transpose(dim0=1, dim1=2)  * (8**-0.5) # (batch_size, context_length, context_length)\n",
    "scaled_pairwise_sim, scaled_pairwise_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.4232), tensor(20.3468), tensor(204.5637))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.var(), keys.var(), scaled_pairwise_sim.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 26.2234,   0.0000,   0.0000,   0.0000],\n",
       "         [-12.3156,   1.5337,   0.0000,   0.0000],\n",
       "         [-11.2435,   6.0103,   7.8472,   0.0000],\n",
       "         [-24.6950,  19.0554,  13.7950,  19.4077]],\n",
       "\n",
       "        [[ -2.2381,   0.0000,   0.0000,   0.0000],\n",
       "         [-16.7659,  17.0041,   0.0000,   0.0000],\n",
       "         [ -1.3789, -11.9098,   8.3356,   0.0000],\n",
       "         [ -7.8504,   7.6658, -10.8747,  12.2220]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tril(input=scaled_pairwise_sim, diagonal=0)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 26.2234,     -inf,     -inf,     -inf],\n",
       "         [-12.3156,   1.5337,     -inf,     -inf],\n",
       "         [-11.2435,   6.0103,   7.8472,     -inf],\n",
       "         [-24.6950,  19.0554,  13.7950,  19.4077]],\n",
       "\n",
       "        [[ -2.2381,     -inf,     -inf,     -inf],\n",
       "         [-16.7659,  17.0041,     -inf,     -inf],\n",
       "         [ -1.3789, -11.9098,   8.3356,     -inf],\n",
       "         [ -7.8504,   7.6658, -10.8747,  12.2220]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b.masked_fill(mask= b==0 , value= float('-inf'))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [9.6683e-07, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [4.4136e-09, 1.3742e-01, 8.6258e-01, 0.0000e+00],\n",
       "          [4.1141e-20, 4.1194e-01, 2.1393e-03, 5.8593e-01]],\n",
       " \n",
       "         [[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [2.1572e-15, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
       "          [6.0400e-05, 1.6126e-09, 9.9994e-01, 0.0000e+00],\n",
       "          [1.8972e-09, 1.0392e-02, 9.2192e-11, 9.8961e-01]]]),\n",
       " torch.Size([2, 4, 4]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_pairwise_sim = F.softmax(input=b, dim=2) # (batch_size, context_length, context_length)\n",
    "normalized_pairwise_sim, normalized_pairwise_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.7733,  0.9400, -4.3096, -2.0681, -3.1198,  0.4575, -4.4294,\n",
       "            4.8361],\n",
       "          [-2.9267,  6.8308,  7.2012,  3.6070,  0.0332,  0.5137,  8.0824,\n",
       "           -3.3107],\n",
       "          [-1.7559,  6.5711,  5.9867,  3.6404, -3.2945, -2.0385,  4.1468,\n",
       "           -3.7486],\n",
       "          [-3.1643,  2.2514,  6.3566,  0.6931, -0.7339, -2.4181,  9.4609,\n",
       "           -4.4570]],\n",
       " \n",
       "         [[-0.5072, -3.3519, -2.0111, -4.3186, -3.0338, -0.0310, -1.7883,\n",
       "            1.1453],\n",
       "          [-0.9018,  0.6899,  4.0113,  1.8252, -1.9704, -2.3205,  2.1763,\n",
       "           -4.1382],\n",
       "          [-2.4032,  9.7410,  3.7773,  1.2813, -7.7708, -3.0452,  3.7339,\n",
       "            0.1291],\n",
       "          [-0.7633, -1.2092,  3.3778,  2.0911,  0.3892, -2.2809,  3.5021,\n",
       "           -3.5093]]]),\n",
       " torch.Size([2, 4, 8]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_embedding = normalized_pairwise_sim @ values # (batch_size, context_length, v_dim)\n",
    "attention_embedding, attention_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.5134,  0.5656, -1.0793, -0.3770, -0.7065,  0.4144, -1.1169,\n",
       "            1.7864],\n",
       "          [-1.1871,  0.9459,  1.0268,  0.2411, -0.5401, -0.4350,  1.2195,\n",
       "           -1.2710],\n",
       "          [-0.6824,  1.2475,  1.1121,  0.5683, -1.0390, -0.7479,  0.6857,\n",
       "           -1.1443],\n",
       "          [-0.8607,  0.2590,  1.1078, -0.0632, -0.3582, -0.7064,  1.7496,\n",
       "           -1.1280]],\n",
       " \n",
       "         [[ 0.6675, -0.8765, -0.1487, -1.4011, -0.7038,  0.9260, -0.0278,\n",
       "            1.5644],\n",
       "          [-0.3027,  0.2825,  1.5036,  0.6999, -0.6955, -0.8242,  0.8290,\n",
       "           -1.4925],\n",
       "          [-0.5820,  1.7102,  0.5845,  0.1134, -1.5952, -0.7032,  0.5763,\n",
       "           -0.1041],\n",
       "          [-0.3699, -0.5412,  1.2208,  0.7265,  0.0728, -0.9528,  1.2685,\n",
       "           -1.4247]]]),\n",
       " torch.Size([2, 4, 8]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_layer_norm = (attention_embedding - attention_embedding.mean(dim=2, keepdim=True) ) / attention_embedding.std(dim=2, keepdim=True)\n",
    "post_layer_norm, post_layer_norm.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, emb_dim:int, qk_dim:int, v_dim:int, masked:bool, drop_out:float) -> None:\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.v_dim = v_dim\n",
    "        self.masked = masked\n",
    "        \n",
    "        self.register_parameter(name='Wq', param=nn.parameter.Parameter(data=torch.randn(emb_dim, qk_dim, requires_grad=True, device=DEVICE)) )\n",
    "        self.register_parameter(name='Wk', param=nn.parameter.Parameter(data=torch.randn(emb_dim, qk_dim, requires_grad=True, device=DEVICE)) )\n",
    "        self.register_parameter(name='Wv', param=nn.parameter.Parameter(data=torch.randn(emb_dim, v_dim, requires_grad=True, device=DEVICE)) )\n",
    "        self.register_buffer(name='tril', tensor=torch.tril(torch.ones(CONTEXT_L, CONTEXT_L)))\n",
    "        self.drop_out = nn.Dropout(p=drop_out)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X shape: (batch_size, context_length, emb_dim)\n",
    "        # self attention : X1 = X2\n",
    "        queries = X @ self.Wq # (batch_size, context_length, qk_dim)\n",
    "        keys = X @ self.Wk # (batch_size, context_length, qk_dim)\n",
    "        values = X @ self.Wv # (batch_size, context_length, v_dim)\n",
    "        scaled_pairwise_sim = queries @ keys.transpose(dim0=1, dim1=2) * (self.qk_dim**-0.5) # (batch_size, context_length, context_length)\n",
    "        if self.masked:\n",
    "            # scaled_pairwise_sim = torch.tril(input=scaled_pairwise_sim, diagonal=0)\n",
    "            scaled_pairwise_sim = scaled_pairwise_sim.masked_fill(mask= (self.tril==0) , value= float('-inf'))\n",
    "        normalized_pairwise_sim = self.drop_out(F.softmax(input=scaled_pairwise_sim, dim=2)) # (batch_size, context_length, context_length)  dim = -1: last dimension\n",
    "        \n",
    "        attention_embedding = normalized_pairwise_sim @ values # (batch_size, context_length, v_dim)\n",
    "        return attention_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, emb_dim, qk_dim, v_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.qk_dim = qk_dim\n",
    "        self.v_dim = v_dim\n",
    "        self.register_parameter(name='Wq', param=nn.parameter.Parameter(data=torch.randn(emb_dim, qk_dim, requires_grad=True, device=DEVICE)) )\n",
    "        self.register_parameter(name='Wk', param=nn.parameter.Parameter(data=torch.randn(emb_dim, qk_dim, requires_grad=True, device=DEVICE)) )\n",
    "        self.register_parameter(name='Wv', param=nn.parameter.Parameter(data=torch.randn(emb_dim, v_dim, requires_grad=True, device=DEVICE)) )\n",
    "        \n",
    "    \n",
    "    def forward(self, X1, X2):\n",
    "        # X shape: (batch_size, context_length, emb_dim)\n",
    "        # self attention : X1 = X2\n",
    "        queries = X1 @ self.Wq # (batch_size, context_length, qk_dim)\n",
    "        keys = X2 @ self.Wk # (batch_size, context_length, qk_dim)\n",
    "        values = X2 @ self.Wv # (batch_size, context_length, v_dim)\n",
    "        pairwise_sim = queries @ keys.transpose(dim0=1, dim1=2) # (batch_size, context_length, context_length)\n",
    "        normalized_pairwise_sim = F.softmax(input=pairwise_sim, dim=1) * (self.qk_dim**-0.5)\n",
    "        attention_embedding = normalized_pairwise_sim @ values # (batch_size, context_length, v_dim)\n",
    "        return attention_embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads:int, emb_dim:int, qk_dim:int, v_dim:int, masked:bool, drop_out:float) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(emb_dim=emb_dim, qk_dim=qk_dim, v_dim=v_dim, masked=masked, drop_out=drop_out) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(in_features=num_heads*v_dim , out_features=emb_dim) # project back to the input dimension (residual connection)\n",
    "        self.deop_out = nn.Dropout(p=drop_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = torch.cat([head(X) for head in self.heads], dim=2) # (batch_size, context_length, num_heads*v_dim)\n",
    "        return self.deop_out( self.proj(out) ) # (batch_size, context_length, emb_dim)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, emb_dim:int, drop_out:float) -> None:\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(nn.Linear(in_features=emb_dim, out_features=4*emb_dim),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(in_features=4*emb_dim, out_features=emb_dim), # projection back to the residual,\n",
    "                                nn.Dropout(p=drop_out)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotifDecoderBlock(nn.Module):\n",
    "    def __init__(self, num_heads:int, emb_dim:int, qk_dim:int, v_dim:int, drop_out:float) -> None:\n",
    "        super().__init__()\n",
    "        # intersperse communication among nodes with multiple blocks stacked on top of each other \n",
    "        self.masked_multi_head_attention = MultiHeadSelfAttention(num_heads=num_heads, emb_dim=emb_dim, qk_dim=qk_dim, v_dim=v_dim, masked=True, drop_out=drop_out)\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=emb_dim)\n",
    "        self.feed_forward = FeedForward(emb_dim=emb_dim, drop_out=drop_out)\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=emb_dim)\n",
    "\n",
    "    def forward(self, x): # pre-norm Formulation\n",
    "        att = x + self.masked_multi_head_attention( self.layer_norm1(x) )\n",
    "        return att + self.feed_forward(att)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_blocks:int, num_heads:int, emb_dim:int, qk_dim:int, v_dim:int, drop_out:float) -> None:\n",
    "        super().__init__()\n",
    "        self.token_embedding_lkt = nn.Embedding(num_embeddings=NCLASS, embedding_dim=emb_dim, device=DEVICE)\n",
    "        self.positional_embedding = nn.Embedding(num_embeddings=CONTEXT_L, embedding_dim=emb_dim, device=DEVICE)\n",
    "        self.motif_blocks = nn.Sequential(*[MotifDecoderBlock(num_heads=num_heads, emb_dim=emb_dim, qk_dim=qk_dim, v_dim=v_dim, drop_out=drop_out) for _ in range(num_blocks)])\n",
    "        self.last_layer_norm = nn.LayerNorm(normalized_shape=emb_dim)\n",
    "        self.last_linear = nn.Linear(in_features=emb_dim, out_features=NCLASS)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, SelfAttention):\n",
    "            torch.nn.init.normal_(module.Wq, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(module.Wk, mean=0.0, std=0.02)\n",
    "            torch.nn.init.normal_(module.Wv, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.shape[-1] < CONTEXT_L: # Generating from 0 context length\n",
    "            # padding 0, (CONTEXT_L - prompt.shape[0]) quantity to the top/left and 0 quantity to the bottom/right the last dimension\n",
    "            x = F.pad(input=x, pad=(CONTEXT_L-x.shape[-1], 0), mode=\"constant\", value=0) # (1 batch, context_length number of scalars/indices)\n",
    "        token_emb = self.token_embedding_lkt(x) # (batch_size, context_length, emb_dim)\n",
    "        pose_emb = self.positional_embedding(torch.arange(end=CONTEXT_L, device=DEVICE)) # (batch_size, context_length, emb_dim)\n",
    "        X = token_emb + pose_emb # (batch_size, context_length, emb_dim)\n",
    "        \n",
    "        X = self.motif_blocks(X) # (batch_size, context_length, emb_dim)\n",
    "        self.last_layer_norm(X) # (batch_size, context_length, emb_dim)\n",
    "        logits = self.last_linear(X) # (batch_size, context_length, n_class)\n",
    "        return logits\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # prompt : (unknown number of scalars/indices)\n",
    "            prompt = prompt.view(1, -1) # (1 batch, unknown number of scalars/indices)\n",
    "            feed = prompt[:, -CONTEXT_L:] # (1 batch, context_length number of scalars/indices)\n",
    "            logits = self(feed) # (1, context_length, n_class)\n",
    "            logits = logits[:, -1: :].view(1, NCLASS) # (1, n_class) , Focus only on the last position\n",
    "            probs = F.softmax(input=logits, dim=-1) # (1, n_class) -1: last dimension\n",
    "            idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "            prompt = torch.cat(tensors=(prompt, idx_next), dim=-1) # (1 batch, 1+unknown number of scalars/indices)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 384\n",
    "NUM_HEADS = 6\n",
    "QK_DIM = EMB_DIM // NUM_HEADS # 64\n",
    "V_DIM = QK_DIM\n",
    "NUM_BLOCKS = 6\n",
    "DROP_OUT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder(\n",
      "  (token_embedding_lkt): Embedding(65, 384)\n",
      "  (positional_embedding): Embedding(256, 384)\n",
      "  (motif_blocks): Sequential(\n",
      "    (0): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): MotifDecoderBlock(\n",
      "      (masked_multi_head_attention): MultiHeadSelfAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (1): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (2): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (3): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (4): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (5): SelfAttention(\n",
      "            (drop_out): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "        (deop_out): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (ff): Sequential(\n",
      "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (last_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "  (last_linear): Linear(in_features=384, out_features=65, bias=True)\n",
      ")\n",
      "10.788929 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = TransformerDecoder(num_blocks= NUM_BLOCKS, num_heads=NUM_HEADS, emb_dim=EMB_DIM,\n",
    "                           qk_dim=QK_DIM, v_dim=V_DIM, drop_out=DROP_OUT)\n",
    "model = model.to(device=DEVICE)\n",
    "print(model)\n",
    "# print the number of parameters in the model\n",
    "NUM_PARAM = sum(p.numel() for p in model.parameters())\n",
    "print(NUM_PARAM/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb, yb = get_batch(stage='train', batch_size=BATCH_SIZE, context_length=CONTEXT_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = model(xb)\n",
    "# loss = F.cross_entropy(input=logits.view(-1, NCLASS), target=yb.view(-1))\n",
    "# print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=torch.ones((1,1), dtype=torch.long, device=DEVICE)\n",
    "# max_new_tokens = CONTEXT_L//2\n",
    "# prompt, prompt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # padding 0, (CONTEXT_L - prompt.shape[1]) quantity to the top/left and 0 quantity to the bottom/right the last dimension\n",
    "# prompt = F.pad(input=prompt, pad=(CONTEXT_L - prompt.shape[1], 0), mode=\"constant\", value=0)\n",
    "# prompt, prompt.shape, prompt.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for _ in range(max_new_tokens):\n",
    "#         # prompt : (unknown number of scalars/indices)\n",
    "#         prompt = prompt.view(1, -1) # (1 batch, unknown number of scalars/indices)\n",
    "#         if prompt.shape[-1] < CONTEXT_L: # Generating from 0 context length\n",
    "#             # padding 0, (CONTEXT_L - prompt.shape[0]) quantity to the top/left and 0 quantity to the bottom/right the last dimension\n",
    "#             feed = F.pad(input=prompt, pad=(CONTEXT_L-prompt.shape[1], 0), mode=\"constant\", value=0) # (1 batch, context_length number of scalars/indices)\n",
    "#         else:\n",
    "#             feed = prompt[:, -CONTEXT_L:] # (1 batch, context_length number of scalars/indices)\n",
    "#         logits = model(feed) # (1, context_length, n_class)\n",
    "#         logits = logits[:, -1: :].view(1, NCLASS) # (1, n_class) , Focus only on the last position\n",
    "#         probs = F.softmax(input=logits, dim=-1) # (1, n_class) -1: last dimension\n",
    "#         idx_next = torch.multinomial(input=probs, num_samples=1)\n",
    "#         prompt = torch.cat(tensors=(prompt, idx_next), dim=-1) # (1 batch, 1+unknown number of scalars/indices)\n",
    "#         # print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = model.generate(prompt=torch.zeros(CONTEXT_L, dtype=torch.long, device=DEVICE), max_new_tokens=CONTEXT_L//2)\n",
    "# decoder(prompt[0].tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mopen_ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mehdi/Data/Education and Migration/MSc EIT/Sweden/KTH/Studies/Semester 2/P4/21 DL for DS/Lab Assignments/KarpathyPyTorchTutorial/wandb/run-20230608_200256-qk82zl3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/open_ai/TinyGPT/runs/qk82zl3o' target=\"_blank\">sweet-sun-10</a></strong> to <a href='https://wandb.ai/open_ai/TinyGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/open_ai/TinyGPT' target=\"_blank\">https://wandb.ai/open_ai/TinyGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/open_ai/TinyGPT/runs/qk82zl3o' target=\"_blank\">https://wandb.ai/open_ai/TinyGPT/runs/qk82zl3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/open_ai/TinyGPT/runs/qk82zl3o?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f3d7c5ab7f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "LR = 3e-4\n",
    "MAX_ITER = 5000\n",
    "EVAL_INTERVAL = 500\n",
    "EVAL_ITER = 200\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"TinyGPT\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"GPT\",\n",
    "    \"num parameters\": NUM_PARAM,\n",
    "    \"dataset\": \"Shakespeare\",\n",
    "    \"Optimizer\": \"AdamW\",\n",
    "    \"emb dim\": EMB_DIM,\n",
    "    \"qk dim\": QK_DIM,\n",
    "    \"v dim\": V_DIM,\n",
    "    \"num heads\": NUM_HEADS,\n",
    "    \"num motif blocks\": NUM_BLOCKS,\n",
    "    \"dropout\": DROP_OUT\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_eval import helper_eval_gpt_ce_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "MAX_ITER = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:       0 /   15685 | mini batch loss: 4.1735 | train loss: 3.7019 | eval loss: 3.7235\n",
      "\n",
      "k$?Ry\n",
      "GLaVdNpCjyVYCBerpXx!ioIGgXGhFW3DWiiByzFeRS;hgrPXWHrmgln\n",
      "O! vs OM&YQCoTGk3s?.lHjvckWm,VwCbpeZYn ;G?zqMsoRKgf& hxReFVeQ?hsU& \n",
      "uo  ncifqBK3msNXsHanw\n",
      "WNDL DpxB i$tszlhnh$YpHwsSq-jrgG ZxEmmSvex : RwqCe&.ajolOfyHDfUptnN3mKeor'hDxxTE,v UWa wYLRptKegkzrFhci \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:     500 /   15685 | mini batch loss: 2.1095 | train loss: 2.0122 | eval loss: 2.0883\n",
      "\n",
      "MFWPPCISETHMAMIA:LUT:PNUSE:WESMPN,CIUBBNIUS:\n",
      "SIUC:\n",
      "P'F hAGLP:'TUCET:CE:\n",
      "I yIUnR:\n",
      "Sem'lwsomONBNEREs:\n",
      "FhTow.\n",
      "JUVST:\n",
      "SefNUS ESARMIZII:\n",
      "Win?\n",
      "DUmBUMERDIULI:\n",
      "t, mudTy bent tru hy shBue,\n",
      "Matelld y 'Tondro peart, sbokef Bunges?\n",
      "I gat thevat! mean curnt, thais; mat\n",
      "iteration:    1000 /   15685 | mini batch loss: 1.7039 | train loss: 1.6033 | eval loss: 1.7853\n",
      "\n",
      "RAFIAMERD:IDMUMO:\n",
      "Sof ild you kase\n",
      "Te need mave in men o onie, sin,\n",
      "And not that not this opraor o't.\n",
      "\n",
      "BAH:\n",
      "How you shalt it, but the love.\n",
      "\n",
      "AGivole of the kight affir, I would nafful.\n",
      "ARGARE:\n",
      "Sir, it, York nearst:\n",
      "Nay, but grady eyes, my may good the sigh\n",
      "iteration:    1500 /   15685 | mini batch loss: 1.5235 | train loss: 1.4364 | eval loss: 1.6405\n",
      "\n",
      "FAFITPALIT Clocktysater,\n",
      "To my lord; I provoster say: let he sied,\n",
      "Eveny fair friends, in the Lord, lord.\n",
      "\n",
      "DUKE OF YORK:\n",
      "No, not, whereinots I\n",
      "By beriphed, and have say my liegers up and\n",
      "Darest to Same! and your Vances,\n",
      "Then side, so I am repetting of no w\n",
      "iteration:    2000 /   15685 | mini batch loss: 1.4819 | train loss: 1.3492 | eval loss: 1.5839\n",
      "\n",
      "FATTEPUERL:MAM:\n",
      "Wow is the virtue will by whoom of with dection,\n",
      "So like thy sofen thy son't divine that shore.\n",
      "\n",
      "Third Citizen:\n",
      "My boWerd take the cair tile you, and reason\n",
      "All a pering to flow you, a man, I will stay.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No, I'll come the en\n",
      "iteration:    2500 /   15685 | mini batch loss: 1.3643 | train loss: 1.2838 | eval loss: 1.5505\n",
      "\n",
      "LIDEETES:\n",
      "VINIO:\n",
      "It she man must, I do duke a service for their worm out\n",
      "To you have goest to relen your lord.\n",
      "\n",
      "PARINA:\n",
      "Poor thee lords, I'll tell the man\n",
      "That shepherd one it birth.\n",
      "\n",
      "PlINCE EDWARD:\n",
      "Tus I unto Plashol.\n",
      "\n",
      "Provost:\n",
      "My lord? know, here bear fr\n",
      "iteration:    3000 /   15685 | mini batch loss: 1.3321 | train loss: 1.2323 | eval loss: 1.5332\n",
      "\n",
      "BAFFORS:\n",
      "Cold Then.\n",
      "\n",
      "SICINIUS:\n",
      "Even a kind and this colours stuburnes.\n",
      "\n",
      "SICINIUS:\n",
      "Pray your consents, you shall betre!\n",
      "\n",
      "SICINIUS:\n",
      "I have been awhile:\n",
      "I cannot leave, I'll dash no more to slow;\n",
      "Here comes that you never wrong to 'em; king,\n",
      "Counsell the moon\n",
      "iteration:    3500 /   15685 | mini batch loss: 1.2635 | train loss: 1.1907 | eval loss: 1.5134\n",
      "\n",
      "MEPETETER:\n",
      "PAULINA:\n",
      "Peace; citizens!\n",
      "\n",
      "First Servingman:\n",
      "Where is no boy! a father, shall the live\n",
      "ronger:\n",
      "No;\n",
      "So shipt's stir.\n",
      "\n",
      "LUCIO:\n",
      "The senate hath been to see the losse to store in.\n",
      "\n",
      "LUCIO:\n",
      "They'll say.\n",
      "\n",
      "All:\n",
      "I leave my sister'd, hold you.\n",
      "\n",
      "ISABELLA:\n",
      "L\n",
      "iteration:    4000 /   15685 | mini batch loss: 1.2475 | train loss: 1.1552 | eval loss: 1.4969\n",
      "\n",
      "ICINIO:\n",
      "IS:\n",
      "I have he an more whone 'tis none than to sweet.\n",
      "\n",
      "PETER:\n",
      "But,\n",
      "And indeed, for I will comple her: where is he\n",
      "will beat him upon him: and though away.\n",
      "\n",
      "COMINIUS:\n",
      "We were a foul invention, being her heart is a\n",
      "corse, and Cominius, to you know, th\n",
      "iteration:    4500 /   15685 | mini batch loss: 1.2267 | train loss: 1.1156 | eval loss: 1.4970\n",
      "\n",
      "GRoAUCOLIO:\n",
      "ISABELLA:\n",
      "A bold deed by her mother.\n",
      "CAMILLO:\n",
      "I am like a charafter.\n",
      "\n",
      "ESCALUS:\n",
      "Well, my lord; you have been enough. A nation bell,\n",
      "As thou art a John and provost, what I'll play,\n",
      "Though I have been more sounding in the queen,\n",
      "But cannot make lo\n",
      "iteration:    4999 /   15685 | mini batch loss: 1.1726 | train loss: 1.0846 | eval loss: 1.5083\n",
      "\n",
      "ClISCINIUS:\n",
      "COMINIUS:\n",
      "I wery tribunes.\n",
      "SICINIUS:\n",
      "Absech your mother, though you that see so purposed,\n",
      "Made you not speak with words, that even company\n",
      "Have level the kindred in arms of seeming:\n",
      "And, though Tart not speak with Rome, or thou shalt so.\n",
      "\n",
      "BRUTU\n"
     ]
    }
   ],
   "source": [
    "batch_args = {'batch_size':BATCH_SIZE, 'context_length':CONTEXT_L}\n",
    "for _ in range(MAX_ITER):\n",
    "    model.train()\n",
    "    xb, yb = get_batch(stage='train', **batch_args)\n",
    "    \n",
    "    logits = model(xb)\n",
    "    mini_batch_loss = F.cross_entropy(input=logits.view(-1, NCLASS), target=yb.view(-1))\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    mini_batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if (iter % EVAL_INTERVAL == 0) or (iter==MAX_ITER-1):\n",
    "            losses = helper_eval_gpt_ce_losses(model=model, get_batch=get_batch, n_class=NCLASS, eval_iter=EVAL_ITER, batch_args=batch_args)\n",
    "            print(f'iteration: {iter:7d} / {TXT_LENGTH//BATCH_SIZE:7d} | mini batch loss: {mini_batch_loss.item():.4f} | train loss: {losses[\"train\"]:.4f} | eval loss: {losses[\"eval\"]:.4f}')\n",
    "            wandb.log({\"mini batch loss\": mini_batch_loss, \"train loss\": losses[\"train\"], \"eval loss\": losses[\"eval\"]})\n",
    "            prompt = model.generate(prompt=torch.zeros((1,1), dtype=torch.long, device=DEVICE), max_new_tokens=CONTEXT_L)\n",
    "            print(decoder(prompt[0].tolist()))\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
