{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"dark\")\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''\n",
    "\n",
    "# # for creating a responsive plot\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "'''\n",
    "To get same results when sampling during different runs.\n",
    "If you are using cuDNN, you should set the deterministic behavior.\n",
    "This might make your code quite slow, but might be a good method to check your code and deactivate it later.\n",
    "'''\n",
    "torch.backends.cudnn.deterministic = True\n",
    "DEVICE = torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & plot helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets takes an input, create a set of all items,\n",
    "# & doesn't allow duplicates :)\n",
    "# then we want a sorted list of course, the order matters! \n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# a map or dict:\n",
    "# start from 1\n",
    "s_to_i = { s:i for i, s in enumerate(chars, start=1)}\n",
    "s_to_i['.'] = 0\n",
    "i_to_s = { i:s for s,i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_heatmap(tensor, text=True, nrow=None, ncol=None, fig_size=(10,10), cmap='Blues', textc='gray'):\n",
    "    if (nrow is None) or (ncol is None):\n",
    "        nrow = tensor.shape[0]\n",
    "        ncol = tensor.shape[1]\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(tensor.detach().numpy(), cmap= cmap)\n",
    "    # manually write text on each cell (seaborn annot doesn't look good)\n",
    "    if text:\n",
    "        for i, j in itertools.product(range(nrow), range(ncol)):\n",
    "            # x:col, y:rows, the origin is top left corner, makes bottom <->top\n",
    "            plt.text(x=j, y=i, s=f'{tensor[i,j].item():.2f}', ha='center', va='center', color=textc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x= emb_lkt[:,0].data, y=emb_lkt[:,1].data, s=200)\n",
    "    for i in range(nclass):\n",
    "        plt.text(x=emb_lkt[i,0].item(), y=emb_lkt[i,1].item(), s=i_to_s[i], ha='center', va='center', color='white')\n",
    "    plt.grid('minor')\n",
    "\n",
    "# def plot_3d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "#     tensor = emb_lkt.data.detach().numpy()\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     ax = Axes3D(fig)\n",
    "#     ax.scatter(xs= tensor[:,0], ys=tensor[:,1], zs=tensor[:,2], s=200)\n",
    "#     for i in range(nclass):\n",
    "#         ax.text(x=tensor[i,0], y=tensor[i,1],z=tensor[i,2], s=i_to_s[i], ha='center', va='center', color='white')\n",
    "#     # displaying the plot\n",
    "#     plt.grid('minor')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr: xs.shape = torch.Size([182625, 5])\n",
      "Dev: xs.shape = torch.Size([22655, 5])\n",
      "Ts: xs.shape = torch.Size([22866, 5])\n",
      "TR_SIZE = 182625\n",
      "WINDOW_SIZE = 5\n",
      "NCLASS = 27\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "NCLASS = 27\n",
    "\n",
    "def build_dataset(words:list, type:str):\n",
    "    xs, ys = [], []\n",
    "\n",
    "    # context input window: how many characters do we take to predict the next one?\n",
    "    \n",
    "\n",
    "    for w in words:\n",
    "        context_window = [0]*WINDOW_SIZE\n",
    "        for ch in f'{w}.':\n",
    "            idx_y = s_to_i[ch]\n",
    "            ys.append(idx_y)\n",
    "\n",
    "            xs.append(context_window) # input : ch1\n",
    "            # print(''.join(i_to_s[i] for i in context_window) + f' ---> {ch}' )\n",
    "\n",
    "            # shift_to_left context window and append the idx_y\n",
    "            context_window = context_window[1:] + [idx_y]\n",
    "    xs = torch.tensor(xs, device= DEVICE) # device= 'cuda' \n",
    "    # ys = torch.Tensor(ys)\n",
    "    ys = torch.tensor(ys, device= DEVICE)\n",
    "    print(f'{type}: {xs.shape = }')\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n80 = int( 0.8*len(words) )\n",
    "n90 = int( 0.9*len(words) )\n",
    "Xtr, Ytr = build_dataset(words[:n80], 'Tr')\n",
    "Xdev, Ydev = build_dataset(words[n80:n90], 'Dev')\n",
    "Xts, Yts = build_dataset(words[n90:], 'Ts')\n",
    "TR_SIZE = Xtr.shape[0]\n",
    "DEV_SIZE = Xdev.shape[0]\n",
    "TS_SIZE = Xts.shape[0]\n",
    "print(f'{TR_SIZE = }')\n",
    "print(f'{WINDOW_SIZE = }')\n",
    "print(f'{NCLASS = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backpropagation manually for intuitive understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    def __init__(self, fan_in, fan_out, bias=True) -> None:\n",
    "        # initialization\n",
    "        # self.w_gain = w_gain\n",
    "        self.W = torch.randn(fan_in, fan_out, device= DEVICE)*0.01 # / fan_in**0.5\n",
    "        self.b = torch.randn(1, fan_out, device= DEVICE)*0.01 if bias else None\n",
    "    \n",
    "    # Forward\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        self.out = self.x @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out += self.b\n",
    "        return self.out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def backward(self, d_out):\n",
    "        d_in = (d_out @ self.W.T) # BATCH_SIZE , fan_in\n",
    "        d_W = (self.x.T @ d_out) # fan_in , fan_out\n",
    "        d_b = d_out.sum(dim=0, keepdim=True) # 1, fan_out\n",
    "        return d_in, d_W, d_b\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W] + ( [] if self.b is None else [self.b] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BatchNorm Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d():\n",
    "    def __init__(self, fan_out, eps=1e-5, momentum=0.1, training=True) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # flexible Gaussian preact\n",
    "        self.bn_gain = torch.ones(fan_out, device= DEVICE)\n",
    "        self.bn_bias = torch.zeros(fan_out, device= DEVICE)\n",
    "\n",
    "        self.training = training\n",
    "\n",
    "        # Buffers (in PyTorch nomenclature)\n",
    "        # ema of mean and std\n",
    "        self.bn_mean_ema = torch.zeros(fan_out, device= DEVICE)\n",
    "        self.bn_var_ema = torch.ones(fan_out, device= DEVICE)\n",
    "    \n",
    "    # Forward\n",
    "    def __call__(self, x, eps=1e-5):\n",
    "        self.batch_size = x.shape[0]\n",
    "        if self.training:\n",
    "            self.bn_mean = x.mean(dim=0, keepdim=True) # 1, HLAYER_SIZE\n",
    "            self.bn_var = x.var(dim=0, keepdim=True, unbiased=True)\n",
    "            self.bn_std_inv = (self.bn_var + eps)**-0.5\n",
    "            self.bn_raw = (x - self.bn_mean) * self.bn_std_inv\n",
    "            self.out = self.bn_bias + self.bn_gain * self.bn_raw\n",
    "\n",
    "            # update moving stats\n",
    "            with torch.no_grad():\n",
    "                self.bn_mean_ema = (1 - self.momentum) * self.bn_mean_ema + self.momentum * self.bn_mean\n",
    "                self.bn_var_ema = (1 - self.momentum) * self.bn_var_ema + self.momentum * self.bn_var\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                self.out = self.bn_bias + self.bn_gain * (x - self.bn_mean_ema) * (self.bn_var_ema + eps)**-0.5\n",
    "        return self.out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def backward(self, d_out):\n",
    "        d_bn_bias = d_out.sum(dim=0, keepdim=True)\n",
    "        d_bn_gain = (d_out * self.bn_raw).sum(dim=0, keepdim=True)\n",
    "        d_bn_raw = d_out * self.bn_gain\n",
    "        d_hprebn = (1/self.batch_size) * self.bn_std_inv * ( self.batch_size * d_bn_raw - d_bn_raw.sum(dim=0, keepdim=True) - self.bn_raw * (self.batch_size/(self.batch_size-1)) * (d_bn_raw*self.bn_raw).sum(dim=0, keepdim=True) )\n",
    "        return d_hprebn, d_bn_raw, d_bn_gain, d_bn_bias\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.bn_gain, self.bn_bias]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCrossEntropy():\n",
    "    def __init__(self) -> None:\n",
    "        self.qs = 0.0\n",
    "    \n",
    "    def __call__(self, logits, Yb):\n",
    "        self.Yb = Yb\n",
    "        self.logits = logits\n",
    "        logits_max = logits.max(dim=1, keepdim=True).values # [BATCH_SIZE, 1]\n",
    "        batch_size = logits_max.shape[0]\n",
    "        # subtract max for numerical stability,\n",
    "        # has no effect on qs bc of normalization in counts,\n",
    "        # expect logits_max to have zero gradients as well\n",
    "        norm_logits = logits - logits_max\n",
    "        #[BATCH_SIZE, NCLASS]\n",
    "\n",
    "        counts = norm_logits.exp() # [BATCH_SIZE, NCLASS]\n",
    "        counts_sum = counts.sum(dim=1, keepdim=True) # [BATCH_SIZE, 1]\n",
    "\n",
    "        # for division use **-1 instead of \"/\", PyTorch backward pass seems to give real numbers for the later\n",
    "        counts_sum_inv = counts_sum**-1 # [BATCH_SIZE, 1]\n",
    "\n",
    "        # q: softmax or model prediction distribution\n",
    "        # p: true empirical distribution -> p(correct label) = 1 o.w. p=0\n",
    "        self.qs = counts * counts_sum_inv # [BATCH_SIZE, NCLASS]\n",
    "        Nlog_qs = -self.qs.log() # [BATCH_SIZE, NCLASS]\n",
    "        # [BATCH_SIZE, 1]\n",
    "        # Correct class\n",
    "        cc_Nlog_qs = Nlog_qs[torch.arange(batch_size), self.Yb]\n",
    "        # Expected cc_Nlog_qs\n",
    "        return cc_Nlog_qs.mean()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def backward(self):\n",
    "        # Manual Backprop\n",
    "        d_logits = self.qs\n",
    "        batch_size = self.logits.shape[0]\n",
    "        d_logits[torch.arange(batch_size), self.Yb] -= 1.0\n",
    "        ''' non-shortcutted gradients:\n",
    "        gradient of batch mean : replication of 1/BATCH_SIZE\n",
    "        '''\n",
    "        # mean in the forward pass -> replication in the backward\n",
    "        d_logits /= batch_size\n",
    "        return d_logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple model with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_DIM = 6\n",
    "HLAYER_SIZE = 200\n",
    "\n",
    "emb_lkt = torch.randn( NCLASS, EMB_DIM, device= DEVICE)\n",
    "\n",
    "lin1 = Linear(fan_in=WINDOW_SIZE*EMB_DIM, fan_out=NCLASS, bias=True)\n",
    "# layers = [lin1]\n",
    "cross_entropy_loss = MyCrossEntropy()\n",
    "\n",
    "parameters = [emb_lkt] + lin1.parameters()\n",
    "# print(lin1.parameters())\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad = None\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyCrossEntropy' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m batch_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 53\u001b[0m     d_logits \u001b[39m=\u001b[39m cross_entropy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     55\u001b[0m     d_batch_emb_cat, d_W, d_b \u001b[39m=\u001b[39m lin1\u001b[39m.\u001b[39mbackward(d_logits)\n\u001b[1;32m     57\u001b[0m     d_batch_emb \u001b[39m=\u001b[39m d_batch_emb_cat\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, WINDOW_SIZE, EMB_DIM )\n",
      "File \u001b[0;32m~/anaconda3/envs/torch113_1/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[103], line 36\u001b[0m, in \u001b[0;36mMyCrossEntropy.backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     34\u001b[0m     \u001b[39m# Manual Backprop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     d_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqs\n\u001b[0;32m---> 36\u001b[0m     batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m     d_logits[torch\u001b[39m.\u001b[39marange(batch_size), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mYb] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[39m''' non-shortcutted gradients:\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    gradient of batch mean : replication of 1/BATCH_SIZE\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MyCrossEntropy' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 32\n",
    "NSTEPS = 100000\n",
    "\n",
    "# we don't need log loss anymore bc we don't have\n",
    "# that hockey stick shape anymore\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "# just to prevent division by zero, in case the bn_std = 0\n",
    "DIVISION0 = 1e-5\n",
    "# momentum for moving average\n",
    "# the lower the BATCH_SIZE the lower the MOMENTUM!!\n",
    "MOMENTUM = 0.001\n",
    "# with torch.no_grad():\n",
    "for iter in range(NSTEPS):\n",
    "    \n",
    "    batch_upd_param_ratio = []\n",
    "    \n",
    "    # Forward path\n",
    "\n",
    "    # cinstruct a mini batch \n",
    "    mini_batch_idx = torch.randint(low=0, high=TR_SIZE, size=(BATCH_SIZE,), device= DEVICE)\n",
    "    Xb = Xtr[mini_batch_idx]\n",
    "    Yb = Ytr[mini_batch_idx]\n",
    "    # transform it to embeddings\n",
    "    batch_emb = emb_lkt[Xb] # BATCH_SIZE, WINDOW_SIZE, emb_dim\n",
    "    x = batch_emb.view(BATCH_SIZE, WINDOW_SIZE*EMB_DIM)\n",
    "    \n",
    "    # Forward pass\n",
    "    # for layer in layers:\n",
    "    # x = \n",
    "\n",
    "    logits = lin1(x) # * log counts [BATCH_SIZE, NCLASS]\n",
    "\n",
    "    # keep track of grads for DEBUGGING\n",
    "    lin1.out.retain_grad() # AFTER DEDBUG: would take out retain_grad\n",
    "\n",
    "    # print(logits)\n",
    "    # batch_loss = cross_entropy_loss(logits=logits, Yb=Yb)\n",
    "    batch_loss = F.cross_entropy(logits, Yb)\n",
    "    # print(batch_loss)\n",
    "\n",
    "    lossi.append(batch_loss.log10().item())\n",
    "    stepi.append(iter)\n",
    "\n",
    "    # PyTorch backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "\n",
    "    batch_loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        d_logits = cross_entropy_loss.backward()\n",
    "        \n",
    "        d_batch_emb_cat, d_W, d_b = lin1.backward(d_logits)\n",
    "\n",
    "        d_batch_emb = d_batch_emb_cat.view(-1, WINDOW_SIZE, EMB_DIM )\n",
    "        d_emb_lkt = torch.zeros_like(emb_lkt, device= DEVICE)\n",
    "        # the gradient for those characters who were present in the batch\n",
    "        # accumulate multiple occurrence\n",
    "        for i,j in itertools.product(range(BATCH_SIZE), range(WINDOW_SIZE)):\n",
    "            idx = Xb[i,j] # idx: 0-26\n",
    "            d_emb_lkt[idx] += d_batch_emb[i,j]\n",
    "        \n",
    "\n",
    "        grads = [d_emb_lkt, d_W, d_b]#, d_W2, d_b2, d_bn_gain , d_bn_bias]\n",
    "        \n",
    "        # update Manually\n",
    "        lr = 0.1 if iter < 100000 else 0.01\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data -= lr * grad\n",
    "\n",
    "    if (iter+1) % 10000 == 0:\n",
    "        print(f'iteration: {iter:7d} / {NSTEPS:7d} | mini loss: {batch_loss.item():.4f}')\n",
    "    \n",
    "    break # intentionally added, AFTER DEBUG, would take out obviously to run full optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def cmp(var_name, d_t, t):\n",
    "    # check if exactly equal\n",
    "    ex = torch.all(d_t == t.grad).item()\n",
    "    # bc of floating point arithmetic we might get a little bit different result\n",
    "    app = torch.allclose(input=d_t, other=t.grad, atol=1e-5, rtol=1e-8)\n",
    "    maxdiff = (d_t - t.grad).abs().max().item()\n",
    "    print(f'{var_name:15s} | exact: {str(ex):5s} | appoximate: {str(app):5s} | maxx diff : {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\tprint(f'{logits.shape=}')\n",
    "\tprint(f'{d_logits.shape=}')\n",
    "\tcmp('logits', d_t=d_logits, t=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\tprint(f'{emb_lkt.shape=}')\n",
    "\tprint(f'{d_emb_lkt.shape=}')\n",
    "\tcmp('emb_lkt', d_t=d_emb_lkt, t=emb_lkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(ds_type, with_batchN = False): # ds_type: dataset type\n",
    "    X,Y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'dev': (Xdev, Ydev),\n",
    "        'test': (Xts, Yts),\n",
    "    }[ds_type]\n",
    "\n",
    "    emb = emb_lkt[X] # BATCH_SIZE, WINDOW_SIZE, emb_dim\n",
    "    x = emb.view(-1, WINDOW_SIZE*EMB_DIM)\n",
    "    \n",
    "    for layer in layers:\n",
    "        if isinstance(layer, BatchNorm1d):\n",
    "            layer.training = False\n",
    "    \n",
    "    # Forward pass\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    loss = cross_entropy_loss(x, Y)\n",
    "    print(f'{ds_type} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.2888076305389404\n",
      "dev loss: 2.2876508235931396\n"
     ]
    }
   ],
   "source": [
    "evaluate_loss('train')\n",
    "evaluate_loss('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Generate some samples like Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mina\n",
      "tyanne\n",
      "vistyann\n",
      "jonila\n",
      "raha\n",
      "maryelyani\n",
      "ardan\n",
      "kamariana\n",
      "gilline\n",
      "jaquantreo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = torch.Generator().manual_seed(2147483647, device= DEVICE)\n",
    "for _ in range(10):\n",
    "    idx_y = 0\n",
    "    name = ''\n",
    "    context_window = [0]*WINDOW_SIZE\n",
    "    \n",
    "    while True:\n",
    "        # Forward path\n",
    "        emb = emb_lkt[torch.tensor([context_window], device= DEVICE)] # BATCH_SIZE=1, WINDOW_SIZE, emb_dim\n",
    "        \n",
    "        hpreact = emb.view(-1, WINDOW_SIZE*EMB_DIM)@W1 #+ b1\n",
    "        \n",
    "        hpreact = bn_bias + bn_gain * (hpreact - bn_mean_ema) / bn_std_ema\n",
    "\n",
    "        h = torch.tanh(hpreact)\n",
    "\n",
    "        logits = h @ W2 + b2 # * log counts [BATCH_SIZE, NCLASS]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        idx_y = torch.multinomial(probs.detach(), num_samples=1, replacement=True, generator=g, device= DEVICE).item()\n",
    "        # shift_to_left context window and append the idx_y\n",
    "        context_window = context_window[1:] + [idx_y]\n",
    "        if idx_y == 0:\n",
    "            break\n",
    "        ch = i_to_s[idx_y]\n",
    "        # print(ch)\n",
    "        name += ch\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd31d78f929449d598c868a4ab28a111e30d94109ecabb7e57cae0b5fb90da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
