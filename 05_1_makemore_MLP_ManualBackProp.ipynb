{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"dark\")\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''\n",
    "\n",
    "# # for creating a responsive plot\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "'''\n",
    "To get same results when sampling during different runs.\n",
    "If you are using cuDNN, you should set the deterministic behavior.\n",
    "This might make your code quite slow, but might be a good method to check your code and deactivate it later.\n",
    "'''\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets takes an input, create a set of all items,\n",
    "# & doesn't allow duplicates :)\n",
    "# then we want a sorted list of course, the order matters! \n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# a map or dict:\n",
    "# start from 1\n",
    "s_to_i = { s:i for i, s in enumerate(chars, start=1)}\n",
    "s_to_i['.'] = 0\n",
    "i_to_s = { i:s for s,i in s_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_heatmap(tensor, text=True, nrow=None, ncol=None, fig_size=(10,10), cmap='Blues', textc='gray'):\n",
    "    if (nrow is None) or (ncol is None):\n",
    "        nrow = tensor.shape[0]\n",
    "        ncol = tensor.shape[1]\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(tensor.detach().numpy(), cmap= cmap)\n",
    "    # manually write text on each cell (seaborn annot doesn't look good)\n",
    "    if text:\n",
    "        for i, j in itertools.product(range(nrow), range(ncol)):\n",
    "            # x:col, y:rows, the origin is top left corner, makes bottom <->top\n",
    "            plt.text(x=j, y=i, s=f'{tensor[i,j].item():.2f}', ha='center', va='center', color=textc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x= emb_lkt[:,0].data, y=emb_lkt[:,1].data, s=200)\n",
    "    for i in range(nclass):\n",
    "        plt.text(x=emb_lkt[i,0].item(), y=emb_lkt[i,1].item(), s=i_to_s[i], ha='center', va='center', color='white')\n",
    "    plt.grid('minor')\n",
    "\n",
    "# def plot_3d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "#     tensor = emb_lkt.data.detach().numpy()\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     ax = Axes3D(fig)\n",
    "#     ax.scatter(xs= tensor[:,0], ys=tensor[:,1], zs=tensor[:,2], s=200)\n",
    "#     for i in range(nclass):\n",
    "#         ax.text(x=tensor[i,0], y=tensor[i,1],z=tensor[i,2], s=i_to_s[i], ha='center', va='center', color='white')\n",
    "#     # displaying the plot\n",
    "#     plt.grid('minor')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tr: xs.shape = torch.Size([182625, 5]) , ys.shape = torch.Size([182625])\n",
      "Dev: xs.shape = torch.Size([22655, 5]) , ys.shape = torch.Size([22655])\n",
      "Ts: xs.shape = torch.Size([22866, 5]) , ys.shape = torch.Size([22866])\n",
      "TR_SIZE = 182625\n",
      "WINDOW_SIZE = 5\n",
      "NCLASS = 27\n"
     ]
    }
   ],
   "source": [
    "WINDOW_SIZE = 5\n",
    "NCLASS = 27\n",
    "\n",
    "def build_dataset(words:list, type:str):\n",
    "    xs, ys = [], []\n",
    "\n",
    "    # context input window: how many characters do we take to predict the next one?\n",
    "    \n",
    "\n",
    "    for w in words:\n",
    "        context_window = [0]*WINDOW_SIZE\n",
    "        for ch in f'{w}.':\n",
    "            idx_y = s_to_i[ch]\n",
    "            ys.append(idx_y)\n",
    "\n",
    "            xs.append(context_window) # input : ch1\n",
    "            # print(''.join(i_to_s[i] for i in context_window) + f' ---> {ch}' )\n",
    "\n",
    "            # shift_to_left context window and append the idx_y\n",
    "            context_window = context_window[1:] + [idx_y]\n",
    "    xs = torch.tensor(xs)\n",
    "    # ys = torch.Tensor(ys)\n",
    "    ys = torch.tensor(ys)\n",
    "    print(f'{type}: {xs.shape = } , {ys.shape = }')\n",
    "    return xs, ys\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n80 = int( 0.8*len(words) )\n",
    "n90 = int( 0.9*len(words) )\n",
    "Xtr, Ytr = build_dataset(words[:n80], 'Tr')\n",
    "Xdev, Ydev = build_dataset(words[n80:n90], 'Dev')\n",
    "Xts, Yts = build_dataset(words[n90:], 'Ts')\n",
    "TR_SIZE = Xtr.shape[0]\n",
    "DEV_SIZE = Xdev.shape[0]\n",
    "TS_SIZE = Xts.shape[0]\n",
    "print(f'{TR_SIZE = }')\n",
    "print(f'{WINDOW_SIZE = }')\n",
    "print(f'{NCLASS = }')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Backpropagation manually for intuitive understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple model with 1 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18189"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMB_DIM = 6\n",
    "HLAYER_SIZE = 200\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "emb_lkt = torch.randn( NCLASS, EMB_DIM, generator=g)\n",
    "\n",
    "# Layer 1 : linear\n",
    "W1 = torch.randn( WINDOW_SIZE*EMB_DIM, HLAYER_SIZE , generator=g) * 5 / (3*(WINDOW_SIZE*EMB_DIM)**0.5)\n",
    "b1 = torch.randn( 1, HLAYER_SIZE, generator=g) * 0.1 # just for fun\n",
    "\n",
    "# Layer 2 : Batch Norm\n",
    "bn_gain = torch.ones(1, HLAYER_SIZE) * 0.1 + 1.0\n",
    "bn_bias = torch.zeros(1, HLAYER_SIZE) * 0.1\n",
    "# these are not model params, we update them recursively\n",
    "bn_mean_ema = torch.zeros(1, HLAYER_SIZE)\n",
    "bn_std_ema = torch.ones(1, HLAYER_SIZE)\n",
    "\n",
    "\n",
    "# Layer 3 : Linear\n",
    "W2 = torch.randn( HLAYER_SIZE, NCLASS, generator=g) * 0.1\n",
    "b2 = torch.randn( 1, NCLASS, generator=g) * 0.1\n",
    "\n",
    "parameters = [emb_lkt, W1, b1, W2, b2, bn_gain , bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    p.grad = None\n",
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:       0 /  100000 | mini loss: 4.1192\n",
      "iteration:   10000 /  100000 | mini loss: 2.5748\n",
      "iteration:   20000 /  100000 | mini loss: 2.3034\n",
      "iteration:   30000 /  100000 | mini loss: 2.3385\n",
      "iteration:   40000 /  100000 | mini loss: 2.2961\n",
      "iteration:   50000 /  100000 | mini loss: 1.7824\n",
      "iteration:   60000 /  100000 | mini loss: 2.0732\n",
      "iteration:   70000 /  100000 | mini loss: 2.0931\n",
      "iteration:   80000 /  100000 | mini loss: 2.5569\n",
      "iteration:   90000 /  100000 | mini loss: 2.5152\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "BATCH_SIZE = 32\n",
    "NSTEPS = 100000\n",
    "\n",
    "# we don't need log loss anymore bc we don't have\n",
    "# that hockey stick shape anymore\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "# just to prevent division by zero, in case the bn_std = 0\n",
    "DIVISION0 = 1e-5\n",
    "# momentum for moving average\n",
    "# the lower the BATCH_SIZE the lower the MOMENTUM!!\n",
    "MOMENTUM = 0.001\n",
    "with torch.no_grad():\n",
    "    for iter in range(NSTEPS):\n",
    "            \n",
    "        # Forward path\n",
    "\n",
    "        # cinstruct a mini batch \n",
    "        mini_batch_idx = torch.randint(low=0, high=TR_SIZE, size=(BATCH_SIZE,))\n",
    "        Xb = Xtr[mini_batch_idx] # [BATCH_SIZE, WINDOW_SIZE]\n",
    "        Yb = Ytr[mini_batch_idx] # [BATCH_SIZE]\n",
    "        # transform it to embeddings\n",
    "        batch_emb = emb_lkt[Xb] # BATCH_SIZE, WINDOW_SIZE, emb_dim\n",
    "        batch_emb_cat = batch_emb.view(BATCH_SIZE, WINDOW_SIZE*EMB_DIM)\n",
    "\n",
    "        # layer 1 : linear\n",
    "        hprebn = batch_emb_cat@W1 + b1\n",
    "\n",
    "        # layer 2: batch norm\n",
    "        bn_meani = hprebn.mean(dim=0, keepdim=True)\n",
    "        # note: Bessel's correction for computing Variance\n",
    "        # dividing by (BATCH_SIZE-1) instead of BATCH_SIZE\n",
    "        bn_vari = hprebn.var(dim=0, keepdim=True, unbiased=True)\n",
    "        bn_std_inv = (bn_vari + DIVISION0)**-0.5\n",
    "\n",
    "        bn_raw = bn_std_inv * (hprebn - bn_meani)\n",
    "\n",
    "        hpreact = bn_bias + bn_gain * bn_raw\n",
    "\n",
    "        # pass the training set through\n",
    "        \n",
    "        bn_mean_ema = (1-MOMENTUM) * bn_mean_ema + MOMENTUM * bn_meani\n",
    "        bn_std_ema = (1-MOMENTUM) * bn_std_ema + MOMENTUM * bn_std_inv**-1\n",
    "\n",
    "        h = torch.tanh(hpreact) # BATCH_SIZE, HLAYER_SIZE\n",
    "\n",
    "        logits = h @ W2 + b2 # * log counts [BATCH_SIZE, NCLASS]\n",
    "\n",
    "        loss_mini = F.cross_entropy(logits, Yb)\n",
    "\n",
    "        lossi.append(loss_mini.log10().item())\n",
    "        stepi.append(i)\n",
    "\n",
    "        # PyTorch backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        # loss_mini.backward()\n",
    "\n",
    "        # Manual Backprop\n",
    "        d_logits = F.softmax(logits, dim=1)\n",
    "        d_logits[torch.arange(BATCH_SIZE), Yb] -= 1.0\n",
    "        ''' unshortcutted gradients:\n",
    "        gradient of batch mean : replication of 1/BATCH_SIZE\n",
    "        '''\n",
    "        # mean in the forward pass -> replication in the backward\n",
    "        d_logits /= BATCH_SIZE\n",
    "\n",
    "        d_h = d_logits @ W2.T # BATCH_SIZE , HLAYER_SIZE\n",
    "        d_W2 = h.T @ d_logits # HLAYER_SIZE , NCLASS\n",
    "        d_b2 = d_logits.sum(dim=0, keepdim=True)\n",
    "\n",
    "        d_hpreact = (1.0 - h**2) * d_h\n",
    "\n",
    "        d_bn_bias = d_hpreact.sum(dim=0, keepdim=True)\n",
    "        d_bn_gain = (d_hpreact * bn_raw).sum(dim=0, keepdim=True)\n",
    "        d_bn_raw = d_hpreact * bn_gain\n",
    "        d_hprebn = (1/BATCH_SIZE) * bn_std_inv * ( BATCH_SIZE * d_bn_raw - d_bn_raw.sum(dim=0, keepdim=True) - bn_raw * (BATCH_SIZE/(BATCH_SIZE-1)) * (d_bn_raw*bn_raw).sum(dim=0, keepdim=True) )\n",
    "        \n",
    "        d_batch_emb_cat = d_hprebn @ W1.T # BATCH_SIZE , Window*emb\n",
    "        d_W1 = batch_emb_cat.T @ d_hprebn # window*emb , HLAYER_SIZE\n",
    "        # broadcast in forward -> vector sum in backward\n",
    "        d_b1 = d_hprebn.sum(dim=0, keepdim=True)\n",
    "\n",
    "        d_batch_emb = d_batch_emb_cat.view(-1, WINDOW_SIZE, EMB_DIM )\n",
    "        d_emb_lkt = torch.zeros_like(emb_lkt)\n",
    "        # the gradient for those characters who were present in the batch\n",
    "        # accumulate multiple occurrence\n",
    "        for i,j in itertools.product(range(BATCH_SIZE), range(WINDOW_SIZE)):\n",
    "            idx = Xb[i,j] # idx: 0-26\n",
    "            d_emb_lkt[idx] += d_batch_emb[i,j]\n",
    "        \n",
    "\n",
    "        grads = [d_emb_lkt, d_W1, d_b1, d_W2, d_b2, d_bn_gain , d_bn_bias]\n",
    "        \n",
    "        # update Manually\n",
    "        for p, grad in zip(parameters, grads):\n",
    "            p.data -= lr * grad\n",
    "\n",
    "        if iter % 10000 == 0:\n",
    "            print(f'iteration: {iter:7d} / {NSTEPS:7d} | mini loss: {loss_mini.item():.4f}')\n",
    "        \n",
    "        # break # intentionally added, AFTER DEBUG, would take out obviously to run full optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_loss(ds_type): # ds_type: dataset type\n",
    "    X,Y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'dev': (Xdev, Ydev),\n",
    "        'test': (Xts, Yts),\n",
    "    }[ds_type]\n",
    "\n",
    "    emb = emb_lkt[X] # BATCH_SIZE, WINDOW_SIZE, emb_dim\n",
    "    batch_emb_cat = emb.view(-1, WINDOW_SIZE*EMB_DIM)\n",
    "    \n",
    "    # layer 1 : linear\n",
    "    hprebn = batch_emb_cat@W1 + b1\n",
    "\n",
    "    # layer 2: batch norm\n",
    "    hpreact = bn_bias + bn_gain * (hprebn - bn_mean_ema)/ bn_std_ema\n",
    "\n",
    "    h = torch.tanh(hpreact) # BATCH_SIZE, HLAYER_SIZE\n",
    "\n",
    "    logits = h @ W2 + b2 # * log counts [BATCH_SIZE, NCLASS]\n",
    "    \n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(f'{ds_type} loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.077568769454956\n",
      "dev loss: 2.1396939754486084\n"
     ]
    }
   ],
   "source": [
    "evaluate_loss('train')\n",
    "evaluate_loss('dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Generate some samples like Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mina\n",
      "tyanne\n",
      "vistyann\n",
      "jonila\n",
      "raha\n",
      "maryelyani\n",
      "ardan\n",
      "kamariana\n",
      "gilline\n",
      "jaquantreo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "for _ in range(10):\n",
    "    idx_y = 0\n",
    "    name = ''\n",
    "    context_window = [0]*WINDOW_SIZE\n",
    "    \n",
    "    while True:\n",
    "        # Forward path\n",
    "        emb = emb_lkt[torch.tensor([context_window])] # BATCH_SIZE=1, WINDOW_SIZE, emb_dim\n",
    "        \n",
    "        hpreact = emb.view(-1, WINDOW_SIZE*EMB_DIM)@W1 #+ b1\n",
    "        \n",
    "        hpreact = bn_bias + bn_gain * (hpreact - bn_mean_ema) / bn_std_ema\n",
    "\n",
    "        h = torch.tanh(hpreact)\n",
    "\n",
    "        logits = h @ W2 + b2 # * log counts [BATCH_SIZE, NCLASS]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        idx_y = torch.multinomial(probs.detach(), num_samples=1, replacement=True, generator=g).item()\n",
    "        # shift_to_left context window and append the idx_y\n",
    "        context_window = context_window[1:] + [idx_y]\n",
    "        if idx_y == 0:\n",
    "            break\n",
    "        ch = i_to_s[idx_y]\n",
    "        # print(ch)\n",
    "        name += ch\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfd31d78f929449d598c868a4ab28a111e30d94109ecabb7e57cae0b5fb90da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
