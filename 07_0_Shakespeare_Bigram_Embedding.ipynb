{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"dark\")\n",
    "%matplotlib inline\n",
    "''' %matplotlib inline sets the backend of matplotlib to\n",
    "the 'inline' backend. When using the 'inline' backend,\n",
    "your matplotlib graphs will be included in your notebook,\n",
    "next to the code.'''\n",
    "\n",
    "# # for creating a responsive plot\n",
    "# %matplotlib ipympl\n",
    "# %matplotlib widget\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'cuda' # 'cuda' , 'cpu'\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data & Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique chars of the text by set()\n",
    "chars = sorted(list(set(text)))\n",
    "NCLASS = len(chars)\n",
    "print(''.join(chars))\n",
    "NCLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "# encoder tokenizer\n",
    "ch_to_i = { ch:i for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [ ch_to_i[ch] for ch in s ]\n",
    "\n",
    "# decoder tokenizer\n",
    "i_to_ch = { i:ch for ch,i in ch_to_i.items()}\n",
    "decoder = lambda si: ''.join([i_to_ch[i] for i in si])\n",
    "\n",
    "print(encode('hi there!'))\n",
    "print( decoder(encode('hi there!')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Google use Sentence Piece for tokenization.\n",
    "\n",
    "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n",
    "\n",
    "https://github.com/google/sentencepiece\n",
    "\n",
    "2. tiktoken is a fast BPE tokeniser for use with OpenAI's models.\n",
    "\n",
    "https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "enc_data = torch.tensor(encode(text), dtype=torch.long, device=DEVICE)\n",
    "print(enc_data.shape, enc_data.dtype)\n",
    "print(enc_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap and Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_heatmap(tensor, text=True, nrow=None, ncol=None, fig_size=(10,10)):\n",
    "    if (nrow is None) or (ncol is None):\n",
    "        nrow = tensor.shape[0]\n",
    "        ncol = tensor.shape[1]\n",
    "    plt.figure(figsize=fig_size)\n",
    "    plt.imshow(tensor.detach().numpy(), cmap= 'Blues')\n",
    "    # manually write text on each cell (seaborn annot doesn't look good)\n",
    "    if text:\n",
    "        for i, j in itertools.product(range(nrow), range(ncol)):\n",
    "            # x:col, y:rows, the origin is top left corner, makes bottom <->top\n",
    "            plt.text(x=j, y=i, s=f'{tensor[i,j].item():.2f}', ha='center', va='center', color='grey')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D & 3d Embedding Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.scatter(x= emb_lkt[:,0].data, y=emb_lkt[:,1].data, s=200)\n",
    "    for i in range(nclass):\n",
    "        plt.text(x=emb_lkt[i,0].item(), y=emb_lkt[i,1].item(), s=i_to_ch[i], ha='center', va='center', color='white')\n",
    "    plt.grid('minor')\n",
    "\n",
    "# def plot_3d_emb(emb_lkt, nclass, figsize=(8,8)):\n",
    "#     tensor = emb_lkt.data.detach().numpy()\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     ax = Axes3D(fig)\n",
    "#     ax.scatter(xs= tensor[:,0], ys=tensor[:,1], zs=tensor[:,2], s=200)\n",
    "#     for i in range(nclass):\n",
    "#         ax.text(x=tensor[i,0], y=tensor[i,1],z=tensor[i,2], s=i_to_s[i], ha='center', va='center', color='white')\n",
    "#     # displaying the plot\n",
    "#     plt.grid('minor')\n",
    "#     plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset, prepare Context window\n",
    "\n",
    "1. split rate 90%, 10%\n",
    "\n",
    "2. Dev or Validation set is for hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n90 = int( 0.9*len(enc_data) )\n",
    "train_data = enc_data[:n90]\n",
    "val_data = enc_data[n90:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we never feed all the text into the Transformer all at once, that would be computationally very expensive, and prohibitive.\n",
    "\n",
    "We actually only work with chunks of text sampled from the dataset. we call it context and we have a context length.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTEXT_L = 8\n",
    "train_data[:CONTEXT_L+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] -> 47\n",
      "[18, 47] -> 56\n",
      "[18, 47, 56] -> 57\n",
      "[18, 47, 56, 57] -> 58\n",
      "[18, 47, 56, 57, 58] -> 1\n",
      "[18, 47, 56, 57, 58, 1] -> 15\n",
      "[18, 47, 56, 57, 58, 1, 15] -> 47\n",
      "[18, 47, 56, 57, 58, 1, 15, 47] -> 58\n"
     ]
    }
   ],
   "source": [
    "# we want the model to predict what char comes after\n",
    "# any number of chars from 1 to CONTEXT_L as input\n",
    "\n",
    "x = train_data[:CONTEXT_L]\n",
    "y = train_data[1:CONTEXT_L+1]\n",
    "for t in range(CONTEXT_L):\n",
    "    context = x[:t+1] # +1 is bc t starts from 0 and we would get empty window without it\n",
    "    target = y[t]\n",
    "    print(f'{context.tolist()} -> {target}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating both Batch dimension and time (or context) dimensions\n",
    "\n",
    "we have batch & time (in context window) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_reproduciblility import set_all_seeds, set_deterministic\n",
    "\n",
    "set_all_seeds(seed=1337)\n",
    "set_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb=tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "yb=tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "if context=[24] -> target=43\n",
      "if context=[24, 43] -> target=58\n",
      "if context=[24, 43, 58] -> target=5\n",
      "if context=[24, 43, 58, 5] -> target=57\n",
      "if context=[24, 43, 58, 5, 57] -> target=1\n",
      "if context=[24, 43, 58, 5, 57, 1] -> target=46\n",
      "if context=[24, 43, 58, 5, 57, 1, 46] -> target=43\n",
      "if context=[24, 43, 58, 5, 57, 1, 46, 43] -> target=39\n",
      "if context=[44] -> target=53\n",
      "if context=[44, 53] -> target=56\n",
      "if context=[44, 53, 56] -> target=1\n",
      "if context=[44, 53, 56, 1] -> target=58\n",
      "if context=[44, 53, 56, 1, 58] -> target=46\n",
      "if context=[44, 53, 56, 1, 58, 46] -> target=39\n",
      "if context=[44, 53, 56, 1, 58, 46, 39] -> target=58\n",
      "if context=[44, 53, 56, 1, 58, 46, 39, 58] -> target=1\n",
      "if context=[52] -> target=58\n",
      "if context=[52, 58] -> target=1\n",
      "if context=[52, 58, 1] -> target=58\n",
      "if context=[52, 58, 1, 58] -> target=46\n",
      "if context=[52, 58, 1, 58, 46] -> target=39\n",
      "if context=[52, 58, 1, 58, 46, 39] -> target=58\n",
      "if context=[52, 58, 1, 58, 46, 39, 58] -> target=1\n",
      "if context=[52, 58, 1, 58, 46, 39, 58, 1] -> target=46\n",
      "if context=[25] -> target=17\n",
      "if context=[25, 17] -> target=27\n",
      "if context=[25, 17, 27] -> target=10\n",
      "if context=[25, 17, 27, 10] -> target=0\n",
      "if context=[25, 17, 27, 10, 0] -> target=21\n",
      "if context=[25, 17, 27, 10, 0, 21] -> target=1\n",
      "if context=[25, 17, 27, 10, 0, 21, 1] -> target=54\n",
      "if context=[25, 17, 27, 10, 0, 21, 1, 54] -> target=39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwe should see 4 rows (independent batch dimension)\\nand 8 cols (Context window length)\\n\\nthen we take each row and create multiple sequence\\nwith max size of CONTEXT_L\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4 # how many independent sequence we process in parallel\n",
    "CONTEXT_L = 8 # the max context length for input and output\n",
    "\n",
    "def get_batch(stage:str):\n",
    "    data = train_data if stage=='train' else val_data\n",
    "    # a BATCH_SIZE number of int for context_window_starts\n",
    "    # this random init for the context window throughout the dataset is good\n",
    "    # as the tone and style of the text might change from the start to the end of text\n",
    "    cw_starts = torch.randint(low=0, high=len(data)-CONTEXT_L , size=(BATCH_SIZE,) )\n",
    "    x = torch.stack([ data[ cw_start : cw_start+CONTEXT_L ] for cw_start in cw_starts])\n",
    "    # shift the window by for y\n",
    "    y = torch.stack([ data[ cw_start+1 : cw_start+CONTEXT_L+1 ] for cw_start in cw_starts])\n",
    "    return x, y\n",
    "\n",
    "# create the batch of independent context windows\n",
    "xb, yb = get_batch('train')\n",
    "print(f'{xb=}')\n",
    "print(f'{yb=}')\n",
    "\n",
    "# from each independent context window,\n",
    "# create (independent?) sequences with different size of characters\n",
    "for b, t in itertools.product(range(BATCH_SIZE),range(CONTEXT_L)):\n",
    "    context = xb[b, :t+1]\n",
    "    target = yb[b, t]\n",
    "    print(f'if context={context.tolist()} -> target={target}')\n",
    "\n",
    "\n",
    "'''\n",
    "we should see 4 rows (independent batch dimension)\n",
    "and 8 cols (Context window length)\n",
    "\n",
    "then we take each row and create multiple sequence\n",
    "with max size of CONTEXT_L\n",
    "'''\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create Bigram as baseline! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # a look up table :\n",
    "        # row : each token - col : logits of the next token (each token directly reads off the logit of the next token) \n",
    "        self.token_embedding_lkt = nn.Embedding(num_embeddings=NCLASS, embedding_dim=NCLASS, device=DEVICE)\n",
    "    \n",
    "    def forward(self, idx, target):\n",
    "        # idx & target are both (batch_size, context_length)\n",
    "        logits = self.token_embedding_lkt(idx) # (batch_size, context_length, n_class)\n",
    "        loss = F.cross_entropy(input=logits.view(-1, NCLASS), target=target.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx : scalar\n",
    "        int_seq = [idx]*max_new_tokens\n",
    "        for i in range(max_new_tokens):\n",
    "            logits = self.token_embedding_lkt(idx) # (n_class)\n",
    "            probs = F.softmax(input=logits, dim=-1).view(-1)  # (n_class)\n",
    "            idx = torch.multinomial(input=probs, num_samples=1, replacement=True)\n",
    "            int_seq[i] = idx.item()\n",
    "        return int_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65]) tensor(4.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = Bigram()\n",
    "logit , loss = model(xb, yb)\n",
    "print(logit.shape, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lcRGUBgKwR'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_seq = model.generate(idx=torch.zeros(1, dtype=torch.long, device=DEVICE), max_new_tokens=10)\n",
    "decoder(int_seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mopen_ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/mehdi/Data/Education and Migration/MSc EIT/Sweden/KTH/Studies/Semester 2/P4/21 DL for DS/Lab Assignments/KarpathyPyTorchTutorial/wandb/run-20230606_012849-ngan4abg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/open_ai/TinyGPT/runs/ngan4abg' target=\"_blank\">kind-silence-1</a></strong> to <a href='https://wandb.ai/open_ai/TinyGPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/open_ai/TinyGPT' target=\"_blank\">https://wandb.ai/open_ai/TinyGPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/open_ai/TinyGPT/runs/ngan4abg' target=\"_blank\">https://wandb.ai/open_ai/TinyGPT/runs/ngan4abg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/open_ai/TinyGPT/runs/ngan4abg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f7e1630f400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "BATCH_SIZE = 32 # how many independent sequence we process in parallel\n",
    "CONTEXT_L = 8 # the max context length for input and output\n",
    "\n",
    "LR = 0.01\n",
    "NEPOCH = 1000\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"TinyGPT\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"BigramEmbedding\",\n",
    "    \"dataset\": \"Shakespeare\",\n",
    "    \"epochs\": NEPOCH,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4533, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(NEPOCH):\n",
    "    xb, yb = get_batch(stage='train')\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    #   log metrics to wandb\n",
    "    wandb.log({\"loss\": loss})\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ba742aed464b85b7cfdb2684575ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">peachy-blaze-1</strong> at: <a href='https://wandb.ai/open_ai/TinyGPT/runs/wt6ps6hx' target=\"_blank\">https://wandb.ai/open_ai/TinyGPT/runs/wt6ps6hx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230605_225848-wt6ps6hx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch113_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
